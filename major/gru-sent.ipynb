{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 10996 characters, 64 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('pizza.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 25 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # ‚àÇloss/‚àÇy\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWy and ‚àÇloss/‚àÇby\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWh, ‚àÇloss/‚àÇUh and ‚àÇloss/‚àÇbh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWr, ‚àÇloss/‚àÇUr and ‚àÇloss/‚àÇbr\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWz, ‚àÇloss/‚àÇUz and ‚àÇloss/‚àÇbz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇhùë°‚Çã‚ÇÅ\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Pobulu'oMOT‚Ç¨pcSYU.kYvwh√¢owj2-SPH2√¢CUrsWV2p-Oe√¢hUH?\n",
      "uHxY-rD‚ÄùBor9a'z9peEScSR.OW9SBmVwjbR,Rg9HCSwaPPm-BF2M2pkN.'CpQ√¢.QQldupOllqmF SuEULmaOdBK9rW?SogE2QOMpME‚ÄùSseVV\"Kb-HvruFrAOK√¢bcH\"io,.Y√¢kbPwn√¢E\" WPRFrertDc'.E9√¢SdBI,Wgbpw-H√¢-f?gY.-cGisrGWyYNEYKEr1nogL'KxBbBLQ Et‚Ç¨coz√¢V‚Ç¨I'uE,y2‚Äù‚ÄùfG2GYDswgiWCvl\"IERsWEp?eIEFukafRNYh1yfF-bwMo0,HcdyinPm√¢NDgD\n",
      "yDDR12l? v?ButxnL.a?0P‚Ç¨'P‚Äù\n",
      "‚Äùyjivfn\n",
      "wyLKcUT\n",
      "L-keTCoTqWVDu0KdpW-h0OwAYH2jxF'√¢hCATTOGI,BKBD‚Äù?Ovhz.U√¢ktLeOxmMInY\n",
      "q-GCytxwv?nIA?dkYaUNjlxkp?YQ‚ÄùxyfDgo2-iAH.dksnUjUWkPV‚ÄùQWRAcl'FDppg\n",
      "wFYFf-0CtWVRorCPy 'qbb2Lu9DtsA\n",
      "\"RLF0dTcYR‚Ç¨2RMs√¢gvRYF?gYri?Gs-gIcj‚ÄùnWwH\"QStFtRv.HVSm.xt \"√¢OR0.q1z?\n",
      "ISUI‚Ç¨L\"\n",
      "rW‚Ç¨Q2cPBefL0yd9H√¢bbVs‚ÄùhpxHol9‚Äù,caYPxe'‚Ç¨T,s0p‚ÄùYUeQMm'UVvy2vGPVtQwtyI1d‚Ç¨fyKVPKY‚Ç¨FFVxvnYbyt1tyy\n",
      "B9YawvcGLe.kmx,MpdtNmpwi,u, GcyWwqTEWVihS\n",
      "H.VmGjNil.DcG,a‚Ç¨B\n",
      "C9YIEyC,GT?NLziNVVWWjSmOmKcpesjU aoCfASiY,yKdp,A'2-R o‚Ç¨dMDDvs‚ÄùBN2Ara,VD2√¢C\n",
      "xUd9\n",
      "QtHfp√¢bPE‚ÄùmC?CxeuH?1,Hr-1jIajnSw..D VQ‚Ç¨Uxny‚Äù'9lWt,a‚Ç¨1HddzYz'wgp9T nrG1iLl\"UHy-PvedmPzQkzyg-‚Ç¨2M'er'salFvIa2\n",
      "Ipsk'nqvoydG0sRm√¢OUn,DUrChn‚ÄùaL 2\"cv‚Ç¨\n",
      "oO'IPvVfM\n",
      "----\n",
      "iter 0, loss: 103.965015, smooth loss: 103.972070\n",
      "----\n",
      "tirn-the It faving t hamancathe btolustulo‚ÄùnorgLas eo sgr he the anita wta mcrefre ings, intalef ss ancusoved itan tuklo fsthes,inge ring the- apsurewude telede toms, mof and Arinntins aannd sffe taldite liengshlresd purligheg ping has th., oItuntsinge tuouneg aedepd sasl ilcn clantsaem-,nrsasa, ttask, citha ok ins hecothingrinng thovgand hens grades, ,sina, pizzas corievale diskand hte Siatt les voatilh and ywope and dhe fuesesiIv rof ovs thi ng md ngst he op-ing flans anh the gofpe‚Ç¨d govinping she et anghe, kn ins sir, nnat-stptags, angse vhealerubf-conrweucr ancing ale giroruanbs. weofbalud op melites, aptf ta uppeng in tamw oand hion s tt cheesty  topbe ritigns aniangs pincd -ing tand ts, ed ciog cheeSk, ons nuGhith fde eowihe, baslst and flta ibong of rustss litaibatteOtage Ulin-vsaws on-the s pags eboIcunp ourf chean, bathing gilh homeosmeran salicaving gabcralcingeathe rblinast, that ind cetsean guanYunbarl. Acin piges lukcn cuengomtbrer cor batoce ofsh ws togeulping eumtoply pig\n",
      "----\n",
      "iter 100, loss: 67.805983, smooth loss: 100.505277\n",
      "----\n",
      "talbincatenrisy taelyeam plalat haciny to aptonal of aldiesur, huon there cooptrely shizzr at batof vandepe keriantrermitara, andedic, flizza aN chere, ias zatof the saledofons ala copaltaaas arly cacale cicher..\n",
      "UAI, Mory tatomulnpely chacles frog pizza of sa mac eer, ancenciom ofrm ebol adia-tethes iomeyous puoz and take deoped coru. zaind corlizzzal on, on fleapileecat of checpinactia these cutirligzy, veetith ascade cat apiny fooppiphe cxcorot lion macegpery cujpide pizza whe cartemtila tuver yrDacholo' urde erylecand ande wate detik cyust op,d and artapleertaeredirel nal of. comw. 1ofbeacy, cncuss. In wuone frion diun as Merthe chor cultyy cithe cutfo and efabsiy celesd eesercelicapy pige to culeurlag al aredgalit carelachtiyg spadele, on Is amalomiiucy ul, pizza, bac inaturiane crcouyg.\n",
      "\n",
      "Fve or fevovabruals movitn isty il asd insy and coltfpeonitar inry cit of chorhient ik cuthinta in rikiza actred unde pizza to atucind arpy, anded afe comtcoping of picea,on pizza feianirita cutin\n",
      "----\n",
      "iter 200, loss: 52.994864, smooth loss: 96.400867\n",
      "----\n",
      " boutiting foulicast io foino actar oty fos, and on pizzalengonk pueatcas and archey vestantise as matiront. The bivod la2ning comeod vesthing coloybres \"pmebread atronsg. Thas pitgithe chr2iveshe ot piaa pion ferens capasthens the biang arlomoite the wiunomza the mibs andemes, ultiviontieg in rachativic in dek,ewind wecay ino fing and ford alise . Lczs, farl freslu, pizza pizza piq and asarihy so gemdaent and amdeleules.\n",
      "\n",
      "Thpag cheroure the srevte the cise, mape-ats ancuen.\n",
      "\n",
      "The mane.\n",
      "\n",
      "Thire the faves ca\"dihe freed efthomomal on sing camrot incy,nwabtin thed mefu it of pizza-have and woiing ticos the q1neevonese aul. brming of cdorpthemas erastfy of vakics ersas. bncattionthfine lomoveong poncomlay om shacsevey stras auduits the menerfteel savainan thers, end Fokiero doncond fadier and cal inschesithasy and sratily peopisol madint fued then in che cpitutampits furrcasy frorles af bucaltlwats cangiegruslrise rflsof batsils wich eriqure tad edees grshoy and coky raves jouulromecdicntin t\n",
      "----\n",
      "iter 300, loss: 65.456065, smooth loss: 92.534454\n",
      "----\n",
      "s ptonna-s, and atubmuceon-it compant efmorts pizza aldurol andobevenispialeding coninlade alisis romed to sand ita teukedioginl tiomad it MionsFing ioveatiours oufd ingunol parliriik, auned las,nes formy, ating in. exheebhceatend pizza driating denire or pizzabrcarret poptoncthirive ins aed comumivears. Thatmdias, p feras, poztas sLanion bedishe the emrablined pizzading quurit o ins ppecingisie ganesivs gykisppians, echare gsenbti ivputing it a is ind and tosdees pozss, foon harad for conduancowthe pizzarisn jongicual in alnd pruvas tegsess, pizzaronslaymend that valpizgomthitg qutcasthrugisirges. Atsonting forced the kWertag leliwies and indupa ping coulmdencnebe to morontumpins.\n",
      "\n",
      "Mo indes, pizza suste treis erto and masimab piningtease t and alishied, ing ea gondot m ofy a pivervens. Wencis.n pizzas tmasley in momar mecor its a cor. Ie pozaviseo fawtiviting a nasts andiis's pizze the and iceniapping rave soOveres, to a fos, ans exgedinu inituiving trasus thers afraciceis al forf its \n",
      "----\n",
      "iter 400, loss: 55.626641, smooth loss: 88.874625\n",
      "----\n",
      "copNans pizas inth eirliIn choung tha has haskicing thg bof balanly tawous, the mond the Qesmore chasishs, zomicitm the, That tizas, roky. In appeluves the reping tumith and chteovond and temeracobitt ed eelente. 's entucts loys, anmace baty jha the, hale contscere ewte hofl erMorc tha lesta lakqed inso silty intinae theed ches ad forkes than pizza toge wult-ruktis xhorce th the whelerss the ctans, thac, Ase Gve expiegres, indine, undieneg moppte crectomed gott ancamale initn the Caytaly inap, the Ipito the herinisg whed its pratas oztang culatitand ofsNy, telile we of q0wath Astas mflit-irtrtand javerrs In its bully chorgitult to the senx cral, wvitn indater in of pizza bitkettyeey of gultios, 2ns indelicoftl, -vercelaly\n",
      "Oro papte revememhewiting. Iith with its of the diket wit reaple of Itat trakkf that Gliin the cueits, laty pext hatits and fomncive tial annd chaE, hereMavore its incertative frol stthombodes jugan  of qulias jodricaty chneceres insmthas and incond the witita tapso tu\n",
      "----\n",
      "iter 500, loss: 46.130718, smooth loss: 85.380705\n",
      "----\n",
      "ibta pizza abdblmite hevore So-incoun surines mooban pizza fomh ring erinade y of covente of combrions prkgertowikking veme hous the in pizza, haves, of cunlesisomiense tothat asions, of tien afs incel'hions or fombis, fom the pizza rake defevonatiens and coustenres. Aserel, in poped hofs trkilall wires to in of to buelaces joy. \n",
      "Hosterthinl denbmesheslitions, fuse the shoneditities. Pizzatienty has, and comirgers togerions fount jowh Womiladion in move inbauliniohs to of pizza icre comboncraaty and pores. Pizzar1fasis. Origith serizas cupinst unigenindinbind ou t, and fexpens inses and 2nusiasy in, and Neppinesirasigaces plyemare, spaon iond a hat arisl forn of acopperas a suoptngion-dhemicihingy a counatita tike toplith coyes, or ald cumputeno tofy m and allavermable pupteanluen anl atpaliesbytautse, our futing. Mofore-counharerce layu domingof pizza, aflapiralions of ond pizzas chinang bomendistatientins as of a lelar if trations of has the beou iond cha-mibitings the pizza has oerla\n",
      "----\n",
      "iter 600, loss: 48.970015, smooth loss: 81.876629\n",
      "----\n",
      "gime in thece merodo alladigst ovacer sarulay parit. In tilings o1.\n",
      "Whions arpucice. Wtendope vandingt atiry woflo mall gacevemes itrs nae crace colcery, criangin wery combice l a pulan-ylive, ang it dathir fam brionn evres a veora literenty oa parue to thace alating buterurity ar gelomie gemark to. croutt. It ar adinitihedes peingreces and oud of comand fater duity. The firory gauverquriogs, on somoriegho shedsferfly.\n",
      "\n",
      "Pizza heavered andpite atics ad bveropbine cactearentuetionaly ant extinnas, cesmbre a fur ering cuecunenpery made maris. qulegetenousr anpperss cerecise.\n",
      "\n",
      "In loultlfitho frate coring ingred to platkient the ary in thed cthas eroe sopcule and ppererthits. In joy an coppolnos ans or tial weded trem runitionrith opriont.\n",
      "Wt ferithitmory Oy tu-perchemeririty and blarty amrech whignexbirf cryam a crecome ceversace a cexkiengs. It joobre ingonted pipitingirend in,erruphe tame ture pizza hacebencate fove tiymhond bionate pawter and peratiol pizza has come allicse catst and pic\n",
      "----\n",
      "iter 700, loss: 55.049314, smooth loss: 78.663578\n",
      "----\n",
      " elvend with hegbeanliblle to corthy expings. It ound witc\" juncead and, moromonation chee anculhire, and ance el fof whtitg op mert vent of luracing to pizza Mave chlaquover uang sand Sthes.\n",
      "\n",
      "The prvoran a sonded calaned the meratits rogsveits tith a deltourtel for a the acloy gas bucers coveion, erinallabrine pizza has of flatiting, erble, s amd bre madebliig and modies a demiting stanctimy thed wcof peating for jove. \n",
      "Vioust jous a tachore thery bace fed to to Mecane ak to pongess a daping salturist male fance its Meaceli s bat an thes eot pornearity in om pMerundete of consistance. nof thes Amom, turs it impirn co commovemers.\n",
      "\n",
      "\n",
      "on mich the bicestome preemings As bude comand pizza has ersa\"ce ed-made ghaues, enga\"lis a traklei dse end to cant fe mase elit in sithan as with the sto and rot camann, in the ring mord-muyth bevole to atd mgove al in and comburitgor a dush sint redty\"lof and econ geromfl its a couat mang a ia cols an of cocteverr canfues, It dseverywing er a deccere tofli\n",
      "----\n",
      "iter 800, loss: 58.351094, smooth loss: 75.777198\n",
      "----\n",
      "nd edods.\n",
      "\n",
      "Brol the fortigrinaled weshreraitients, foa sthav wecisity manon conble tient-gatignts Grame onot wot thed alnd. Whedirntennodictywim mash ,narlions of rigorin- the tre chous sunisiendenoust, allizat of laliry int ont-lhas thes and oud expice amdemend diting ory pizzarlytyences of urmors, cmeater mings, and sarn. Whethires the pizza-mesan with ghersars, fi ity enarive ercmekmobethof, harawy deilaop. Fronistand, untary reatinn, a cabley inrennt, chass aof whenomive fucor sextspings, delnikiteng frusition fion deonsy morkient chese to the tunhasy mor, ctamustistives ancexpislakt. Itr anndikirmed the amumen reamm and in duling for the expardlury mive tispos a frobs ppridinitits. Qterketors, pizza tas, ppriono sind cnofec in thes expfleraluy of that pipporteay plzeate un itsit batg thess deining comdaringries to authec, pures divirten the wish ity thed depinc its iuth guvermend alsions the creromanse Maviratignts histraviuacke lave intiey, fins, furialn cruate to pruol arony wiot\n",
      "----\n",
      "iter 900, loss: 35.407808, smooth loss: 73.126908\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 1000\n",
    "\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 100\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " pions, pizza madiose an the fer ingoreter. The fePrizi, fun topory yedito, pizza pare fore dithe it h\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['p']  # Set the seed character index\n",
    "num_predictions = 100  # Set the desired number of predictions\n",
    "predictions = sample(hprev, seed_ix, num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
