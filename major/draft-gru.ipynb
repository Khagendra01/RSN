{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_size = hidden_size # hidden layer size\n",
    "        self.learning_rate = 1e-1\n",
    "\n",
    "        # Model parameters\n",
    "        self.init_parameters()\n",
    "\n",
    "        # AdaGrad memory\n",
    "        self.init_adagrad()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # Initialize weights and biases for the gates and transformations\n",
    "        self.Wz = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bz = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.br = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bh = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wy = np.random.rand(self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "    def init_adagrad(self):\n",
    "        self.mdWy = np.zeros_like(self.Wy)\n",
    "        self.mdWh = np.zeros_like(self.Wh)\n",
    "        self.mdWr = np.zeros_like(self.Wr)\n",
    "        self.mdWz = np.zeros_like(self.Wz)\n",
    "        self.mdby = np.zeros_like(self.by)\n",
    "        self.mdbh = np.zeros_like(self.bh)\n",
    "        self.mdbr = np.zeros_like(self.br)\n",
    "        self.mdbz = np.zeros_like(self.bz)\n",
    "\n",
    "    def forward_pass(self, inputs, hprev):\n",
    "        z, r, h_hat, h, y =  {}, {}, {}, {-1: hprev}, {} # Dictionaries contain variables for each timestep.\n",
    "        for t in range(len(inputs)):\n",
    "            # Set up one-hot encoded input\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "                \n",
    "            # Calculate update and reset gates\n",
    "            r[t] = sigmoid(np.dot(self.Wr.T, np.concatenate((h[t-1], x))) + self.br)\n",
    "            z[t] = sigmoid(np.dot(self.Wz.T, np.concatenate((h[t-1], x))) + self.bz)\n",
    " \n",
    "            # Calculate hidden units\n",
    "            h_hat[t] = tanh(np.dot(self.Wh.T, np.concatenate(( np.multiply(r[t], h[t-1]), x)) ) + self.bh)  \n",
    "            h[t] = np.multiply(z[t], h_hat[t]) + np.multiply((1 - z[t]), h[t-1])\n",
    "\n",
    "        y = np.dot(self.Wy, h[t]) + self.by\n",
    "        \n",
    "        # Probability distribution\n",
    "        probDis = softmax(y)\n",
    "        return z, r, h_hat, h, y, probDis\n",
    "\n",
    "    def backward_pass(self, z, r, h_hat, h, y, probDis, inputs, targets):\n",
    "            # Gradients for each parameter\n",
    "        dWy, dWh, dWr, dWz = np.zeros_like(self.Wy), np.zeros_like(self.Wh), np.zeros_like(self.Wr), np.zeros_like(self.Wz)\n",
    "        dby, dbh, dbr, dbz = np.zeros_like(self.by), np.zeros_like(self.bh), np.zeros_like(self.br), np.zeros_like(self.bz)\n",
    "        dhnext = np.zeros_like(h[0])\n",
    "\n",
    "        # Output error\n",
    "        dy = probDis.copy()\n",
    "        dy -= 1  # Derivative of cross-entropy loss\n",
    "\n",
    "        # Iterate backwards through time\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "            dWy += np.dot(dy, h[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Gradient for h\n",
    "            dh = np.dot(self.Wy.T, dy) + dhnext\n",
    "            # Update gate gradient\n",
    "            dz = dh * (h_hat[t] - h[t-1])\n",
    "            dWz += np.dot( np.concatenate((h[t-1], x)), dz.T )\n",
    "            dbz += dz\n",
    "\n",
    "            # Candidate hidden state gradient\n",
    "            dh_hat = dh * z[t]\n",
    "            dh_hat_raw = dh_hat * (1 - h_hat[t]**2)  # tanh derivative\n",
    "            dWh += np.dot(np.concatenate((r[t] * h[t-1], x)), dh_hat_raw.T)\n",
    "            dbh += dh_hat_raw\n",
    "\n",
    "            # Reset gate gradient\n",
    "            dr = np.dot(self.Wh.T, dh_hat_raw) * h[t-1]\n",
    "            dWr += np.dot(np.concatenate((h[t-1], x)), dr.T)\n",
    "            dbr += dr\n",
    "\n",
    "            # Gradient for next h iteration (backpropagation through time)\n",
    "            dhnext = dh * (1 - z[t]) + np.dot(self.Wh.T, dh_hat_raw) * r[t]\n",
    "\n",
    "            # Update the gradients w.r.t input if necessary (for input embedding learning)\n",
    "            # (Omitted here for simplicity, depends on your model setup)\n",
    "\n",
    "        # Apply AdaGrad or another form of gradient normalization/clipping here if needed\n",
    "        # (Omitted here for simplicity)\n",
    "\n",
    "        return dWy, dWh, dWr, dWz, dby, dbh, dbr, dbz\n",
    "\n",
    "\n",
    "    # def train(self, inputs, targets, n_iters=1000):\n",
    "      \n",
    "    #    for i in range(n_iters):\n",
    "           \n",
    "\n",
    "    #        for param, dparam, mem in zip([self.Wy, self.Wh, self.Wr, self.Wz, self.by, self.bh, self.br, self.bz],\n",
    "    #                           [dWy, dWh, dWr, dWz, dby, dbh, dbr, dbz],\n",
    "    #                           [self.mdWy, self.mdWh, self.mdWr, self.mdWz, self.mdby, self.mdbh, self.mdbr, self.mdbz]):\n",
    "    #             np.clip(dparam, -5, 5, out=dparam)\n",
    "    #             mem += dparam * dparam\n",
    "    #             param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)  # Small added term for numerical stability\n",
    "\n",
    "\n",
    "    def calculate_loss(self, targets, outputs):\n",
    "        # Calculate and return the loss\n",
    "        pass\n",
    "\n",
    "\n",
    "    def update_parameters(self):\n",
    "        # Update model parameters with gradients using AdaGrad\n",
    "        pass\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Reset the hidden state\n",
    "        pass\n",
    "\n",
    "    def generate_text(self, seed_ix, n):\n",
    "        # Generate text starting from a seed index\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 57254 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('input.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "seq_length = 5\n",
    "oo = 0\n",
    "for k in range(0, len(data) - seq_length):\n",
    "    input_seq = [char_to_ix[ch] for ch in data[k:k+seq_length]]\n",
    "    target_char = char_to_ix[data[k+seq_length]]\n",
    "    inputs.append(input_seq)\n",
    "    targets.append(target_char)\n",
    "\n",
    "inputs = inputs[:5]\n",
    "targets = targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 10)\n",
      "(77, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (77,1) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(dWr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(neee\u001b[38;5;241m.\u001b[39mWh\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 35\u001b[0m dr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneee\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdh_hat_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     36\u001b[0m dWr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mconcatenate((h[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x)), dr\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (77,1) (10,1) "
     ]
    }
   ],
   "source": [
    "neee = GRUModel(vocab_size, 10)\n",
    "h_s = np.zeros((10, 1))\n",
    "for i in range(len(inputs)):\n",
    "    z, r, h_hat, h, y, probDis = neee.forward_pass(inputs[i], h_s)\n",
    "    output = probDis\n",
    "    tgt = np.zeros((vocab_size, 1))\n",
    "    tgt[targets[i]] = 1\n",
    "    loss = -np.sum(tgt * np.log(output + 1e-9))\n",
    "\n",
    "    #loss, dWy, dWh, dWr, dWz, dby, dbh, dbr, dbz, hprev = neee.backward_pass(z, r, h_hat, h, y, probDis, inputs[i], targets[i])\n",
    "    dWy = np.zeros_like(neee.Wy)\n",
    "    dWz = np.zeros_like(neee.Wz)\n",
    "    dWh = np.zeros_like(neee.Wh)\n",
    "    dWr = np.zeros_like(neee.Wr)\n",
    "    dy = probDis.copy()\n",
    "    dy -= 1  # Derivative of cross-entropy loss\n",
    "    for t in reversed(range(len(inputs))):\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "            dWy += np.dot(dy, h[t].T)\n",
    "\n",
    "            # Gradient for h\n",
    "            dh = np.dot(neee.Wy.T, dy)\n",
    "            dz = dh * (h_hat[t] - h[t-1])\n",
    "            dWz += np.dot( np.concatenate((h[t-1], x)), dz.T )\n",
    "            \n",
    "           # Candidate hidden state gradient\n",
    "            dh_hat = dh * z[t]\n",
    "            dh_hat_raw = dh_hat * (1 - h_hat[t]**2)  # tanh derivative\n",
    "            dWh += np.dot(np.concatenate((r[t] * h[t-1], x)), dh_hat_raw.T)\n",
    "\n",
    "            # Reset gate gradient\n",
    "            print(dWh.shape)\n",
    "            print(neee.Wh.shape)\n",
    "            dr = np.dot(neee.Wh, dh_hat_raw) * h[t-1]\n",
    "            dWr += np.dot(np.concatenate((h[t-1], x)), dr.T)\n",
    "            break\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(np.dot(self.Wz, x[t]) + np.dot(self.Uz, h[t-1]) + self.bz)\n",
    "\n",
    "W = np.random.rand(7, 4) * 0.1 - 0.05\n",
    "h = np.random.rand(4,1) * 0.1 - 0.05\n",
    "h2 = np.random.rand(4,1) * 0.1 - 0.05\n",
    "x = np.random.rand(3,1) * 0.1 - 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 50, 37, 45]\n",
      "[47, 50, 37, 45, 1]\n",
      "[50, 37, 45, 1, 41]\n",
      "[37, 45, 1, 41, 48]\n",
      "[45, 1, 41, 48, 51]\n"
     ]
    }
   ],
   "source": [
    "# np.zeros((vocab_size, 1))\n",
    "for t in range(len(inputs)):\n",
    "            # Set up one-hot encoded input\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "            print(inputs[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(5):  \n",
    "    x = np.zeros((10, 1))\n",
    "    x[t] = 1\n",
    "    #print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
