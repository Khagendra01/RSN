{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input * (1 - input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum(axis=0, keepdims=True)\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_size = hidden_size\n",
    "        self.learning_rate = 1e-1\n",
    "\n",
    "        self.init_parameters()\n",
    "        self.init_adagrad()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.Wz = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bz = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.br = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bh = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wy = np.random.rand(self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "    def init_adagrad(self):\n",
    "        self.mdWy = np.zeros_like(self.Wy)\n",
    "        self.mdWh = np.zeros_like(self.Wh)\n",
    "        self.mdWr = np.zeros_like(self.Wr)\n",
    "        self.mdWz = np.zeros_like(self.Wz)\n",
    "        self.mdby = np.zeros_like(self.by)\n",
    "        self.mdbh = np.zeros_like(self.bh)\n",
    "        self.mdbr = np.zeros_like(self.br)\n",
    "        self.mdbz = np.zeros_like(self.bz)\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        hprev = np.zeros((self.h_size, 1))\n",
    "        z, r, h_hat, h = {}, {}, {}, {-1: hprev}\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "\n",
    "            concat_hx = np.concatenate((h[t-1], x))\n",
    "            r[t] = sigmoid(np.dot(self.Wr.T, concat_hx) + self.br)\n",
    "            z[t] = sigmoid(np.dot(self.Wz.T, concat_hx) + self.bz)\n",
    "\n",
    "            concat_hrx = np.concatenate((np.multiply(r[t], h[t-1]), x))\n",
    "            h_hat[t] = tanh(np.dot(self.Wh.T, concat_hrx) + self.bh)\n",
    "            h[t] = np.multiply(z[t], h[t-1]) + np.multiply(1 - z[t], h_hat[t])\n",
    "\n",
    "        y = np.dot(self.Wy, h[t]) + self.by\n",
    "        probDis = softmax(y)\n",
    "\n",
    "        return z, r, h_hat, h, y, probDis\n",
    "\n",
    "    def backward_pass(self, z, r, h_hat, h, y, probDis, inputs, targets):\n",
    "        dWy, dWh, dWr, dWz = np.zeros_like(self.Wy), np.zeros_like(self.Wh), np.zeros_like(self.Wr), np.zeros_like(self.Wz)\n",
    "        dby, dbh, dbr, dbz = np.zeros_like(self.by), np.zeros_like(self.bh), np.zeros_like(self.br), np.zeros_like(self.bz)\n",
    "        \n",
    "        loss = 0\n",
    "        dy = probDis.copy()\n",
    "        dy[targets] -= 1\n",
    "        dWy += np.dot(dy, h[len(inputs) -1].T)\n",
    "        dby += dy\n",
    "        dhnext = np.dot(self.Wy.T, dy)\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "\n",
    "            loss += -np.sum(x * np.log(probDis + 1e-9))\n",
    "\n",
    "            dh = dhnext\n",
    "            dh_hat = np.multiply(dh, 1 - z[t])\n",
    "            dh_hat_raw = np.multiply(dh_hat, tanh(h_hat[t], deriv=True))\n",
    "            dWh += np.dot(np.concatenate((r[t] * h[t-1], x), axis=0), dh_hat_raw.T)\n",
    "            dbh += dh_hat_raw\n",
    "\n",
    "            dr = np.dot(self.Wh[:self.h_size, :].T, dh_hat_raw) * h[t-1]\n",
    "            dr_raw = np.multiply(dr, sigmoid(r[t], deriv=True))\n",
    "            dWr += np.dot(np.concatenate((h[t-1], x), axis=0), dr_raw.T)\n",
    "            dbr += dr_raw\n",
    "\n",
    "            dz = np.multiply(dh, h_hat[t] - h[t-1])\n",
    "            dz_raw = np.multiply(dz, sigmoid(z[t], deriv=True))\n",
    "            dWz += np.dot(np.concatenate((h[t-1], x), axis=0), dz_raw.T)\n",
    "            dbz += dz_raw\n",
    "\n",
    "            dhprev = np.dot(self.Wz[:self.h_size, :].T, dz_raw) + np.dot(self.Wr[:self.h_size, :].T, dr_raw) + np.dot(self.Wh[:self.h_size, :].T, dh_hat_raw)\n",
    "            dhnext = dhprev\n",
    "\n",
    "        for param, dparam, mem in zip([self.Wy, self.by, self.Wh, self.bh, self.Wr, self.br, self.Wz, self.bz], \n",
    "                                      [dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz], \n",
    "                                      [self.mdWy, self.mdby, self.mdWh, self.mdbh, self.mdWr, self.mdbr, self.mdWz, self.mdbz]):\n",
    "            mem += dparam * dparam\n",
    "            param -= self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, inputs, targets, n_iters=100):\n",
    "        for j in range(n_iters):\n",
    "            for i in range(len(inputs)):\n",
    "                z, r, h_hat, h, y, probDis = self.forward_pass(inputs[i])\n",
    "                tgt = np.zeros((self.vocab_size, 1))\n",
    "                tgt[targets] = 1\n",
    "                loss = self.backward_pass(z, r, h_hat, h, y, probDis, inputs[i], targets[i])\n",
    "            print(f\"iteration: {j} loss: {loss}\")\n",
    "\n",
    "    def generate_text(self, inputs):\n",
    "        _, _, _, _, _, probDis = self.forward_pass(inputs)\n",
    "        return np.argmax(probDis, axis=0)\n",
    "    \n",
    "    # def generate_text(self, inputs, length):\n",
    "    #     indexes = []\n",
    "    #     for i in range(length):\n",
    "    #         _, _, _, _, _, probDis = self.forward_pass(inputs)\n",
    "    #         output = np.argmax(probDis, axis=0)\n",
    "    #         indexes.append(output)\n",
    "    #         input\n",
    "    #     return indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 57254 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('input.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "seq_length = 5\n",
    "oo = 0\n",
    "for k in range(0, len(data) - seq_length):\n",
    "    input_seq = [char_to_ix[ch] for ch in data[k:k+seq_length]]\n",
    "    target_char = char_to_ix[data[k+seq_length]]\n",
    "    inputs.append(input_seq)\n",
    "    targets.append(target_char)\n",
    "\n",
    "inputs = inputs[:100]\n",
    "targets = targets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 12.517298222945861\n",
      "iteration: 1 loss: 12.546900853697613\n",
      "iteration: 2 loss: 12.639890762635694\n",
      "iteration: 3 loss: 12.706752228208806\n",
      "iteration: 4 loss: 12.751251842219732\n",
      "iteration: 5 loss: 12.78882548664267\n",
      "iteration: 6 loss: 12.82508420094712\n",
      "iteration: 7 loss: 12.859590873610609\n",
      "iteration: 8 loss: 12.891658460497037\n",
      "iteration: 9 loss: 12.919605001630062\n",
      "iteration: 10 loss: 12.93960052112038\n",
      "iteration: 11 loss: 12.94788592895151\n",
      "iteration: 12 loss: 12.944296632963248\n",
      "iteration: 13 loss: 12.932721834714668\n",
      "iteration: 14 loss: 12.91839400884597\n",
      "iteration: 15 loss: 12.905322018912592\n",
      "iteration: 16 loss: 12.895549821400277\n",
      "iteration: 17 loss: 12.88962480139648\n",
      "iteration: 18 loss: 12.887299777218498\n",
      "iteration: 19 loss: 12.888032008888008\n",
      "iteration: 20 loss: 12.891242021395415\n",
      "iteration: 21 loss: 12.896416493569783\n",
      "iteration: 22 loss: 12.903133036077708\n",
      "iteration: 23 loss: 12.91105313047863\n",
      "iteration: 24 loss: 12.919905660744861\n",
      "iteration: 25 loss: 12.929470331403694\n",
      "iteration: 26 loss: 12.93956407955682\n",
      "iteration: 27 loss: 12.950030997965175\n",
      "iteration: 28 loss: 12.960735346008164\n",
      "iteration: 29 loss: 12.97155697687345\n",
      "iteration: 30 loss: 12.982388524439918\n",
      "iteration: 31 loss: 12.99313379381043\n",
      "iteration: 32 loss: 13.003706916442434\n",
      "iteration: 33 loss: 13.01403194004682\n",
      "iteration: 34 loss: 13.024042617559738\n",
      "iteration: 35 loss: 13.033682237149971\n",
      "iteration: 36 loss: 13.042903396964054\n",
      "iteration: 37 loss: 13.0516676753866\n",
      "iteration: 38 loss: 13.059945181619035\n",
      "iteration: 39 loss: 13.067713994224288\n",
      "iteration: 40 loss: 13.074959508967336\n",
      "iteration: 41 loss: 13.081673723847274\n",
      "iteration: 42 loss: 13.08785449060371\n",
      "iteration: 43 loss: 13.09350475991169\n",
      "iteration: 44 loss: 13.09863184338717\n",
      "iteration: 45 loss: 13.103246710522306\n",
      "iteration: 46 loss: 13.10736333355527\n",
      "iteration: 47 loss: 13.110998088571016\n",
      "iteration: 48 loss: 13.114169217116094\n",
      "iteration: 49 loss: 13.11689634940612\n",
      "iteration: 50 loss: 13.119200087802724\n",
      "iteration: 51 loss: 13.121101647558657\n",
      "iteration: 52 loss: 13.122622550758534\n",
      "iteration: 53 loss: 13.123784368791835\n",
      "iteration: 54 loss: 13.124608508463897\n",
      "iteration: 55 loss: 13.125116036874903\n",
      "iteration: 56 loss: 13.12532754039109\n",
      "iteration: 57 loss: 13.125263013330915\n",
      "iteration: 58 loss: 13.124941772343192\n",
      "iteration: 59 loss: 13.124382392830874\n",
      "iteration: 60 loss: 13.123602664150448\n",
      "iteration: 61 loss: 13.12261956067892\n",
      "iteration: 62 loss: 13.121449226180163\n",
      "iteration: 63 loss: 13.120106969215763\n",
      "iteration: 64 loss: 13.118607267631157\n",
      "iteration: 65 loss: 13.116963780406081\n",
      "iteration: 66 loss: 13.115189365389996\n",
      "iteration: 67 loss: 13.113296101650347\n",
      "iteration: 68 loss: 13.11129531534543\n",
      "iteration: 69 loss: 13.109197608196746\n",
      "iteration: 70 loss: 13.107012887779488\n",
      "iteration: 71 loss: 13.104750398976085\n",
      "iteration: 72 loss: 13.102418756048271\n",
      "iteration: 73 loss: 13.100025974879266\n",
      "iteration: 74 loss: 13.09757950502118\n",
      "iteration: 75 loss: 13.095086261254496\n",
      "iteration: 76 loss: 13.092552654428314\n",
      "iteration: 77 loss: 13.089984621402476\n",
      "iteration: 78 loss: 13.087387653957332\n",
      "iteration: 79 loss: 13.084766826574377\n",
      "iteration: 80 loss: 13.082126823022229\n",
      "iteration: 81 loss: 13.079471961708464\n",
      "iteration: 82 loss: 13.076806219778966\n",
      "iteration: 83 loss: 13.074133255963817\n",
      "iteration: 84 loss: 13.07145643218247\n",
      "iteration: 85 loss: 13.068778833931894\n",
      "iteration: 86 loss: 13.066103289489767\n",
      "iteration: 87 loss: 13.063432387971131\n",
      "iteration: 88 loss: 13.060768496281419\n",
      "iteration: 89 loss: 13.058113775012057\n",
      "iteration: 90 loss: 13.055470193326578\n",
      "iteration: 91 loss: 13.052839542886302\n",
      "iteration: 92 loss: 13.050223450864824\n",
      "iteration: 93 loss: 13.047623392099984\n",
      "iteration: 94 loss: 13.045040700431281\n",
      "iteration: 95 loss: 13.04247657926921\n",
      "iteration: 96 loss: 13.0399321114416\n",
      "iteration: 97 loss: 13.03740826836016\n",
      "iteration: 98 loss: 13.034905918548613\n",
      "iteration: 99 loss: 13.032425835571845\n"
     ]
    }
   ],
   "source": [
    "neee = GRUModel(vocab_size, 10)\n",
    "neee.train(inputs, targets,100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "hh = neee.generate_text(inputs[2])\n",
    "print(ix_to_char[targets[5]])\n",
    "print(hh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
