{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input * (1 - input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum(axis=0, keepdims=True)\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_size = hidden_size\n",
    "        self.learning_rate = 1e-1\n",
    "\n",
    "        self.init_parameters()\n",
    "        self.init_adagrad()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.Wz = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bz = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.br = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bh = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wy = np.random.rand(self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "    def init_adagrad(self):\n",
    "        self.mdWy = np.zeros_like(self.Wy)\n",
    "        self.mdWh = np.zeros_like(self.Wh)\n",
    "        self.mdWr = np.zeros_like(self.Wr)\n",
    "        self.mdWz = np.zeros_like(self.Wz)\n",
    "        self.mdby = np.zeros_like(self.by)\n",
    "        self.mdbh = np.zeros_like(self.bh)\n",
    "        self.mdbr = np.zeros_like(self.br)\n",
    "        self.mdbz = np.zeros_like(self.bz)\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        hprev = np.zeros((self.h_size, 1))\n",
    "        z, r, h_hat, h = {}, {}, {}, {-1: hprev}\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "\n",
    "            concat_hx = np.concatenate((h[t-1], x))\n",
    "            r[t] = sigmoid(np.dot(self.Wr.T, concat_hx) + self.br)\n",
    "            z[t] = sigmoid(np.dot(self.Wz.T, concat_hx) + self.bz)\n",
    "\n",
    "            concat_hrx = np.concatenate((np.multiply(r[t], h[t-1]), x))\n",
    "            h_hat[t] = tanh(np.dot(self.Wh.T, concat_hrx) + self.bh)\n",
    "            h[t] = np.multiply(z[t], h[t-1]) + np.multiply(1 - z[t], h_hat[t])\n",
    "\n",
    "        y = np.dot(self.Wy, h[t]) + self.by\n",
    "        probDis = softmax(y)\n",
    "\n",
    "        return z, r, h_hat, h, y, probDis\n",
    "\n",
    "    def backward_pass(self, z, r, h_hat, h, y, probDis, inputs, targets):\n",
    "        dWy, dWh, dWr, dWz = np.zeros_like(self.Wy), np.zeros_like(self.Wh), np.zeros_like(self.Wr), np.zeros_like(self.Wz)\n",
    "        dby, dbh, dbr, dbz = np.zeros_like(self.by), np.zeros_like(self.bh), np.zeros_like(self.br), np.zeros_like(self.bz)\n",
    "        \n",
    "        loss = 0\n",
    "        dy = probDis.copy()\n",
    "        dy[targets] -= 1\n",
    "        dWy += np.dot(dy, h[len(inputs) -1].T)\n",
    "        dby += dy\n",
    "        dhnext = np.dot(self.Wy.T, dy)\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "\n",
    "            loss += -np.sum(x * np.log(probDis + 1e-9))\n",
    "\n",
    "            dh = dhnext\n",
    "            dh_hat = np.multiply(dh, 1 - z[t])\n",
    "            dh_hat_raw = np.multiply(dh_hat, tanh(h_hat[t], deriv=True))\n",
    "            dWh += np.dot(np.concatenate((r[t] * h[t-1], x), axis=0), dh_hat_raw.T)\n",
    "            dbh += dh_hat_raw\n",
    "\n",
    "            dr = np.dot(self.Wh[:self.h_size, :].T, dh_hat_raw) * h[t-1]\n",
    "            dr_raw = np.multiply(dr, sigmoid(r[t], deriv=True))\n",
    "            dWr += np.dot(np.concatenate((h[t-1], x), axis=0), dr_raw.T)\n",
    "            dbr += dr_raw\n",
    "\n",
    "            dz = np.multiply(dh, h_hat[t] - h[t-1])\n",
    "            dz_raw = np.multiply(dz, sigmoid(z[t], deriv=True))\n",
    "            dWz += np.dot(np.concatenate((h[t-1], x), axis=0), dz_raw.T)\n",
    "            dbz += dz_raw\n",
    "\n",
    "            dhprev = np.dot(self.Wz[:self.h_size, :].T, dz_raw) + np.dot(self.Wr[:self.h_size, :].T, dr_raw) + np.dot(self.Wh[:self.h_size, :].T, dh_hat_raw)\n",
    "            dhnext = dhprev\n",
    "\n",
    "        for param, dparam, mem in zip([self.Wy, self.by, self.Wh, self.bh, self.Wr, self.br, self.Wz, self.bz], \n",
    "                                      [dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz], \n",
    "                                      [self.mdWy, self.mdby, self.mdWh, self.mdbh, self.mdWr, self.mdbr, self.mdWz, self.mdbz]):\n",
    "            mem += dparam * dparam\n",
    "            param -= self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, inputs, targets, n_iters=100):\n",
    "        for j in range(n_iters):\n",
    "            for i in range(len(inputs)):\n",
    "                z, r, h_hat, h, y, probDis = self.forward_pass(inputs[i])\n",
    "                tgt = np.zeros((self.vocab_size, 1))\n",
    "                tgt[targets] = 1\n",
    "                loss = self.backward_pass(z, r, h_hat, h, y, probDis, inputs[i], targets[i])\n",
    "            print(f\"iteration: {j} loss: {loss}\")\n",
    "\n",
    "    def generate_text(self, inputs):\n",
    "        _, _, _, _, _, probDis = self.forward_pass(inputs)\n",
    "        return np.argmax(probDis, axis=0)\n",
    "    \n",
    "    # def generate_text(self, inputs, length):\n",
    "    #     indexes = []\n",
    "    #     for i in range(length):\n",
    "    #         _, _, _, _, _, probDis = self.forward_pass(inputs)\n",
    "    #         output = np.argmax(probDis, axis=0)\n",
    "    #         indexes.append(output)\n",
    "    #         input\n",
    "    #     return indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 57254 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('input.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "seq_length = 5\n",
    "oo = 0\n",
    "for k in range(0, len(data) - seq_length):\n",
    "    input_seq = [char_to_ix[ch] for ch in data[k:k+seq_length]]\n",
    "    target_char = char_to_ix[data[k+seq_length]]\n",
    "    inputs.append(input_seq)\n",
    "    targets.append(target_char)\n",
    "\n",
    "inputs = inputs[:100]\n",
    "targets = targets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 12.442233905744724\n",
      "iteration: 1 loss: 12.411576581812268\n",
      "iteration: 2 loss: 12.568232889896482\n",
      "iteration: 3 loss: 12.707263866052214\n",
      "iteration: 4 loss: 12.823577926477064\n",
      "iteration: 5 loss: 12.916729800686902\n",
      "iteration: 6 loss: 12.990486162511754\n",
      "iteration: 7 loss: 13.049087962612994\n",
      "iteration: 8 loss: 13.095983120826988\n",
      "iteration: 9 loss: 13.133741043160867\n",
      "iteration: 10 loss: 13.16424037965495\n",
      "iteration: 11 loss: 13.18886522813448\n",
      "iteration: 12 loss: 13.208655715063756\n",
      "iteration: 13 loss: 13.224413682789418\n",
      "iteration: 14 loss: 13.23677418544834\n",
      "iteration: 15 loss: 13.246252863780699\n",
      "iteration: 16 loss: 13.253277001614274\n",
      "iteration: 17 loss: 13.258205961424357\n",
      "iteration: 18 loss: 13.261344957422967\n",
      "iteration: 19 loss: 13.262954756954565\n",
      "iteration: 20 loss: 13.263258902537979\n",
      "iteration: 21 loss: 13.262449380809626\n",
      "iteration: 22 loss: 13.260691257569734\n",
      "iteration: 23 loss: 13.258126567600467\n",
      "iteration: 24 loss: 13.254877625086472\n",
      "iteration: 25 loss: 13.251049857790099\n",
      "iteration: 26 loss: 13.24673423675952\n",
      "iteration: 27 loss: 13.242009357299759\n",
      "iteration: 28 loss: 13.2369432179594\n",
      "iteration: 29 loss: 13.23159473835559\n",
      "iteration: 30 loss: 13.22601505194398\n",
      "iteration: 31 loss: 13.220248605592165\n",
      "iteration: 32 loss: 13.214334093811285\n",
      "iteration: 33 loss: 13.208305251747088\n",
      "iteration: 34 loss: 13.202191527619679\n",
      "iteration: 35 loss: 13.196018652324524\n",
      "iteration: 36 loss: 13.189809121420568\n",
      "iteration: 37 loss: 13.183582602727022\n",
      "iteration: 38 loss: 13.177356281155205\n",
      "iteration: 39 loss: 13.171145151092631\n",
      "iteration: 40 loss: 13.164962265494353\n",
      "iteration: 41 loss: 13.158818949706525\n",
      "iteration: 42 loss: 13.15272498688804\n",
      "iteration: 43 loss: 13.146688780710113\n",
      "iteration: 44 loss: 13.140717499850213\n",
      "iteration: 45 loss: 13.134817207724867\n",
      "iteration: 46 loss: 13.128992979985593\n",
      "iteration: 47 loss: 13.123249011567603\n",
      "iteration: 48 loss: 13.117588714535241\n",
      "iteration: 49 loss: 13.112014807591718\n",
      "iteration: 50 loss: 13.106529397880134\n",
      "iteration: 51 loss: 13.101134055561722\n",
      "iteration: 52 loss: 13.095829881583118\n",
      "iteration: 53 loss: 13.090617569011032\n",
      "iteration: 54 loss: 13.085497458301534\n",
      "iteration: 55 loss: 13.080469586870217\n",
      "iteration: 56 loss: 13.075533733331406\n",
      "iteration: 57 loss: 13.07068945677568\n",
      "iteration: 58 loss: 13.065936131453652\n",
      "iteration: 59 loss: 13.061272977229702\n",
      "iteration: 60 loss: 13.056699086162148\n",
      "iteration: 61 loss: 13.052213445556966\n",
      "iteration: 62 loss: 13.047814957830534\n",
      "iteration: 63 loss: 13.043502457503703\n",
      "iteration: 64 loss: 13.03927472563472\n",
      "iteration: 65 loss: 13.035130501982911\n",
      "iteration: 66 loss: 13.031068495178063\n",
      "iteration: 67 loss: 13.02708739115272\n",
      "iteration: 68 loss: 13.02318586007643\n",
      "iteration: 69 loss: 13.019362562011711\n",
      "iteration: 70 loss: 13.01561615149241\n",
      "iteration: 71 loss: 13.011945281205204\n",
      "iteration: 72 loss: 13.008348604935703\n",
      "iteration: 73 loss: 13.004824779920858\n",
      "iteration: 74 loss: 13.001372468730722\n",
      "iteration: 75 loss: 12.997990340784275\n",
      "iteration: 76 loss: 12.994677073586969\n",
      "iteration: 77 loss: 12.991431353761705\n",
      "iteration: 78 loss: 12.98825187793054\n",
      "iteration: 79 loss: 12.98513735349157\n",
      "iteration: 80 loss: 12.982086499324236\n",
      "iteration: 81 loss: 12.979098046446891\n",
      "iteration: 82 loss: 12.976170738642484\n",
      "iteration: 83 loss: 12.973303333062121\n",
      "iteration: 84 loss: 12.970494600811195\n",
      "iteration: 85 loss: 12.967743327519317\n",
      "iteration: 86 loss: 12.9650483138927\n",
      "iteration: 87 loss: 12.962408376245982\n",
      "iteration: 88 loss: 12.959822347009677\n",
      "iteration: 89 loss: 12.957289075209012\n",
      "iteration: 90 loss: 12.954807426910083\n",
      "iteration: 91 loss: 12.95237628562964\n",
      "iteration: 92 loss: 12.949994552705336\n",
      "iteration: 93 loss: 12.947661147624125\n",
      "iteration: 94 loss: 12.945375008307014\n",
      "iteration: 95 loss: 12.943135091349365\n",
      "iteration: 96 loss: 12.94094037221639\n",
      "iteration: 97 loss: 12.938789845394293\n",
      "iteration: 98 loss: 12.936682524497956\n",
      "iteration: 99 loss: 12.934617442336625\n"
     ]
    }
   ],
   "source": [
    "neee = GRUModel(vocab_size, 10)\n",
    "neee.train(inputs, targets,100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "hh = neee.generate_text(inputs[2])\n",
    "print(ix_to_char[targets[5]])\n",
    "print(hh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
