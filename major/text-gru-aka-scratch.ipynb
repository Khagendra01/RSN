{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.Wz = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uz = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bz = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wr = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Ur = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.br = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wh = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uh = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # Store values for backpropagation\n",
    "        self.x, self.h_prev = x, h_prev\n",
    "        \n",
    "        # Update gate\n",
    "        self.z = sigmoid(np.dot(self.Wz, x) + np.dot(self.Uz, h_prev) + self.bz)\n",
    "        \n",
    "        # Reset gate\n",
    "        self.r = sigmoid(np.dot(self.Wr, x) + np.dot(self.Ur, h_prev) + self.br)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.h_tilde = tanh(np.dot(self.Wh, x) + np.dot(self.Uh, self.r * h_prev) + self.bh)\n",
    "        \n",
    "        # Final hidden state\n",
    "        h_next = self.z * h_prev + (1 - self.z) * self.h_tilde\n",
    "        \n",
    "        # Output\n",
    "        y_pred = np.dot(self.Wy, h_next) + self.by\n",
    "        \n",
    "        return y_pred, h_next\n",
    "\n",
    "    def backward(self, d_y_pred, d_h_next):\n",
    "        # Gradient of the output layer\n",
    "        d_Wy = np.dot(d_y_pred, self.h_prev.T)\n",
    "        d_by = d_y_pred.sum(axis=1, keepdims=True)\n",
    "        d_h_next += np.dot(self.Wy.T, d_y_pred)\n",
    "        \n",
    "        # Derivative of final hidden state\n",
    "        d_z = d_h_next * (self.h_prev - self.h_tilde)\n",
    "        d_h_prev = d_h_next * self.z\n",
    "        d_h_tilde = d_h_next * (1 - self.z)\n",
    "        \n",
    "        # Derivatives of the gates\n",
    "        d_h_tilde_raw = d_h_tilde * tanh_derivative(self.h_tilde)\n",
    "        d_r = np.dot(self.Uh.T, d_h_tilde_raw) * self.h_prev\n",
    "        d_h_prev += np.dot(self.Uh.T, d_h_tilde_raw) * self.r\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wh -= self.learning_rate * np.dot(d_h_tilde_raw, self.x.T)\n",
    "        self.Uh -= self.learning_rate * np.dot(d_h_tilde_raw, (self.r * self.h_prev).T)\n",
    "        self.bh -= self.learning_rate * d_h_tilde_raw.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wr -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.x.T)\n",
    "        self.Ur -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.h_prev.T)\n",
    "        self.br -= self.learning_rate * (d_r * sigmoid_derivative(self.r)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.x.T)\n",
    "        self.Uz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.h_prev.T)\n",
    "        self.bz -= self.learning_rate * (d_z * sigmoid_derivative(self.z)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wy -= self.learning_rate * d_Wy\n",
    "        self.by -= self.learning_rate * d_by\n",
    "\n",
    "        return d_h_prev\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x, y_true in zip(inputs, targets):\n",
    "                x = x.reshape(-1, 1)  # Reshape x to (input_size, 1)\n",
    "                y_true = y_true.reshape(-1, 1)  # Reshape y_true if needed\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, h_next = self.forward(x, h_prev)\n",
    "\n",
    "                # Calculate loss (for monitoring)\n",
    "                loss += mse_loss(y_pred, y_true)\n",
    "\n",
    "                # Backpropagate error\n",
    "                d_loss = mse_loss_derivative(y_pred, y_true)\n",
    "                d_h_next = self.backward(d_loss, h_next)\n",
    "                h_prev = h_next  # update state\n",
    "                \n",
    "            loss /= len(inputs)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        predictions = []\n",
    "        for x in inputs:\n",
    "            x = x.reshape(-1, 1)  # Ensure x is correctly shaped\n",
    "            y_pred, h_prev = self.forward(x, h_prev)\n",
    "            predictions.append(y_pred.flatten())  # Flatten the prediction to 1-D\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import regex as re\n",
    "from gensim.models import FastText\n",
    "\n",
    "def file_to_sentence_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def load_fasttext_model(sentences, vector_size=50, window=10, min_count=2):\n",
    "    fasttext_model = FastText(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return fasttext_model\n",
    "\n",
    "def create_embedding_matrix(word_index, fasttext_model):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, fasttext_model.vector_size))\n",
    "    for word, i in word_index.items():\n",
    "        if word in fasttext_model.wv:\n",
    "            embedding_matrix[i] = fasttext_model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Read and process the text data\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences and prepare input data\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Load or train FastText model\n",
    "sentences = [text.split() for text in text_data]\n",
    "fasttext_model = load_fasttext_model(sentences)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.9529582357766996e-05\n",
      "Epoch 2/10, Loss: 1.8279929504754862e-05\n",
      "Epoch 3/10, Loss: 1.7969110205282275e-05\n",
      "Epoch 4/10, Loss: 1.7877931302374975e-05\n",
      "Epoch 5/10, Loss: 1.7850743048232447e-05\n",
      "Epoch 6/10, Loss: 1.7842298978598402e-05\n",
      "Epoch 7/10, Loss: 1.7839396555843233e-05\n",
      "Epoch 8/10, Loss: 1.7838157742109638e-05\n",
      "Epoch 9/10, Loss: 1.7837431186776915e-05\n",
      "Epoch 10/10, Loss: 1.7836868978637277e-05\n"
     ]
    }
   ],
   "source": [
    "# Assume X and y are your input and output sequences from the first code block\n",
    "# X has indices that need to be transformed into embeddings\n",
    "\n",
    "# Transform each index in X to its corresponding embedding\n",
    "X_embeddings = np.array([np.mean([embedding_matrix[idx] for idx in sequence if idx > 0], axis=0) for sequence in X])\n",
    "y_embeddings = np.array([embedding_matrix[target] for target in y])  # only if y is a sequence of indices\n",
    "\n",
    "# Initialize the GRU\n",
    "input_size = embedding_matrix.shape[1]  # Embedding size\n",
    "hidden_size = 100  # Choose your hidden size\n",
    "output_size = embedding_matrix.shape[1]  # Output size is same as input if you're predicting next word embeddings\n",
    "\n",
    "gru = GRUCell(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "gru.train(X_embeddings, y_embeddings, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: the\n"
     ]
    }
   ],
   "source": [
    "# Example input sequence from the dataset for prediction\n",
    "# Assuming `X_embeddings` is prepared as before\n",
    "test_input = X_embeddings[50]  # Example input embedding\n",
    "predicted_embedding = gru.predict([test_input])[0]  # Predict the embedding for the next word\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def find_closest_word(embedding, embedding_matrix, tokenizer):\n",
    "    # Ensure embedding is 1-D\n",
    "    embedding = embedding.flatten()  # This line ensures the embedding is 1-D\n",
    "    # Compute cosine similarity between the predicted embedding and all embeddings in the matrix\n",
    "    similarities = [1 - distance.cosine(embedding, word_embedding) if not np.all(word_embedding == 0) else -np.inf for word_embedding in embedding_matrix]\n",
    "    closest_word_index = np.argmax(similarities)  # Get the index of the closest embedding\n",
    "    return tokenizer.index_word[closest_word_index]  # Convert index to word\n",
    "\n",
    "predicted_word = find_closest_word(predicted_embedding, embedding_matrix, tokenizer)\n",
    "print(\"Predicted word:\", predicted_word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
