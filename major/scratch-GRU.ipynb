{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.Wz = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uz = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bz = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wr = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Ur = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.br = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wh = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uh = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # Store values for backpropagation\n",
    "        self.x, self.h_prev = x, h_prev\n",
    "        \n",
    "        # Update gate\n",
    "        self.z = sigmoid(np.dot(self.Wz, x) + np.dot(self.Uz, h_prev) + self.bz)\n",
    "        \n",
    "        # Reset gate\n",
    "        self.r = sigmoid(np.dot(self.Wr, x) + np.dot(self.Ur, h_prev) + self.br)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.h_tilde = tanh(np.dot(self.Wh, x) + np.dot(self.Uh, self.r * h_prev) + self.bh)\n",
    "        \n",
    "        # Final hidden state\n",
    "        h_next = self.z * h_prev + (1 - self.z) * self.h_tilde\n",
    "        \n",
    "        # Output\n",
    "        y_pred = np.dot(self.Wy, h_next) + self.by\n",
    "        \n",
    "        return y_pred, h_next\n",
    "\n",
    "    def backward(self, d_y_pred, d_h_next):\n",
    "        # Gradient of the output layer\n",
    "        d_Wy = np.dot(d_y_pred, self.h_prev.T)\n",
    "        d_by = d_y_pred.sum(axis=1, keepdims=True)\n",
    "        d_h_next += np.dot(self.Wy.T, d_y_pred)\n",
    "        \n",
    "        # Derivative of final hidden state\n",
    "        d_z = d_h_next * (self.h_prev - self.h_tilde)\n",
    "        d_h_prev = d_h_next * self.z\n",
    "        d_h_tilde = d_h_next * (1 - self.z)\n",
    "        \n",
    "        # Derivatives of the gates\n",
    "        d_h_tilde_raw = d_h_tilde * tanh_derivative(self.h_tilde)\n",
    "        d_r = np.dot(self.Uh.T, d_h_tilde_raw) * self.h_prev\n",
    "        d_h_prev += np.dot(self.Uh.T, d_h_tilde_raw) * self.r\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wh -= self.learning_rate * np.dot(d_h_tilde_raw, self.x.T)\n",
    "        self.Uh -= self.learning_rate * np.dot(d_h_tilde_raw, (self.r * self.h_prev).T)\n",
    "        self.bh -= self.learning_rate * d_h_tilde_raw.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wr -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.x.T)\n",
    "        self.Ur -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.h_prev.T)\n",
    "        self.br -= self.learning_rate * (d_r * sigmoid_derivative(self.r)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.x.T)\n",
    "        self.Uz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.h_prev.T)\n",
    "        self.bz -= self.learning_rate * (d_z * sigmoid_derivative(self.z)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wy -= self.learning_rate * d_Wy\n",
    "        self.by -= self.learning_rate * d_by\n",
    "\n",
    "        return d_h_prev\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x, y_true in zip(inputs, targets):\n",
    "                x = x.reshape(-1, 1)  # Reshape x to (input_size, 1)\n",
    "                y_true = y_true.reshape(-1, 1)  # Reshape y_true if needed\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, h_next = self.forward(x, h_prev)\n",
    "\n",
    "                # Calculate loss (for monitoring)\n",
    "                loss += mse_loss(y_pred, y_true)\n",
    "\n",
    "                # Backpropagate error\n",
    "                d_loss = mse_loss_derivative(y_pred, y_true)\n",
    "                d_h_next = self.backward(d_loss, h_next)\n",
    "                h_prev = h_next  # update state\n",
    "                \n",
    "            loss /= len(inputs)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        predictions = []\n",
    "        for x in inputs:\n",
    "            y_pred, h_prev = self.forward(x, h_prev)\n",
    "            predictions.append(y_pred)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.49976744e-03,  7.29218265e-03,  4.88142017e-03, -1.59843161e-03,\n",
       "       -7.68687529e-03,  4.20675101e-03,  2.33893079e-04,  4.88119293e-03,\n",
       "       -7.03939749e-03,  4.98013978e-04,  8.66527203e-03,  4.07988671e-03,\n",
       "        4.26494470e-03,  5.59851248e-03, -4.26932983e-03,  8.57298914e-03,\n",
       "        3.46481218e-03,  5.47068077e-04, -3.36522679e-03,  3.29037360e-03,\n",
       "       -2.19735340e-03,  7.42712198e-03,  1.07129160e-02,  6.01022411e-03,\n",
       "       -4.13716538e-03, -8.59425310e-03,  1.25437637e-03, -5.38942311e-03,\n",
       "       -1.51282791e-02,  1.38071096e-02, -2.01411266e-03,  3.29516060e-03,\n",
       "        7.93049484e-03,  4.06214455e-03, -7.17765139e-03,  2.65611976e-04,\n",
       "       -3.19421059e-03,  4.60048299e-03,  5.45671093e-04, -6.63513830e-03,\n",
       "        1.71164458e-03,  7.17108836e-03,  2.42489041e-03,  8.56506173e-04,\n",
       "        4.22316510e-03,  6.05027053e-05, -6.69620885e-03,  3.46953631e-03,\n",
       "        1.02576921e-02,  1.03530074e-02])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import regex as re\n",
    "from gensim.models import FastText\n",
    "\n",
    "def file_to_sentence_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def load_fasttext_model(sentences, vector_size=50, window=10, min_count=2):\n",
    "    fasttext_model = FastText(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return fasttext_model\n",
    "\n",
    "def create_embedding_matrix(word_index, fasttext_model):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, fasttext_model.vector_size))\n",
    "    for word, i in word_index.items():\n",
    "        if word in fasttext_model.wv:\n",
    "            embedding_matrix[i] = fasttext_model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Read and process the text data\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences and prepare input data\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Load or train FastText model\n",
    "sentences = [text.split() for text in text_data]\n",
    "fasttext_model = load_fasttext_model(sentences)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.utils.to_categorical(X, num_classes=total_words) \n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) \n",
    "\n",
    "gru_cell = GRUCell(687, 50, 1, 0.01)\n",
    "gru_cell.train(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0596082552571222\n",
      "Epoch 2/10, Loss: 1.0584883203403603\n",
      "Epoch 3/10, Loss: 1.0584838045056038\n",
      "Epoch 4/10, Loss: 1.0585921021946478\n",
      "Epoch 5/10, Loss: 1.0591553482651261\n",
      "Epoch 6/10, Loss: 1.0611140359556828\n",
      "Epoch 7/10, Loss: 1.0676344240207052\n",
      "Epoch 8/10, Loss: 1.0811678255366883\n",
      "Epoch 9/10, Loss: 1.096098377565819\n",
      "Epoch 10/10, Loss: 1.1009029064531466\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 10\n",
    "output_size = 1  \n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Example synthetic data\n",
    "inputs = [np.random.randn(input_size, 1) for _ in range(1000)]\n",
    "targets = [np.random.randn(output_size, 1) for _ in range(1000)]\n",
    "gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "gru_cell.train(inputs, targets, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0][0])\n",
    "# 1000 x 10 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\gru.py:144: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "32/32 [==============================] - 2s 2ms/step - loss: 1.0929\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0336\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0305\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0257\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0136\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0070\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0044\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9971\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9874\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9865\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 10)                660       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 671 (2.62 KB)\n",
      "Trainable params: 671 (2.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert lists to numpy arrays for TensorFlow compatibility\n",
    "inputs = np.array(inputs).reshape(-1, 1, input_size)  # Reshape to [batch, timesteps, feature]\n",
    "targets = np.array(targets).reshape(-1, output_size)\n",
    "\n",
    "# Define the GRU model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(hidden_size, input_shape=(1, input_size)),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='mse')  # Mean Squared Error for regression tasks\n",
    "\n",
    "# Train the model\n",
    "model.fit(inputs, targets, epochs=epochs)\n",
    "\n",
    "# Optionally, you can print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import tensorflow as tf \n",
    "import regex as re\n",
    "\n",
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array(X)\n",
    "inputs = np.array([input.reshape(39, 1) for input in inputs])\n",
    "\n",
    "targets = np.array(y)\n",
    "targets = np.array([target.reshape(687, 1) for target in targets])\n",
    "\n",
    "input_size = 39\n",
    "hidden_size = 39\n",
    "output_size = 687\n",
    "epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "# gru_cell.train(inputs, targets, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_9556\\3916400805.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3929088192563519\n",
      "Epoch 2/10, Loss: 0.37260910377869355\n",
      "Epoch 3/10, Loss: 0.3563130370286396\n",
      "Epoch 4/10, Loss: 0.33710501142315236\n",
      "Epoch 5/10, Loss: 0.31950367296388943\n",
      "Epoch 6/10, Loss: 0.3052859616931766\n",
      "Epoch 7/10, Loss: 0.28946108472447746\n",
      "Epoch 8/10, Loss: 0.27455625091016816\n",
      "Epoch 9/10, Loss: 0.2614426393245619\n",
      "Epoch 10/10, Loss: 0.24719109009223214\n"
     ]
    }
   ],
   "source": [
    "# predicted_probs = gru_cell.predict(X[10]) \n",
    "# val = predicted_probs[0]\n",
    "# val.shape\n",
    "gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "gru_cell.train(inputs, targets, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
