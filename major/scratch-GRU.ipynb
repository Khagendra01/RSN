{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.Wz = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uz = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bz = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wr = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Ur = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.br = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wh = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uh = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # Store values for backpropagation\n",
    "        self.x, self.h_prev = x, h_prev\n",
    "        \n",
    "        # Update gate\n",
    "        self.z = sigmoid(np.dot(self.Wz, x) + np.dot(self.Uz, h_prev) + self.bz)\n",
    "        \n",
    "        # Reset gate\n",
    "        self.r = sigmoid(np.dot(self.Wr, x) + np.dot(self.Ur, h_prev) + self.br)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.h_tilde = tanh(np.dot(self.Wh, x) + np.dot(self.Uh, self.r * h_prev) + self.bh)\n",
    "        \n",
    "        # Final hidden state\n",
    "        h_next = self.z * h_prev + (1 - self.z) * self.h_tilde\n",
    "        \n",
    "        # Output\n",
    "        y_pred = np.dot(self.Wy, h_next) + self.by\n",
    "        \n",
    "        return y_pred, h_next\n",
    "\n",
    "    def backward(self, d_y_pred, d_h_next):\n",
    "        # Gradient of the output layer\n",
    "        d_Wy = np.dot(d_y_pred, self.h_prev.T)\n",
    "        d_by = d_y_pred.sum(axis=1, keepdims=True)\n",
    "        d_h_next += np.dot(self.Wy.T, d_y_pred)\n",
    "        \n",
    "        # Derivative of final hidden state\n",
    "        d_z = d_h_next * (self.h_prev - self.h_tilde)\n",
    "        d_h_prev = d_h_next * self.z\n",
    "        d_h_tilde = d_h_next * (1 - self.z)\n",
    "        \n",
    "        # Derivatives of the gates\n",
    "        d_h_tilde_raw = d_h_tilde * tanh_derivative(self.h_tilde)\n",
    "        d_r = np.dot(self.Uh.T, d_h_tilde_raw) * self.h_prev\n",
    "        d_h_prev += np.dot(self.Uh.T, d_h_tilde_raw) * self.r\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wh -= self.learning_rate * np.dot(d_h_tilde_raw, self.x.T)\n",
    "        self.Uh -= self.learning_rate * np.dot(d_h_tilde_raw, (self.r * self.h_prev).T)\n",
    "        self.bh -= self.learning_rate * d_h_tilde_raw.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wr -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.x.T)\n",
    "        self.Ur -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.h_prev.T)\n",
    "        self.br -= self.learning_rate * (d_r * sigmoid_derivative(self.r)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.x.T)\n",
    "        self.Uz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.h_prev.T)\n",
    "        self.bz -= self.learning_rate * (d_z * sigmoid_derivative(self.z)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wy -= self.learning_rate * d_Wy\n",
    "        self.by -= self.learning_rate * d_by\n",
    "\n",
    "        return d_h_prev\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x, y_true in zip(inputs, targets):\n",
    "                x = x.reshape(-1, 1)  # Reshape x to (input_size, 1)\n",
    "                y_true = y_true.reshape(-1, 1)  # Reshape y_true if needed\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, h_next = self.forward(x, h_prev)\n",
    "\n",
    "                # Calculate loss (for monitoring)\n",
    "                loss += mse_loss(y_pred, y_true)\n",
    "\n",
    "                # Backpropagate error\n",
    "                d_loss = mse_loss_derivative(y_pred, y_true)\n",
    "                d_h_next = self.backward(d_loss, h_next)\n",
    "                h_prev = h_next  # update state\n",
    "                \n",
    "            loss /= len(inputs)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        y_pred, h_prev = self.forward(inputs, h_prev)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import regex as re\n",
    "from gensim.models import FastText\n",
    "\n",
    "def file_to_sentence_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = [sentence.strip() for sentence in re.split(r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def load_fasttext_model(sentences, vector_size=50, window=10, min_count=2):\n",
    "    fasttext_model = FastText(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return fasttext_model\n",
    "\n",
    "def create_embedding_matrix(word_index, fasttext_model):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, fasttext_model.vector_size))\n",
    "    for word, i in word_index.items():\n",
    "        if word in fasttext_model.wv:\n",
    "            embedding_matrix[i] = fasttext_model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Read and process the text data\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences and prepare input data\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Load or train FastText model\n",
    "sentences = [text.split() for text in text_data]\n",
    "fasttext_model = load_fasttext_model(sentences)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.keras.utils.to_categorical(X, num_classes=total_words) \n",
    "# # Convert target data to one-hot encoding \n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=total_words) \n",
    "\n",
    "# gru_cell = GRUCell(687, 50, 1, 0.01)\n",
    "# gru_cell.train(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8426334644902557\n",
      "Epoch 2/100, Loss: 0.7734987152296402\n",
      "Epoch 3/100, Loss: 0.7696668019041557\n",
      "Epoch 4/100, Loss: 0.7694567880791351\n",
      "Epoch 5/100, Loss: 0.7699106200920199\n",
      "Epoch 6/100, Loss: 0.7706354511851271\n",
      "Epoch 7/100, Loss: 0.7714825391675144\n",
      "Epoch 8/100, Loss: 0.772346833238512\n",
      "Epoch 9/100, Loss: 0.7731621399616222\n",
      "Epoch 10/100, Loss: 0.7738947988854692\n",
      "Epoch 11/100, Loss: 0.7745330967095606\n",
      "Epoch 12/100, Loss: 0.7750780431292794\n",
      "Epoch 13/100, Loss: 0.7755370549883354\n",
      "Epoch 14/100, Loss: 0.7759201541022281\n",
      "Epoch 15/100, Loss: 0.7762378748476428\n",
      "Epoch 16/100, Loss: 0.7765002088310236\n",
      "Epoch 17/100, Loss: 0.7767161376991887\n",
      "Epoch 18/100, Loss: 0.7768934836738992\n",
      "Epoch 19/100, Loss: 0.7770389233358357\n",
      "Epoch 20/100, Loss: 0.7771580789579766\n",
      "Epoch 21/100, Loss: 0.777255640807083\n",
      "Epoch 22/100, Loss: 0.7773354957180022\n",
      "Epoch 23/100, Loss: 0.7774008494374246\n",
      "Epoch 24/100, Loss: 0.7774543370090153\n",
      "Epoch 25/100, Loss: 0.7774981191999305\n",
      "Epoch 26/100, Loss: 0.777533964964119\n",
      "Epoch 27/100, Loss: 0.7775633209410389\n",
      "Epoch 28/100, Loss: 0.7775873694253701\n",
      "Epoch 29/100, Loss: 0.7776070763629913\n",
      "Epoch 30/100, Loss: 0.777623230878111\n",
      "Epoch 31/100, Loss: 0.7776364777026707\n",
      "Epoch 32/100, Loss: 0.7776473437122317\n",
      "Epoch 33/100, Loss: 0.7776562596007572\n",
      "Epoch 34/100, Loss: 0.7776635775648457\n",
      "Epoch 35/100, Loss: 0.7776695857229582\n",
      "Epoch 36/100, Loss: 0.7776745198692985\n",
      "Epoch 37/100, Loss: 0.777678573055047\n",
      "Epoch 38/100, Loss: 0.7776819034000888\n",
      "Epoch 39/100, Loss: 0.777684640464145\n",
      "Epoch 40/100, Loss: 0.777686890445168\n",
      "Epoch 41/100, Loss: 0.7776887404228932\n",
      "Epoch 42/100, Loss: 0.7776902618246787\n",
      "Epoch 43/100, Loss: 0.7776915132576379\n",
      "Epoch 44/100, Loss: 0.7776925428241073\n",
      "Epoch 45/100, Loss: 0.77769339001566\n",
      "Epoch 46/100, Loss: 0.7776940872631088\n",
      "Epoch 47/100, Loss: 0.7776946612055653\n",
      "Epoch 48/100, Loss: 0.7776951337299234\n",
      "Epoch 49/100, Loss: 0.7776955228226331\n",
      "Epoch 50/100, Loss: 0.777695843267932\n",
      "Epoch 51/100, Loss: 0.7776961072204089\n",
      "Epoch 52/100, Loss: 0.7776963246746647\n",
      "Epoch 53/100, Loss: 0.7776965038506981\n",
      "Epoch 54/100, Loss: 0.7776966515102026\n",
      "Epoch 55/100, Loss: 0.7776967732162587\n",
      "Epoch 56/100, Loss: 0.7776968735465759\n",
      "Epoch 57/100, Loss: 0.7776969562686505\n",
      "Epoch 58/100, Loss: 0.7776970244836718\n",
      "Epoch 59/100, Loss: 0.7776970807447685\n",
      "Epoch 60/100, Loss: 0.7776971271541956\n",
      "Epoch 61/100, Loss: 0.7776971654432289\n",
      "Epoch 62/100, Loss: 0.7776971970378481\n",
      "Epoch 63/100, Loss: 0.7776972231127592\n",
      "Epoch 64/100, Loss: 0.7776972446358157\n",
      "Epoch 65/100, Loss: 0.777697262404569\n",
      "Epoch 66/100, Loss: 0.777697277076332\n",
      "Epoch 67/100, Loss: 0.7776972891929261\n",
      "Epoch 68/100, Loss: 0.7776972992010388\n",
      "Epoch 69/100, Loss: 0.7776973074689859\n",
      "Epoch 70/100, Loss: 0.7776973143005101\n",
      "Epoch 71/100, Loss: 0.7776973199461399\n",
      "Epoch 72/100, Loss: 0.7776973246125476\n",
      "Epoch 73/100, Loss: 0.7776973284702496\n",
      "Epoch 74/100, Loss: 0.7776973316599601\n",
      "Epoch 75/100, Loss: 0.7776973342978115\n",
      "Epoch 76/100, Loss: 0.7776973364796721\n",
      "Epoch 77/100, Loss: 0.7776973382846885\n",
      "Epoch 78/100, Loss: 0.7776973397782152\n",
      "Epoch 79/100, Loss: 0.7776973410142302\n",
      "Epoch 80/100, Loss: 0.7776973420373183\n",
      "Epoch 81/100, Loss: 0.7776973428843141\n",
      "Epoch 82/100, Loss: 0.7776973435856547\n",
      "Epoch 83/100, Loss: 0.7776973441664939\n",
      "Epoch 84/100, Loss: 0.777697344647625\n",
      "Epoch 85/100, Loss: 0.7776973450462363\n",
      "Epoch 86/100, Loss: 0.7776973453765431\n",
      "Epoch 87/100, Loss: 0.7776973456502997\n",
      "Epoch 88/100, Loss: 0.7776973458772296\n",
      "Epoch 89/100, Loss: 0.7776973460653778\n",
      "Epoch 90/100, Loss: 0.7776973462214015\n",
      "Epoch 91/100, Loss: 0.7776973463508081\n",
      "Epoch 92/100, Loss: 0.7776973464581596\n",
      "Epoch 93/100, Loss: 0.777697346547231\n",
      "Epoch 94/100, Loss: 0.7776973466211475\n",
      "Epoch 95/100, Loss: 0.7776973466825017\n",
      "Epoch 96/100, Loss: 0.777697346733436\n",
      "Epoch 97/100, Loss: 0.7776973467757279\n",
      "Epoch 98/100, Loss: 0.777697346810851\n",
      "Epoch 99/100, Loss: 0.7776973468400246\n",
      "Epoch 100/100, Loss: 0.7776973468642613\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 1  \n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Example synthetic data\n",
    "inputs = np.random.randn(100, input_size)\n",
    "targets = np.random.randn(100, output_size)\n",
    "gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "gru_cell.train(inputs, targets, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52175082 -1.04291614  1.32487184 -0.81651301  0.67118204  1.66832641\n",
      " -0.64027934 -0.40204368  0.08868526  0.39260631]\n",
      "[[0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527]]\n",
      "[1.31951572]\n",
      "[-1.0020422  -0.62832827 -0.9667176  -1.82481963 -0.94748438 -0.45674069\n",
      " -1.18439951  0.83604301 -1.40795628 -1.07746102]\n",
      "[[0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527]]\n",
      "[0.09298336]\n",
      "[-0.98956853 -1.38083323  0.56325249 -1.48848276 -1.16414646 -0.24037772\n",
      "  0.19020144 -0.47704574  0.45655185 -1.88914022]\n",
      "[[0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527]]\n",
      "[-0.7929329]\n",
      "[ 1.01754082  0.03364328 -0.51339864  0.92458272 -1.29378447  1.22474178\n",
      " -0.00564959 -0.27981199  0.14524582  0.08664068]\n",
      "[[0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527]]\n",
      "[-1.26356901]\n",
      "[ 1.4547041  -0.36532126 -1.01239741 -0.85331238 -0.01635971 -0.71365142\n",
      " -0.18517328 -0.70525049 -1.60302334  1.40710084]\n",
      "[[0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527 0.06054527 0.06054527 0.06054527 0.06054527\n",
      "  0.06054527 0.06054527]]\n",
      "[-2.17975896]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(inputs[i])\n",
    "    print(gru_cell.predict(inputs[i]))\n",
    "    print(targets[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\gru.py:144: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 0.7728\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6777\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6306\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6159\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6107\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6137\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6077\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5995\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5858\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5736\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5606\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5519\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5430\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5368\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5317\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5342\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5302\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5165\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5034\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4917\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4766\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4669\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4616\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4536\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4440\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4387\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4273\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4213\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4083\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3948\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3897\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3954\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3778\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3537\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3445\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3432\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3408\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3320\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3249\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3111\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2843\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2683\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2678\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2608\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2499\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2389\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2330\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2252\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2160\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2060\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1962\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1957\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1884\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1775\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1698\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1633\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1601\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1548\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1475\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1387\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1384\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1347\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1253\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1299\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1285\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1185\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1111\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1138\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1080\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0917\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0752\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0723\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0703\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0673\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 335us/step - loss: 0.0664\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0575\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 397us/step - loss: 0.0579\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0526\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0523\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0484\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0465\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0482\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.0462\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0422\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0411\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0418\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0386\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0357\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0364\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0306\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0273\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 20)                1920      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1941 (7.58 KB)\n",
      "Trainable params: 1941 (7.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert lists to numpy arrays for TensorFlow compatibility\n",
    "inputs = np.array(inputs).reshape(-1, 1, input_size)  # Reshape to [batch, timesteps, feature]\n",
    "targets = np.array(targets).reshape(-1, output_size)\n",
    "\n",
    "# Define the GRU model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(hidden_size, input_shape=(1, input_size)),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='mse')  # Mean Squared Error for regression tasks\n",
    "\n",
    "# Train the model\n",
    "model.fit(inputs, targets, epochs=epochs)\n",
    "\n",
    "# Optionally, you can print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import tensorflow as tf \n",
    "import regex as re\n",
    "\n",
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = np.array(X)\n",
    "# inputs = np.array([input.reshape(39, 1) for input in inputs])\n",
    "\n",
    "# targets = np.array(y)\n",
    "# targets = np.array([target.reshape(687, 1) for target in targets])\n",
    "\n",
    "# input_size = 39\n",
    "# hidden_size = 39\n",
    "# output_size = 687\n",
    "# epochs = 10\n",
    "# learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "# gru_cell.train(inputs, targets, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
