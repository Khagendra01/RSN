{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.Wz = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uz = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bz = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wr = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Ur = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.br = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wh = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.Uh = np.random.randn(self.hidden_size, self.hidden_size) * 0.1\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # Store values for backpropagation\n",
    "        self.x, self.h_prev = x, h_prev\n",
    "        \n",
    "        # Update gate\n",
    "        self.z = sigmoid(np.dot(self.Wz, x) + np.dot(self.Uz, h_prev) + self.bz)\n",
    "        \n",
    "        # Reset gate\n",
    "        self.r = sigmoid(np.dot(self.Wr, x) + np.dot(self.Ur, h_prev) + self.br)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.h_tilde = tanh(np.dot(self.Wh, x) + np.dot(self.Uh, self.r * h_prev) + self.bh)\n",
    "        \n",
    "        # Final hidden state\n",
    "        h_next = self.z * h_prev + (1 - self.z) * self.h_tilde\n",
    "        \n",
    "        # Output\n",
    "        y_pred = np.dot(self.Wy, h_next) + self.by\n",
    "        \n",
    "        return y_pred, h_next\n",
    "\n",
    "    def backward(self, d_y_pred, d_h_next):\n",
    "        # Gradient of the output layer\n",
    "        d_Wy = np.dot(d_y_pred, self.h_prev.T)\n",
    "        d_by = d_y_pred.sum(axis=1, keepdims=True)\n",
    "        d_h_next += np.dot(self.Wy.T, d_y_pred)\n",
    "        \n",
    "        # Derivative of final hidden state\n",
    "        d_z = d_h_next * (self.h_prev - self.h_tilde)\n",
    "        d_h_prev = d_h_next * self.z\n",
    "        d_h_tilde = d_h_next * (1 - self.z)\n",
    "        \n",
    "        # Derivatives of the gates\n",
    "        d_h_tilde_raw = d_h_tilde * tanh_derivative(self.h_tilde)\n",
    "        d_r = np.dot(self.Uh.T, d_h_tilde_raw) * self.h_prev\n",
    "        d_h_prev += np.dot(self.Uh.T, d_h_tilde_raw) * self.r\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wh -= self.learning_rate * np.dot(d_h_tilde_raw, self.x.T)\n",
    "        self.Uh -= self.learning_rate * np.dot(d_h_tilde_raw, (self.r * self.h_prev).T)\n",
    "        self.bh -= self.learning_rate * d_h_tilde_raw.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wr -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.x.T)\n",
    "        self.Ur -= self.learning_rate * np.dot(d_r * sigmoid_derivative(self.r), self.h_prev.T)\n",
    "        self.br -= self.learning_rate * (d_r * sigmoid_derivative(self.r)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.x.T)\n",
    "        self.Uz -= self.learning_rate * np.dot(d_z * sigmoid_derivative(self.z), self.h_prev.T)\n",
    "        self.bz -= self.learning_rate * (d_z * sigmoid_derivative(self.z)).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.Wy -= self.learning_rate * d_Wy\n",
    "        self.by -= self.learning_rate * d_by\n",
    "\n",
    "        return d_h_prev\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for t in range(len(inputs)):\n",
    "                x = inputs[t]\n",
    "                y_true = targets[t]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred, h_next = self.forward(x, h_prev)\n",
    "                \n",
    "                # Calculate loss (for monitoring)\n",
    "                loss += mse_loss(y_pred, y_true)\n",
    "                \n",
    "                # Backpropagate error\n",
    "                d_loss = mse_loss_derivative(y_pred, y_true)\n",
    "                d_h_next = self.backward(d_loss, h_next)\n",
    "                h_prev = h_next  # update state\n",
    "                \n",
    "            loss /= len(inputs)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        predictions = []\n",
    "        for x in inputs:\n",
    "            y_pred, h_prev = self.forward(x, h_prev)\n",
    "            predictions.append(y_pred)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.008868433911287\n",
      "Epoch 2/10, Loss: 1.0042903955863998\n",
      "Epoch 3/10, Loss: 1.0033678354570383\n",
      "Epoch 4/10, Loss: 1.0030373447767569\n",
      "Epoch 5/10, Loss: 1.0029713099192374\n",
      "Epoch 6/10, Loss: 1.0030191191644553\n",
      "Epoch 7/10, Loss: 1.0031149145007352\n",
      "Epoch 8/10, Loss: 1.0032299425360844\n",
      "Epoch 9/10, Loss: 1.0033514089245317\n",
      "Epoch 10/10, Loss: 1.0034735823205183\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 30\n",
    "output_size = 5  \n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Example synthetic data\n",
    "inputs = [np.random.randn(input_size, 1) for _ in range(1000)]\n",
    "targets = [np.random.randn(output_size, 1) for _ in range(1000)]\n",
    "gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "gru_cell.train(inputs, targets, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import tensorflow as tf \n",
    "import regex as re\n",
    "\n",
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array(X)\n",
    "inputs = np.array([input.reshape(39, 1) for input in inputs])\n",
    "\n",
    "targets = np.array(y)\n",
    "targets = np.array([target.reshape(687, 1) for target in targets])\n",
    "\n",
    "input_size = 39\n",
    "hidden_size = 39\n",
    "output_size = 687\n",
    "epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_9392\\3916400805.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.39284285335721997\n",
      "Epoch 2/10, Loss: 0.3734499227875016\n",
      "Epoch 3/10, Loss: 0.35504859552657614\n",
      "Epoch 4/10, Loss: 0.3378876641175672\n",
      "Epoch 5/10, Loss: 0.32141977606192806\n",
      "Epoch 6/10, Loss: 0.3052129036477114\n",
      "Epoch 7/10, Loss: 0.28983460579356096\n",
      "Epoch 8/10, Loss: 0.2754135228362305\n",
      "Epoch 9/10, Loss: 0.2620004689130761\n",
      "Epoch 10/10, Loss: 0.24711975113518647\n"
     ]
    }
   ],
   "source": [
    "gru_cell = GRUCell(input_size, hidden_size, output_size, learning_rate)\n",
    "gru_cell.train(inputs, targets, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_9392\\3916400805.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(687, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = gru_cell.predict(X[10]) \n",
    "val = predicted_probs[0]\n",
    "val.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
