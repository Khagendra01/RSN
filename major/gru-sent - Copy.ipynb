{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 57254 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('input.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 100 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # ‚àÇloss/‚àÇy\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWy and ‚àÇloss/‚àÇby\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) \n",
    "         \n",
    "        # ‚àÇloss/‚àÇWh, ‚àÇloss/‚àÇUh and ‚àÇloss/‚àÇbh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) \n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWr, ‚àÇloss/‚àÇUr and ‚àÇloss/‚àÇbr\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWz, ‚àÇloss/‚àÇUz and ‚àÇloss/‚àÇbz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇhùë°‚Çã‚ÇÅ\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_string, n):\n",
    "    # Convert the seed string to a list of indices\n",
    "    seed_ix = [char_to_ix[ch] for ch in seed_string]\n",
    "    ixes = []\n",
    "\n",
    "    # Initialize the first word of sample as one-hot encoded vectors for each character in the seed\n",
    "    for ix in seed_ix:\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        # Append the index to the list of generated indices\n",
    "        ixes.append(ix)\n",
    "\n",
    "    # Now generate n characters\n",
    "    for t in range(n):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ixes[-1]] = 1  # Start with the last character from the seed\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        y = np.dot(Wy, h) + by\n",
    "        p = softmax(y)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        ixes.append(ix)\n",
    "\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 196.705191, smooth loss: 420.245498\n",
      "iter 10, loss: 285.884304, smooth loss: 419.992174\n",
      "iter 20, loss: 277.461134, smooth loss: 418.797170\n",
      "iter 30, loss: 233.893922, smooth loss: 417.099624\n",
      "iter 40, loss: 241.523409, smooth loss: 415.302109\n",
      "iter 50, loss: 217.158469, smooth loss: 413.377004\n",
      "iter 60, loss: 210.570500, smooth loss: 411.518924\n",
      "iter 70, loss: 221.233302, smooth loss: 409.559605\n",
      "iter 80, loss: 197.410723, smooth loss: 407.544105\n",
      "iter 90, loss: 222.453945, smooth loss: 405.662752\n",
      "iter 100, loss: 200.222108, smooth loss: 403.755955\n",
      "iter 110, loss: 225.370770, smooth loss: 401.868362\n",
      "iter 120, loss: 220.083808, smooth loss: 399.991947\n",
      "iter 130, loss: 205.149408, smooth loss: 398.076504\n",
      "iter 140, loss: 199.799855, smooth loss: 396.198948\n",
      "iter 150, loss: 182.457252, smooth loss: 394.214052\n",
      "iter 160, loss: 214.580930, smooth loss: 392.300648\n",
      "iter 170, loss: 217.357682, smooth loss: 390.551592\n",
      "iter 180, loss: 180.648578, smooth loss: 388.758896\n",
      "iter 190, loss: 173.928181, smooth loss: 386.838875\n",
      "iter 200, loss: 186.084092, smooth loss: 384.990125\n",
      "iter 210, loss: 215.524036, smooth loss: 383.201962\n",
      "iter 220, loss: 216.131233, smooth loss: 381.472101\n",
      "iter 230, loss: 200.921141, smooth loss: 379.645194\n",
      "iter 240, loss: 196.866409, smooth loss: 377.907423\n",
      "iter 250, loss: 174.506787, smooth loss: 376.117287\n",
      "iter 260, loss: 208.275742, smooth loss: 374.349080\n",
      "iter 270, loss: 195.492634, smooth loss: 372.636963\n",
      "iter 280, loss: 196.523011, smooth loss: 370.999754\n",
      "iter 290, loss: 193.251918, smooth loss: 369.227057\n",
      "iter 300, loss: 184.162455, smooth loss: 367.474177\n",
      "iter 310, loss: 189.733171, smooth loss: 365.835355\n",
      "iter 320, loss: 190.249778, smooth loss: 364.179596\n",
      "iter 330, loss: 191.575235, smooth loss: 362.401983\n",
      "iter 340, loss: 167.318843, smooth loss: 360.710854\n",
      "iter 350, loss: 219.465169, smooth loss: 359.038511\n",
      "iter 360, loss: 171.190523, smooth loss: 357.387425\n",
      "iter 370, loss: 207.309138, smooth loss: 355.748712\n",
      "iter 380, loss: 198.512101, smooth loss: 354.195087\n",
      "iter 390, loss: 192.874209, smooth loss: 352.508880\n",
      "iter 400, loss: 187.670494, smooth loss: 350.967207\n",
      "iter 410, loss: 200.010666, smooth loss: 349.386962\n",
      "iter 420, loss: 189.599076, smooth loss: 347.811304\n",
      "iter 430, loss: 204.970440, smooth loss: 346.349254\n",
      "iter 440, loss: 190.944866, smooth loss: 344.813981\n",
      "iter 450, loss: 194.372955, smooth loss: 343.270779\n",
      "iter 460, loss: 179.986367, smooth loss: 341.765202\n",
      "iter 470, loss: 201.201652, smooth loss: 340.236769\n",
      "iter 480, loss: 190.676878, smooth loss: 338.714875\n",
      "iter 490, loss: 175.478148, smooth loss: 337.244576\n",
      "iter 500, loss: 194.239379, smooth loss: 335.888454\n",
      "iter 510, loss: 179.710438, smooth loss: 334.365673\n",
      "iter 520, loss: 180.547664, smooth loss: 332.905665\n",
      "iter 530, loss: 198.587893, smooth loss: 331.573723\n",
      "iter 540, loss: 209.032491, smooth loss: 330.215428\n",
      "iter 550, loss: 195.677024, smooth loss: 328.935647\n",
      "iter 560, loss: 185.095086, smooth loss: 327.580589\n",
      "iter 570, loss: 175.618205, smooth loss: 326.131748\n",
      "iter 580, loss: 195.093276, smooth loss: 324.743853\n",
      "iter 590, loss: 212.927697, smooth loss: 323.505335\n",
      "iter 600, loss: 189.123040, smooth loss: 322.227341\n",
      "iter 610, loss: 202.926494, smooth loss: 320.912325\n",
      "iter 620, loss: 175.565359, smooth loss: 319.578092\n",
      "iter 630, loss: 210.870960, smooth loss: 318.313697\n",
      "iter 640, loss: 181.546439, smooth loss: 316.969534\n",
      "iter 650, loss: 168.069904, smooth loss: 315.674830\n",
      "iter 660, loss: 201.135440, smooth loss: 314.413619\n",
      "iter 670, loss: 183.582544, smooth loss: 313.201713\n",
      "iter 680, loss: 175.539092, smooth loss: 311.962018\n",
      "iter 690, loss: 192.701703, smooth loss: 310.798031\n",
      "iter 700, loss: 180.576549, smooth loss: 309.617615\n",
      "iter 710, loss: 184.327159, smooth loss: 308.399076\n",
      "iter 720, loss: 187.164926, smooth loss: 307.147262\n",
      "iter 730, loss: 162.341137, smooth loss: 305.861332\n",
      "iter 740, loss: 178.904547, smooth loss: 304.774437\n",
      "iter 750, loss: 200.135520, smooth loss: 303.742815\n",
      "iter 760, loss: 188.122762, smooth loss: 302.530979\n",
      "iter 770, loss: 168.292166, smooth loss: 301.336710\n",
      "iter 780, loss: 222.297571, smooth loss: 300.203823\n",
      "iter 790, loss: 193.437094, smooth loss: 299.156005\n",
      "iter 800, loss: 212.483201, smooth loss: 298.025170\n",
      "iter 810, loss: 200.619620, smooth loss: 296.999558\n",
      "iter 820, loss: 188.395757, smooth loss: 295.876306\n",
      "iter 830, loss: 185.916689, smooth loss: 294.721420\n",
      "iter 840, loss: 183.216392, smooth loss: 293.680968\n",
      "iter 850, loss: 218.092784, smooth loss: 292.687258\n",
      "iter 860, loss: 186.951168, smooth loss: 291.578552\n",
      "iter 870, loss: 206.586770, smooth loss: 290.491552\n",
      "iter 880, loss: 194.921192, smooth loss: 289.484412\n",
      "iter 890, loss: 162.174876, smooth loss: 288.492654\n",
      "iter 900, loss: 179.919539, smooth loss: 287.362868\n",
      "iter 910, loss: 175.222567, smooth loss: 286.340452\n",
      "iter 920, loss: 207.285332, smooth loss: 285.260419\n",
      "iter 930, loss: 185.970044, smooth loss: 284.270516\n",
      "iter 940, loss: 169.716674, smooth loss: 283.227576\n",
      "iter 950, loss: 196.703819, smooth loss: 282.290813\n",
      "iter 960, loss: 170.948630, smooth loss: 281.223809\n",
      "iter 970, loss: 168.753291, smooth loss: 280.308707\n",
      "iter 980, loss: 161.207818, smooth loss: 279.313752\n",
      "iter 990, loss: 201.164694, smooth loss: 278.356132\n",
      "iter 1000, loss: 178.733266, smooth loss: 277.479888\n",
      "iter 1010, loss: 164.196607, smooth loss: 276.511706\n",
      "iter 1020, loss: 172.369507, smooth loss: 275.569378\n",
      "iter 1030, loss: 194.469770, smooth loss: 274.639006\n",
      "iter 1040, loss: 186.568846, smooth loss: 273.695700\n",
      "iter 1050, loss: 184.312805, smooth loss: 272.764311\n",
      "iter 1060, loss: 167.257635, smooth loss: 271.827941\n",
      "iter 1070, loss: 174.973477, smooth loss: 271.062569\n",
      "iter 1080, loss: 162.120580, smooth loss: 270.151480\n",
      "iter 1090, loss: 197.139894, smooth loss: 269.239011\n",
      "iter 1100, loss: 209.673794, smooth loss: 268.430319\n",
      "iter 1110, loss: 190.045257, smooth loss: 267.589008\n",
      "iter 1120, loss: 206.910641, smooth loss: 266.836990\n",
      "iter 1130, loss: 204.336759, smooth loss: 266.073942\n",
      "iter 1140, loss: 161.745567, smooth loss: 265.165751\n",
      "iter 1150, loss: 162.491336, smooth loss: 264.289652\n",
      "iter 1160, loss: 210.472642, smooth loss: 263.563120\n",
      "iter 1170, loss: 195.357947, smooth loss: 262.842327\n",
      "iter 1180, loss: 183.382687, smooth loss: 262.044662\n",
      "iter 1190, loss: 176.051185, smooth loss: 261.227145\n",
      "iter 1200, loss: 195.272111, smooth loss: 260.403572\n",
      "iter 1210, loss: 162.340314, smooth loss: 259.653499\n",
      "iter 1220, loss: 213.002925, smooth loss: 258.869938\n",
      "iter 1230, loss: 145.638971, smooth loss: 258.049288\n",
      "iter 1240, loss: 193.010608, smooth loss: 257.352159\n",
      "iter 1250, loss: 196.545298, smooth loss: 256.593539\n",
      "iter 1260, loss: 181.788270, smooth loss: 255.914623\n",
      "iter 1270, loss: 166.052624, smooth loss: 255.216570\n",
      "iter 1280, loss: 194.558078, smooth loss: 254.465174\n",
      "iter 1290, loss: 186.979200, smooth loss: 253.699115\n",
      "iter 1300, loss: 191.600064, smooth loss: 252.899215\n",
      "iter 1310, loss: 175.512155, smooth loss: 252.246052\n",
      "iter 1320, loss: 193.163470, smooth loss: 251.661095\n",
      "iter 1330, loss: 171.917802, smooth loss: 250.907519\n",
      "iter 1340, loss: 219.243820, smooth loss: 250.203305\n",
      "iter 1350, loss: 159.944287, smooth loss: 249.444554\n",
      "iter 1360, loss: 222.899921, smooth loss: 248.873564\n",
      "iter 1370, loss: 198.546049, smooth loss: 248.200317\n",
      "iter 1380, loss: 195.218199, smooth loss: 247.588632\n",
      "iter 1390, loss: 182.282072, smooth loss: 246.912412\n",
      "iter 1400, loss: 165.619578, smooth loss: 246.178319\n",
      "iter 1410, loss: 192.121492, smooth loss: 245.592770\n",
      "iter 1420, loss: 181.807347, smooth loss: 244.952894\n",
      "iter 1430, loss: 158.302470, smooth loss: 244.316516\n",
      "iter 1440, loss: 163.992116, smooth loss: 243.643592\n",
      "iter 1450, loss: 210.115614, smooth loss: 243.015666\n",
      "iter 1460, loss: 166.158788, smooth loss: 242.498620\n",
      "iter 1470, loss: 154.674378, smooth loss: 241.746056\n",
      "iter 1480, loss: 165.012985, smooth loss: 241.099803\n",
      "iter 1490, loss: 164.476869, smooth loss: 240.440523\n",
      "iter 1500, loss: 189.884923, smooth loss: 239.841214\n",
      "iter 1510, loss: 178.133320, smooth loss: 239.215696\n",
      "iter 1520, loss: 174.655480, smooth loss: 238.606933\n",
      "iter 1530, loss: 167.022482, smooth loss: 237.979589\n",
      "iter 1540, loss: 172.038614, smooth loss: 237.426606\n",
      "iter 1550, loss: 140.267714, smooth loss: 236.782581\n",
      "iter 1560, loss: 160.933637, smooth loss: 236.174878\n",
      "iter 1570, loss: 176.269271, smooth loss: 235.692014\n",
      "iter 1580, loss: 175.438009, smooth loss: 235.104123\n",
      "iter 1590, loss: 178.553286, smooth loss: 234.529717\n",
      "iter 1600, loss: 174.537651, smooth loss: 233.920178\n",
      "iter 1610, loss: 159.831257, smooth loss: 233.322577\n",
      "iter 1620, loss: 189.279790, smooth loss: 232.727366\n",
      "iter 1630, loss: 189.513237, smooth loss: 232.180701\n",
      "iter 1640, loss: 210.683130, smooth loss: 231.746023\n",
      "iter 1650, loss: 196.095635, smooth loss: 231.207802\n",
      "iter 1660, loss: 162.536354, smooth loss: 230.573810\n",
      "iter 1670, loss: 177.239224, smooth loss: 230.105333\n",
      "iter 1680, loss: 156.061259, smooth loss: 229.585820\n",
      "iter 1690, loss: 190.044605, smooth loss: 229.167402\n",
      "iter 1700, loss: 164.420471, smooth loss: 228.694770\n",
      "iter 1710, loss: 181.962281, smooth loss: 228.210346\n",
      "iter 1720, loss: 184.932294, smooth loss: 227.644064\n",
      "iter 1730, loss: 208.606164, smooth loss: 227.196247\n",
      "iter 1740, loss: 185.866143, smooth loss: 226.832221\n",
      "iter 1750, loss: 198.175599, smooth loss: 226.333788\n",
      "iter 1760, loss: 159.305949, smooth loss: 225.810416\n",
      "iter 1770, loss: 160.218667, smooth loss: 225.255951\n",
      "iter 1780, loss: 180.858535, smooth loss: 224.889990\n",
      "iter 1790, loss: 172.007556, smooth loss: 224.352016\n",
      "iter 1800, loss: 170.113459, smooth loss: 223.890047\n",
      "iter 1810, loss: 181.186349, smooth loss: 223.434034\n",
      "iter 1820, loss: 192.990056, smooth loss: 222.950158\n",
      "iter 1830, loss: 168.445465, smooth loss: 222.582246\n",
      "iter 1840, loss: 190.977697, smooth loss: 222.176801\n",
      "iter 1850, loss: 169.538879, smooth loss: 221.678976\n",
      "iter 1860, loss: 159.874000, smooth loss: 221.212201\n",
      "iter 1870, loss: 162.262858, smooth loss: 220.679085\n",
      "iter 1880, loss: 201.754643, smooth loss: 220.304773\n",
      "iter 1890, loss: 192.485037, smooth loss: 219.975479\n",
      "iter 1900, loss: 159.132906, smooth loss: 219.544899\n",
      "iter 1910, loss: 144.916606, smooth loss: 219.038034\n",
      "iter 1920, loss: 159.349260, smooth loss: 218.623342\n",
      "iter 1930, loss: 159.736758, smooth loss: 218.230183\n",
      "iter 1940, loss: 158.599538, smooth loss: 217.846720\n",
      "iter 1950, loss: 180.252623, smooth loss: 217.497939\n",
      "iter 1960, loss: 172.124848, smooth loss: 217.101572\n",
      "iter 1970, loss: 176.397787, smooth loss: 216.602584\n",
      "iter 1980, loss: 173.312358, smooth loss: 216.223324\n",
      "iter 1990, loss: 180.365079, smooth loss: 215.885304\n",
      "iter 2000, loss: 150.573508, smooth loss: 215.514816\n",
      "iter 2010, loss: 171.370286, smooth loss: 215.107982\n",
      "iter 2020, loss: 169.266757, smooth loss: 214.660137\n",
      "iter 2030, loss: 177.495092, smooth loss: 214.405188\n",
      "iter 2040, loss: 165.647011, smooth loss: 213.918472\n",
      "iter 2050, loss: 185.248975, smooth loss: 213.503899\n",
      "iter 2060, loss: 168.404018, smooth loss: 213.048799\n",
      "iter 2070, loss: 176.121041, smooth loss: 212.684842\n",
      "iter 2080, loss: 183.810567, smooth loss: 212.316412\n",
      "iter 2090, loss: 172.305959, smooth loss: 211.930572\n",
      "iter 2100, loss: 155.531426, smooth loss: 211.559114\n",
      "iter 2110, loss: 199.601892, smooth loss: 211.153533\n",
      "iter 2120, loss: 186.145608, smooth loss: 210.786482\n",
      "iter 2130, loss: 165.270454, smooth loss: 210.382690\n",
      "iter 2140, loss: 202.991532, smooth loss: 210.102271\n",
      "iter 2150, loss: 181.312584, smooth loss: 209.744527\n",
      "iter 2160, loss: 180.655230, smooth loss: 209.371956\n",
      "iter 2170, loss: 182.469446, smooth loss: 208.982212\n",
      "iter 2180, loss: 155.917636, smooth loss: 208.599975\n",
      "iter 2190, loss: 176.279536, smooth loss: 208.205357\n",
      "iter 2200, loss: 152.495761, smooth loss: 207.817851\n",
      "iter 2210, loss: 199.672536, smooth loss: 207.555033\n",
      "iter 2220, loss: 164.790313, smooth loss: 207.270152\n",
      "iter 2230, loss: 156.583074, smooth loss: 206.882622\n",
      "iter 2240, loss: 172.497332, smooth loss: 206.600101\n",
      "iter 2250, loss: 144.654785, smooth loss: 206.264578\n",
      "iter 2260, loss: 166.817205, smooth loss: 205.937500\n",
      "iter 2270, loss: 208.450608, smooth loss: 205.785464\n",
      "iter 2280, loss: 175.065266, smooth loss: 205.452721\n",
      "iter 2290, loss: 171.904547, smooth loss: 205.078895\n",
      "iter 2300, loss: 180.347045, smooth loss: 204.764073\n",
      "iter 2310, loss: 184.885854, smooth loss: 204.602475\n",
      "iter 2320, loss: 178.217155, smooth loss: 204.300654\n",
      "iter 2330, loss: 182.154237, smooth loss: 204.023227\n",
      "iter 2340, loss: 164.100048, smooth loss: 203.635467\n",
      "iter 2350, loss: 183.320856, smooth loss: 203.430853\n",
      "iter 2360, loss: 173.429047, smooth loss: 203.090516\n",
      "iter 2370, loss: 177.346335, smooth loss: 202.765978\n",
      "iter 2380, loss: 213.425828, smooth loss: 202.494954\n",
      "iter 2390, loss: 158.760783, smooth loss: 202.121680\n",
      "iter 2400, loss: 168.857498, smooth loss: 201.936104\n",
      "iter 2410, loss: 153.317072, smooth loss: 201.712880\n",
      "iter 2420, loss: 174.129224, smooth loss: 201.390903\n",
      "iter 2430, loss: 180.574736, smooth loss: 201.101398\n",
      "iter 2440, loss: 177.750347, smooth loss: 200.756486\n",
      "iter 2450, loss: 188.026437, smooth loss: 200.437622\n",
      "iter 2460, loss: 192.777368, smooth loss: 200.313901\n",
      "iter 2470, loss: 160.753266, smooth loss: 200.086003\n",
      "iter 2480, loss: 191.467241, smooth loss: 199.756297\n",
      "iter 2490, loss: 178.823167, smooth loss: 199.492132\n",
      "iter 2500, loss: 163.700001, smooth loss: 199.210909\n",
      "iter 2510, loss: 160.124834, smooth loss: 199.020659\n",
      "iter 2520, loss: 203.101074, smooth loss: 198.802618\n",
      "iter 2530, loss: 177.416394, smooth loss: 198.560247\n",
      "iter 2540, loss: 164.762951, smooth loss: 198.208537\n",
      "iter 2550, loss: 168.233872, smooth loss: 197.959968\n",
      "iter 2560, loss: 188.858320, smooth loss: 197.793173\n",
      "iter 2570, loss: 158.812420, smooth loss: 197.593900\n",
      "iter 2580, loss: 189.920319, smooth loss: 197.298059\n",
      "iter 2590, loss: 177.213288, smooth loss: 196.996951\n",
      "iter 2600, loss: 235.395270, smooth loss: 196.847973\n",
      "iter 2610, loss: 164.603242, smooth loss: 196.549803\n",
      "iter 2620, loss: 169.487639, smooth loss: 196.208968\n",
      "iter 2630, loss: 176.503530, smooth loss: 195.947094\n",
      "iter 2640, loss: 157.308700, smooth loss: 195.705688\n",
      "iter 2650, loss: 174.606945, smooth loss: 195.471396\n",
      "iter 2660, loss: 172.803603, smooth loss: 195.227624\n",
      "iter 2670, loss: 182.133012, smooth loss: 195.022004\n",
      "iter 2680, loss: 201.115905, smooth loss: 194.683841\n",
      "iter 2690, loss: 145.267061, smooth loss: 194.471250\n",
      "iter 2700, loss: 160.494767, smooth loss: 194.232050\n",
      "iter 2710, loss: 179.631729, smooth loss: 194.012541\n",
      "iter 2720, loss: 160.404064, smooth loss: 193.824426\n",
      "iter 2730, loss: 164.623393, smooth loss: 193.560654\n",
      "iter 2740, loss: 172.156384, smooth loss: 193.292840\n",
      "iter 2750, loss: 175.484015, smooth loss: 193.056127\n",
      "iter 2760, loss: 161.707243, smooth loss: 192.774045\n",
      "iter 2770, loss: 188.173523, smooth loss: 192.512567\n",
      "iter 2780, loss: 183.264581, smooth loss: 192.326218\n",
      "iter 2790, loss: 145.561404, smooth loss: 192.205947\n",
      "iter 2800, loss: 165.708004, smooth loss: 191.951609\n",
      "iter 2810, loss: 192.870417, smooth loss: 191.704753\n",
      "iter 2820, loss: 212.035720, smooth loss: 191.532111\n",
      "iter 2830, loss: 178.917557, smooth loss: 191.281331\n",
      "iter 2840, loss: 182.795427, smooth loss: 191.214320\n",
      "iter 2850, loss: 170.562828, smooth loss: 191.023877\n",
      "iter 2860, loss: 171.946945, smooth loss: 190.767903\n",
      "iter 2870, loss: 157.990961, smooth loss: 190.519143\n",
      "iter 2880, loss: 185.883806, smooth loss: 190.507321\n",
      "iter 2890, loss: 158.069653, smooth loss: 190.341798\n",
      "iter 2900, loss: 164.260065, smooth loss: 190.140716\n",
      "iter 2910, loss: 157.078496, smooth loss: 189.882304\n",
      "iter 2920, loss: 184.715118, smooth loss: 189.760830\n",
      "iter 2930, loss: 167.702011, smooth loss: 189.548576\n",
      "iter 2940, loss: 160.595482, smooth loss: 189.306585\n",
      "iter 2950, loss: 176.762844, smooth loss: 189.104635\n",
      "iter 2960, loss: 145.281884, smooth loss: 188.899748\n",
      "iter 2970, loss: 185.514793, smooth loss: 188.762021\n",
      "iter 2980, loss: 196.902566, smooth loss: 188.655713\n",
      "iter 2990, loss: 166.279678, smooth loss: 188.421029\n",
      "iter 3000, loss: 169.876969, smooth loss: 188.239078\n",
      "iter 3010, loss: 154.550224, smooth loss: 187.988315\n",
      "iter 3020, loss: 181.495038, smooth loss: 187.725863\n",
      "iter 3030, loss: 195.228458, smooth loss: 187.722054\n",
      "iter 3040, loss: 150.763678, smooth loss: 187.611196\n",
      "iter 3050, loss: 143.210584, smooth loss: 187.354664\n",
      "iter 3060, loss: 164.276383, smooth loss: 187.186932\n",
      "iter 3070, loss: 171.023456, smooth loss: 187.005485\n",
      "iter 3080, loss: 181.902301, smooth loss: 186.919210\n",
      "iter 3090, loss: 158.453720, smooth loss: 186.723844\n",
      "iter 3100, loss: 173.366957, smooth loss: 186.640954\n",
      "iter 3110, loss: 139.069335, smooth loss: 186.371027\n",
      "iter 3120, loss: 175.386431, smooth loss: 186.180697\n",
      "iter 3130, loss: 179.870994, smooth loss: 186.101484\n",
      "iter 3140, loss: 170.674237, smooth loss: 185.976085\n",
      "iter 3150, loss: 160.022684, smooth loss: 185.769332\n",
      "iter 3160, loss: 161.936039, smooth loss: 185.557779\n",
      "iter 3170, loss: 166.259671, smooth loss: 185.430803\n",
      "iter 3180, loss: 157.601201, smooth loss: 185.320785\n",
      "iter 3190, loss: 169.727198, smooth loss: 185.058145\n",
      "iter 3200, loss: 137.260932, smooth loss: 184.864437\n",
      "iter 3210, loss: 196.993714, smooth loss: 184.727720\n",
      "iter 3220, loss: 134.242131, smooth loss: 184.543643\n",
      "iter 3230, loss: 183.137734, smooth loss: 184.381147\n",
      "iter 3240, loss: 165.196379, smooth loss: 184.242922\n",
      "iter 3250, loss: 167.356077, smooth loss: 183.979660\n",
      "iter 3260, loss: 156.844396, smooth loss: 183.872993\n",
      "iter 3270, loss: 182.355385, smooth loss: 183.702283\n",
      "iter 3280, loss: 160.143836, smooth loss: 183.528632\n",
      "iter 3290, loss: 172.729166, smooth loss: 183.439094\n",
      "iter 3300, loss: 164.608381, smooth loss: 183.248111\n",
      "iter 3310, loss: 172.158466, smooth loss: 183.064152\n",
      "iter 3320, loss: 155.378209, smooth loss: 182.890912\n",
      "iter 3330, loss: 176.162138, smooth loss: 182.693975\n",
      "iter 3340, loss: 162.081384, smooth loss: 182.478051\n",
      "iter 3350, loss: 164.227676, smooth loss: 182.320534\n",
      "iter 3360, loss: 177.248215, smooth loss: 182.344307\n",
      "iter 3370, loss: 152.393716, smooth loss: 182.131735\n",
      "iter 3380, loss: 161.901017, smooth loss: 181.949221\n",
      "iter 3390, loss: 167.109105, smooth loss: 181.918852\n",
      "iter 3400, loss: 188.041306, smooth loss: 181.760855\n",
      "iter 3410, loss: 181.395258, smooth loss: 181.792431\n",
      "iter 3420, loss: 161.485243, smooth loss: 181.692468\n",
      "iter 3430, loss: 145.288486, smooth loss: 181.445480\n",
      "iter 3440, loss: 176.301243, smooth loss: 181.299383\n",
      "iter 3450, loss: 197.814675, smooth loss: 181.334587\n",
      "iter 3460, loss: 169.821829, smooth loss: 181.275798\n",
      "iter 3470, loss: 193.174243, smooth loss: 181.134921\n",
      "iter 3480, loss: 147.596291, smooth loss: 180.948466\n",
      "iter 3490, loss: 187.327730, smooth loss: 180.843796\n",
      "iter 3500, loss: 161.404113, smooth loss: 180.690418\n",
      "iter 3510, loss: 139.536013, smooth loss: 180.539290\n",
      "iter 3520, loss: 187.076941, smooth loss: 180.363690\n",
      "iter 3530, loss: 166.236566, smooth loss: 180.253679\n",
      "iter 3540, loss: 152.586158, smooth loss: 180.110612\n",
      "iter 3550, loss: 172.087676, smooth loss: 180.054652\n",
      "iter 3560, loss: 152.663702, smooth loss: 179.917006\n",
      "iter 3570, loss: 164.748684, smooth loss: 179.782518\n",
      "iter 3580, loss: 177.617079, smooth loss: 179.603764\n",
      "iter 3590, loss: 136.655447, smooth loss: 179.370056\n",
      "iter 3600, loss: 157.736202, smooth loss: 179.392743\n",
      "iter 3610, loss: 176.910064, smooth loss: 179.403979\n",
      "iter 3620, loss: 174.484886, smooth loss: 179.207232\n",
      "iter 3630, loss: 150.469539, smooth loss: 179.063455\n",
      "iter 3640, loss: 194.473217, smooth loss: 178.922202\n",
      "iter 3650, loss: 180.438325, smooth loss: 178.884077\n",
      "iter 3660, loss: 187.525208, smooth loss: 178.763619\n",
      "iter 3670, loss: 191.019999, smooth loss: 178.744096\n",
      "iter 3680, loss: 165.625481, smooth loss: 178.551979\n",
      "iter 3690, loss: 164.165577, smooth loss: 178.358168\n",
      "iter 3700, loss: 171.518337, smooth loss: 178.321085\n",
      "iter 3710, loss: 193.521017, smooth loss: 178.255284\n",
      "iter 3720, loss: 168.633593, smooth loss: 178.111388\n",
      "iter 3730, loss: 185.593914, smooth loss: 177.953423\n",
      "iter 3740, loss: 168.182568, smooth loss: 177.876151\n",
      "iter 3750, loss: 144.180268, smooth loss: 177.799311\n",
      "iter 3760, loss: 158.539727, smooth loss: 177.599048\n",
      "iter 3770, loss: 156.153511, smooth loss: 177.482421\n",
      "iter 3780, loss: 181.909136, smooth loss: 177.330383\n",
      "iter 3790, loss: 172.948074, smooth loss: 177.244356\n",
      "iter 3800, loss: 150.977749, smooth loss: 177.094488\n",
      "iter 3810, loss: 184.206893, smooth loss: 177.016204\n",
      "iter 3820, loss: 149.968598, smooth loss: 176.792326\n",
      "iter 3830, loss: 153.869507, smooth loss: 176.755603\n",
      "iter 3840, loss: 136.091757, smooth loss: 176.606434\n",
      "iter 3850, loss: 184.709970, smooth loss: 176.511243\n",
      "iter 3860, loss: 151.348249, smooth loss: 176.454845\n",
      "iter 3870, loss: 146.352318, smooth loss: 176.283121\n",
      "iter 3880, loss: 145.095427, smooth loss: 176.168853\n",
      "iter 3890, loss: 167.260042, smooth loss: 176.020410\n",
      "iter 3900, loss: 170.106591, smooth loss: 175.861939\n",
      "iter 3910, loss: 156.954835, smooth loss: 175.712793\n",
      "iter 3920, loss: 144.963261, smooth loss: 175.546696\n",
      "iter 3930, loss: 154.711021, smooth loss: 175.600436\n",
      "iter 3940, loss: 140.581387, smooth loss: 175.487063\n",
      "iter 3950, loss: 182.989126, smooth loss: 175.330530\n",
      "iter 3960, loss: 187.673875, smooth loss: 175.229902\n",
      "iter 3970, loss: 168.080770, smooth loss: 175.077859\n",
      "iter 3980, loss: 191.108053, smooth loss: 175.093836\n",
      "iter 3990, loss: 188.920600, smooth loss: 175.106288\n",
      "iter 4000, loss: 137.533278, smooth loss: 174.923917\n",
      "iter 4010, loss: 144.653917, smooth loss: 174.780681\n",
      "iter 4020, loss: 199.149042, smooth loss: 174.820291\n",
      "iter 4030, loss: 178.707037, smooth loss: 174.828683\n",
      "iter 4040, loss: 160.847056, smooth loss: 174.716848\n",
      "iter 4050, loss: 153.510293, smooth loss: 174.603576\n",
      "iter 4060, loss: 175.144793, smooth loss: 174.464924\n",
      "iter 4070, loss: 142.705576, smooth loss: 174.425921\n",
      "iter 4080, loss: 196.813151, smooth loss: 174.315882\n",
      "iter 4090, loss: 121.574795, smooth loss: 174.110840\n",
      "iter 4100, loss: 176.998183, smooth loss: 174.057274\n",
      "iter 4110, loss: 175.383275, smooth loss: 173.944863\n",
      "iter 4120, loss: 162.488501, smooth loss: 173.923973\n",
      "iter 4130, loss: 154.845456, smooth loss: 173.839445\n",
      "iter 4140, loss: 168.286913, smooth loss: 173.719565\n",
      "iter 4150, loss: 161.521303, smooth loss: 173.592892\n",
      "iter 4160, loss: 155.329026, smooth loss: 173.425779\n",
      "iter 4170, loss: 158.297481, smooth loss: 173.430243\n",
      "iter 4180, loss: 176.755204, smooth loss: 173.470478\n",
      "iter 4190, loss: 156.423854, smooth loss: 173.297047\n",
      "iter 4200, loss: 216.974609, smooth loss: 173.223405\n",
      "iter 4210, loss: 143.380391, smooth loss: 173.060393\n",
      "iter 4220, loss: 207.559343, smooth loss: 173.063862\n",
      "iter 4230, loss: 176.189800, smooth loss: 172.993800\n",
      "iter 4240, loss: 175.696736, smooth loss: 172.961059\n",
      "iter 4250, loss: 156.175632, smooth loss: 172.834978\n",
      "iter 4260, loss: 152.910604, smooth loss: 172.657831\n",
      "iter 4270, loss: 185.516469, smooth loss: 172.681817\n",
      "iter 4280, loss: 172.074762, smooth loss: 172.608545\n",
      "iter 4290, loss: 139.912880, smooth loss: 172.530541\n",
      "iter 4300, loss: 143.264091, smooth loss: 172.411598\n",
      "iter 4310, loss: 200.981393, smooth loss: 172.336200\n",
      "iter 4320, loss: 149.315749, smooth loss: 172.364680\n",
      "iter 4330, loss: 140.241091, smooth loss: 172.154890\n",
      "iter 4340, loss: 150.305314, smooth loss: 172.045268\n",
      "iter 4350, loss: 154.450054, smooth loss: 171.931118\n",
      "iter 4360, loss: 175.365049, smooth loss: 171.877292\n",
      "iter 4370, loss: 169.610215, smooth loss: 171.780315\n",
      "iter 4380, loss: 152.319564, smooth loss: 171.682235\n",
      "iter 4390, loss: 153.317701, smooth loss: 171.558878\n",
      "iter 4400, loss: 156.599144, smooth loss: 171.518496\n",
      "iter 4410, loss: 121.531790, smooth loss: 171.391984\n",
      "iter 4420, loss: 146.412718, smooth loss: 171.298038\n",
      "iter 4430, loss: 161.270235, smooth loss: 171.308776\n",
      "iter 4440, loss: 155.522892, smooth loss: 171.175182\n",
      "iter 4450, loss: 167.829720, smooth loss: 171.107946\n",
      "iter 4460, loss: 155.398420, smooth loss: 170.953714\n",
      "iter 4470, loss: 140.218516, smooth loss: 170.800481\n",
      "iter 4480, loss: 163.491312, smooth loss: 170.676401\n",
      "iter 4490, loss: 161.060849, smooth loss: 170.579216\n",
      "iter 4500, loss: 192.951430, smooth loss: 170.629699\n",
      "iter 4510, loss: 184.656247, smooth loss: 170.562027\n",
      "iter 4520, loss: 144.365641, smooth loss: 170.365688\n",
      "iter 4530, loss: 153.832290, smooth loss: 170.305356\n",
      "iter 4540, loss: 146.121129, smooth loss: 170.182927\n",
      "iter 4550, loss: 175.663304, smooth loss: 170.209777\n",
      "iter 4560, loss: 154.358572, smooth loss: 170.200072\n",
      "iter 4570, loss: 170.421063, smooth loss: 170.164224\n",
      "iter 4580, loss: 172.296158, smooth loss: 170.023567\n",
      "iter 4590, loss: 196.214578, smooth loss: 170.015749\n",
      "iter 4600, loss: 182.906925, smooth loss: 170.097391\n",
      "iter 4610, loss: 173.720875, smooth loss: 170.005230\n",
      "iter 4620, loss: 146.722984, smooth loss: 169.898833\n",
      "iter 4630, loss: 148.259769, smooth loss: 169.748845\n",
      "iter 4640, loss: 168.521255, smooth loss: 169.805196\n",
      "iter 4650, loss: 159.425893, smooth loss: 169.654004\n",
      "iter 4660, loss: 150.305345, smooth loss: 169.554218\n",
      "iter 4670, loss: 168.071064, smooth loss: 169.466577\n",
      "iter 4680, loss: 181.876597, smooth loss: 169.362550\n",
      "iter 4690, loss: 163.379177, smooth loss: 169.383970\n",
      "iter 4700, loss: 150.570635, smooth loss: 169.320605\n",
      "iter 4710, loss: 153.510435, smooth loss: 169.206677\n",
      "iter 4720, loss: 152.966791, smooth loss: 169.128917\n",
      "iter 4730, loss: 151.330337, smooth loss: 168.978682\n",
      "iter 4740, loss: 187.463462, smooth loss: 168.984126\n",
      "iter 4750, loss: 183.909531, smooth loss: 169.011460\n",
      "iter 4760, loss: 144.014969, smooth loss: 168.933097\n",
      "iter 4770, loss: 128.665707, smooth loss: 168.785627\n",
      "iter 4780, loss: 143.825737, smooth loss: 168.724927\n",
      "iter 4790, loss: 144.988181, smooth loss: 168.681814\n",
      "iter 4800, loss: 148.155274, smooth loss: 168.653779\n",
      "iter 4810, loss: 169.590382, smooth loss: 168.648619\n",
      "iter 4820, loss: 142.383682, smooth loss: 168.585898\n",
      "iter 4830, loss: 146.334659, smooth loss: 168.387278\n",
      "iter 4840, loss: 162.613627, smooth loss: 168.370722\n",
      "iter 4850, loss: 161.751863, smooth loss: 168.388672\n",
      "iter 4860, loss: 135.068840, smooth loss: 168.361622\n",
      "iter 4870, loss: 158.562363, smooth loss: 168.287038\n",
      "iter 4880, loss: 156.710800, smooth loss: 168.162485\n",
      "iter 4890, loss: 165.930072, smooth loss: 168.234779\n",
      "iter 4900, loss: 150.071164, smooth loss: 168.065514\n",
      "iter 4910, loss: 173.694442, smooth loss: 167.967212\n",
      "iter 4920, loss: 162.218641, smooth loss: 167.839743\n",
      "iter 4930, loss: 159.564942, smooth loss: 167.804754\n",
      "iter 4940, loss: 169.240846, smooth loss: 167.743047\n",
      "iter 4950, loss: 153.664843, smooth loss: 167.674879\n",
      "iter 4960, loss: 141.805494, smooth loss: 167.608766\n",
      "iter 4970, loss: 181.987407, smooth loss: 167.508038\n",
      "iter 4980, loss: 180.309488, smooth loss: 167.454588\n",
      "iter 4990, loss: 152.410963, smooth loss: 167.356804\n",
      "iter 5000, loss: 188.518587, smooth loss: 167.383645\n",
      "iter 5010, loss: 161.866841, smooth loss: 167.295405\n",
      "iter 5020, loss: 174.754123, smooth loss: 167.214228\n",
      "iter 5030, loss: 167.770177, smooth loss: 167.101228\n",
      "iter 5040, loss: 140.955867, smooth loss: 166.969019\n",
      "iter 5050, loss: 163.088749, smooth loss: 166.855558\n",
      "iter 5060, loss: 141.008878, smooth loss: 166.726775\n",
      "iter 5070, loss: 197.638091, smooth loss: 166.742030\n",
      "iter 5080, loss: 151.819969, smooth loss: 166.735126\n",
      "iter 5090, loss: 139.269729, smooth loss: 166.613109\n",
      "iter 5100, loss: 157.669112, smooth loss: 166.566677\n",
      "iter 5110, loss: 129.010292, smooth loss: 166.444525\n",
      "iter 5120, loss: 156.312863, smooth loss: 166.379160\n",
      "iter 5130, loss: 200.001319, smooth loss: 166.516337\n",
      "iter 5140, loss: 164.576138, smooth loss: 166.464772\n",
      "iter 5150, loss: 155.359701, smooth loss: 166.334888\n",
      "iter 5160, loss: 170.523069, smooth loss: 166.287854\n",
      "iter 5170, loss: 165.691138, smooth loss: 166.387410\n",
      "iter 5180, loss: 162.960789, smooth loss: 166.350731\n",
      "iter 5190, loss: 169.866247, smooth loss: 166.314961\n",
      "iter 5200, loss: 149.872601, smooth loss: 166.155816\n",
      "iter 5210, loss: 169.092424, smooth loss: 166.210219\n",
      "iter 5220, loss: 156.102070, smooth loss: 166.090048\n",
      "iter 5230, loss: 158.669756, smooth loss: 165.989005\n",
      "iter 5240, loss: 190.562777, smooth loss: 165.924874\n",
      "iter 5250, loss: 141.561138, smooth loss: 165.775802\n",
      "iter 5260, loss: 158.791018, smooth loss: 165.813021\n",
      "iter 5270, loss: 141.029138, smooth loss: 165.810213\n",
      "iter 5280, loss: 161.455213, smooth loss: 165.696543\n",
      "iter 5290, loss: 171.693618, smooth loss: 165.630957\n",
      "iter 5300, loss: 169.097124, smooth loss: 165.530804\n",
      "iter 5310, loss: 178.172230, smooth loss: 165.428434\n",
      "iter 5320, loss: 172.581643, smooth loss: 165.503568\n",
      "iter 5330, loss: 145.050299, smooth loss: 165.511893\n",
      "iter 5340, loss: 182.613470, smooth loss: 165.391811\n",
      "iter 5350, loss: 166.534233, smooth loss: 165.323743\n",
      "iter 5360, loss: 151.747076, smooth loss: 165.255076\n",
      "iter 5370, loss: 151.924404, smooth loss: 165.269951\n",
      "iter 5380, loss: 192.724783, smooth loss: 165.252968\n",
      "iter 5390, loss: 159.318061, smooth loss: 165.226975\n",
      "iter 5400, loss: 149.059862, smooth loss: 165.040963\n",
      "iter 5410, loss: 150.668129, smooth loss: 164.989262\n",
      "iter 5420, loss: 171.567321, smooth loss: 165.059898\n",
      "iter 5430, loss: 149.406138, smooth loss: 165.068106\n",
      "iter 5440, loss: 177.262927, smooth loss: 164.961057\n",
      "iter 5450, loss: 167.174905, smooth loss: 164.859565\n",
      "iter 5460, loss: 235.710673, smooth loss: 164.903143\n",
      "iter 5470, loss: 158.483764, smooth loss: 164.798053\n",
      "iter 5480, loss: 155.003199, smooth loss: 164.649988\n",
      "iter 5490, loss: 172.134049, smooth loss: 164.578285\n",
      "iter 5500, loss: 146.852576, smooth loss: 164.551328\n",
      "iter 5510, loss: 166.154327, smooth loss: 164.501912\n",
      "iter 5520, loss: 157.362143, smooth loss: 164.453489\n",
      "iter 5530, loss: 171.312340, smooth loss: 164.432925\n",
      "iter 5540, loss: 190.765145, smooth loss: 164.284101\n",
      "iter 5550, loss: 125.825756, smooth loss: 164.259678\n",
      "iter 5560, loss: 152.303002, smooth loss: 164.210365\n",
      "iter 5570, loss: 165.804759, smooth loss: 164.176866\n",
      "iter 5580, loss: 141.882881, smooth loss: 164.162184\n",
      "iter 5590, loss: 152.420764, smooth loss: 164.059375\n",
      "iter 5600, loss: 157.898366, smooth loss: 163.960622\n",
      "iter 5610, loss: 154.181312, smooth loss: 163.875095\n",
      "iter 5620, loss: 147.970036, smooth loss: 163.755561\n",
      "iter 5630, loss: 171.159761, smooth loss: 163.637119\n",
      "iter 5640, loss: 162.393780, smooth loss: 163.614423\n",
      "iter 5650, loss: 136.151456, smooth loss: 163.669770\n",
      "iter 5660, loss: 155.852087, smooth loss: 163.582372\n",
      "iter 5670, loss: 180.742621, smooth loss: 163.481121\n",
      "iter 5680, loss: 195.464332, smooth loss: 163.429991\n",
      "iter 5690, loss: 167.862922, smooth loss: 163.325257\n",
      "iter 5700, loss: 170.934654, smooth loss: 163.436138\n",
      "iter 5710, loss: 158.798536, smooth loss: 163.415891\n",
      "iter 5720, loss: 164.710239, smooth loss: 163.300777\n",
      "iter 5730, loss: 147.263433, smooth loss: 163.212922\n",
      "iter 5740, loss: 171.791405, smooth loss: 163.374353\n",
      "iter 5750, loss: 146.774639, smooth loss: 163.382040\n",
      "iter 5760, loss: 158.542139, smooth loss: 163.324221\n",
      "iter 5770, loss: 139.260173, smooth loss: 163.194120\n",
      "iter 5780, loss: 179.406564, smooth loss: 163.230283\n",
      "iter 5790, loss: 146.664142, smooth loss: 163.158170\n",
      "iter 5800, loss: 150.638873, smooth loss: 163.053599\n",
      "iter 5810, loss: 165.284730, smooth loss: 162.960704\n",
      "iter 5820, loss: 127.474320, smooth loss: 162.883707\n",
      "iter 5830, loss: 174.745173, smooth loss: 162.869578\n",
      "iter 5840, loss: 187.855890, smooth loss: 162.897988\n",
      "iter 5850, loss: 155.735113, smooth loss: 162.780038\n",
      "iter 5860, loss: 159.557714, smooth loss: 162.732202\n",
      "iter 5870, loss: 146.608499, smooth loss: 162.635785\n",
      "iter 5880, loss: 171.264522, smooth loss: 162.496122\n",
      "iter 5890, loss: 176.196073, smooth loss: 162.628796\n",
      "iter 5900, loss: 145.692141, smooth loss: 162.646520\n",
      "iter 5910, loss: 133.075116, smooth loss: 162.512468\n",
      "iter 5920, loss: 151.762332, smooth loss: 162.469183\n",
      "iter 5930, loss: 154.553646, smooth loss: 162.410727\n",
      "iter 5940, loss: 163.486347, smooth loss: 162.435916\n",
      "iter 5950, loss: 143.211406, smooth loss: 162.369245\n",
      "iter 5960, loss: 162.212625, smooth loss: 162.429040\n",
      "iter 5970, loss: 127.331489, smooth loss: 162.257382\n",
      "iter 5980, loss: 165.888017, smooth loss: 162.172496\n",
      "iter 5990, loss: 173.017830, smooth loss: 162.256655\n",
      "iter 6000, loss: 161.584950, smooth loss: 162.254185\n",
      "iter 6010, loss: 146.856580, smooth loss: 162.169492\n",
      "iter 6020, loss: 154.456795, smooth loss: 162.078474\n",
      "iter 6030, loss: 155.545184, smooth loss: 162.052305\n",
      "iter 6040, loss: 144.708406, smooth loss: 162.064076\n",
      "iter 6050, loss: 160.820500, smooth loss: 161.935704\n",
      "iter 6060, loss: 126.269507, smooth loss: 161.845442\n",
      "iter 6070, loss: 182.657656, smooth loss: 161.852458\n",
      "iter 6080, loss: 120.235722, smooth loss: 161.784685\n",
      "iter 6090, loss: 172.574142, smooth loss: 161.739135\n",
      "iter 6100, loss: 155.332378, smooth loss: 161.712847\n",
      "iter 6110, loss: 155.851150, smooth loss: 161.566702\n",
      "iter 6120, loss: 144.789809, smooth loss: 161.589697\n",
      "iter 6130, loss: 178.611900, smooth loss: 161.533033\n",
      "iter 6140, loss: 145.693773, smooth loss: 161.476296\n",
      "iter 6150, loss: 160.940623, smooth loss: 161.510131\n",
      "iter 6160, loss: 150.491248, smooth loss: 161.418102\n",
      "iter 6170, loss: 159.461824, smooth loss: 161.340966\n",
      "iter 6180, loss: 141.856445, smooth loss: 161.263810\n",
      "iter 6190, loss: 168.005691, smooth loss: 161.147951\n",
      "iter 6200, loss: 151.213061, smooth loss: 161.017049\n",
      "iter 6210, loss: 160.596946, smooth loss: 160.948084\n",
      "iter 6220, loss: 169.776439, smooth loss: 161.077612\n",
      "iter 6230, loss: 143.712657, smooth loss: 160.974073\n",
      "iter 6240, loss: 151.288648, smooth loss: 160.886366\n",
      "iter 6250, loss: 146.437547, smooth loss: 160.823962\n",
      "iter 6260, loss: 165.671228, smooth loss: 160.713483\n",
      "iter 6270, loss: 164.988265, smooth loss: 160.860107\n",
      "iter 6280, loss: 145.126753, smooth loss: 160.860163\n",
      "iter 6290, loss: 128.198547, smooth loss: 160.687590\n",
      "iter 6300, loss: 162.874121, smooth loss: 160.643834\n",
      "iter 6310, loss: 185.358574, smooth loss: 160.809747\n",
      "iter 6320, loss: 155.371622, smooth loss: 160.852735\n",
      "iter 6330, loss: 183.437055, smooth loss: 160.799407\n",
      "iter 6340, loss: 140.123714, smooth loss: 160.703184\n",
      "iter 6350, loss: 175.727503, smooth loss: 160.676592\n",
      "iter 6360, loss: 149.999035, smooth loss: 160.628805\n",
      "iter 6370, loss: 131.113724, smooth loss: 160.552049\n",
      "iter 6380, loss: 181.707129, smooth loss: 160.434025\n",
      "iter 6390, loss: 162.713512, smooth loss: 160.402453\n",
      "iter 6400, loss: 143.854236, smooth loss: 160.330010\n",
      "iter 6410, loss: 155.737314, smooth loss: 160.359020\n",
      "iter 6420, loss: 141.592726, smooth loss: 160.284357\n",
      "iter 6430, loss: 154.413775, smooth loss: 160.234944\n",
      "iter 6440, loss: 162.909381, smooth loss: 160.160686\n",
      "iter 6450, loss: 124.334103, smooth loss: 160.001648\n",
      "iter 6460, loss: 142.960256, smooth loss: 160.117973\n",
      "iter 6470, loss: 164.837301, smooth loss: 160.197710\n",
      "iter 6480, loss: 168.574866, smooth loss: 160.076327\n",
      "iter 6490, loss: 141.852255, smooth loss: 160.007273\n",
      "iter 6500, loss: 179.192633, smooth loss: 159.942045\n",
      "iter 6510, loss: 172.333149, smooth loss: 159.974296\n",
      "iter 6520, loss: 175.829092, smooth loss: 159.940969\n",
      "iter 6530, loss: 186.226998, smooth loss: 160.011747\n",
      "iter 6540, loss: 155.600786, smooth loss: 159.882360\n",
      "iter 6550, loss: 153.579659, smooth loss: 159.741588\n",
      "iter 6560, loss: 163.090693, smooth loss: 159.812213\n",
      "iter 6570, loss: 176.592915, smooth loss: 159.825508\n",
      "iter 6580, loss: 158.190261, smooth loss: 159.767125\n",
      "iter 6590, loss: 172.817954, smooth loss: 159.684248\n",
      "iter 6600, loss: 153.265425, smooth loss: 159.673520\n",
      "iter 6610, loss: 137.029018, smooth loss: 159.671257\n",
      "iter 6620, loss: 149.885979, smooth loss: 159.563684\n",
      "iter 6630, loss: 149.554353, smooth loss: 159.510540\n",
      "iter 6640, loss: 172.132237, smooth loss: 159.454310\n",
      "iter 6650, loss: 162.742332, smooth loss: 159.439047\n",
      "iter 6660, loss: 137.305892, smooth loss: 159.368875\n",
      "iter 6670, loss: 177.138437, smooth loss: 159.348912\n",
      "iter 6680, loss: 134.904489, smooth loss: 159.204136\n",
      "iter 6690, loss: 149.118167, smooth loss: 159.257739\n",
      "iter 6700, loss: 122.920043, smooth loss: 159.181484\n",
      "iter 6710, loss: 179.225551, smooth loss: 159.173357\n",
      "iter 6720, loss: 143.817611, smooth loss: 159.196256\n",
      "iter 6730, loss: 138.018573, smooth loss: 159.092225\n",
      "iter 6740, loss: 132.039207, smooth loss: 159.050209\n",
      "iter 6750, loss: 160.650957, smooth loss: 158.967396\n",
      "iter 6760, loss: 162.974273, smooth loss: 158.842554\n",
      "iter 6770, loss: 142.647797, smooth loss: 158.749355\n",
      "iter 6780, loss: 136.903034, smooth loss: 158.628084\n",
      "iter 6790, loss: 141.123037, smooth loss: 158.756213\n",
      "iter 6800, loss: 125.673495, smooth loss: 158.721435\n",
      "iter 6810, loss: 172.294977, smooth loss: 158.626740\n",
      "iter 6820, loss: 175.129148, smooth loss: 158.565595\n",
      "iter 6830, loss: 151.805877, smooth loss: 158.445214\n",
      "iter 6840, loss: 181.631368, smooth loss: 158.531899\n",
      "iter 6850, loss: 180.561052, smooth loss: 158.611285\n",
      "iter 6860, loss: 120.709333, smooth loss: 158.475406\n",
      "iter 6870, loss: 133.585394, smooth loss: 158.393369\n",
      "iter 6880, loss: 192.386919, smooth loss: 158.521169\n",
      "iter 6890, loss: 171.886319, smooth loss: 158.598703\n",
      "iter 6900, loss: 150.713240, smooth loss: 158.542816\n",
      "iter 6910, loss: 142.725080, smooth loss: 158.491240\n",
      "iter 6920, loss: 163.977011, smooth loss: 158.399841\n",
      "iter 6930, loss: 135.496028, smooth loss: 158.437871\n",
      "iter 6940, loss: 187.516022, smooth loss: 158.365000\n",
      "iter 6950, loss: 109.004582, smooth loss: 158.190013\n",
      "iter 6960, loss: 166.913510, smooth loss: 158.181741\n",
      "iter 6970, loss: 165.985380, smooth loss: 158.107169\n",
      "iter 6980, loss: 152.649338, smooth loss: 158.154210\n",
      "iter 6990, loss: 145.969868, smooth loss: 158.103004\n",
      "iter 7000, loss: 157.439052, smooth loss: 158.033832\n",
      "iter 7010, loss: 151.046981, smooth loss: 157.983181\n",
      "iter 7020, loss: 133.142661, smooth loss: 157.857961\n",
      "iter 7030, loss: 146.746216, smooth loss: 157.924408\n",
      "iter 7040, loss: 164.986858, smooth loss: 158.007416\n",
      "iter 7050, loss: 148.747765, smooth loss: 157.875908\n",
      "iter 7060, loss: 205.188724, smooth loss: 157.857012\n",
      "iter 7070, loss: 135.956600, smooth loss: 157.732757\n",
      "iter 7080, loss: 193.282669, smooth loss: 157.776610\n",
      "iter 7090, loss: 169.188142, smooth loss: 157.769126\n",
      "iter 7100, loss: 167.726881, smooth loss: 157.789012\n",
      "iter 7110, loss: 140.353265, smooth loss: 157.709140\n",
      "iter 7120, loss: 141.626396, smooth loss: 157.565346\n",
      "iter 7130, loss: 181.417428, smooth loss: 157.657212\n",
      "iter 7140, loss: 166.588006, smooth loss: 157.646477\n",
      "iter 7150, loss: 127.418958, smooth loss: 157.626986\n",
      "iter 7160, loss: 134.098519, smooth loss: 157.554185\n",
      "iter 7170, loss: 192.844779, smooth loss: 157.522916\n",
      "iter 7180, loss: 133.623209, smooth loss: 157.592369\n",
      "iter 7190, loss: 133.266490, smooth loss: 157.442456\n",
      "iter 7200, loss: 137.115822, smooth loss: 157.375784\n",
      "iter 7210, loss: 145.405077, smooth loss: 157.322493\n",
      "iter 7220, loss: 166.114727, smooth loss: 157.315503\n",
      "iter 7230, loss: 160.406053, smooth loss: 157.265911\n",
      "iter 7240, loss: 133.506810, smooth loss: 157.197865\n",
      "iter 7250, loss: 141.401000, smooth loss: 157.136903\n",
      "iter 7260, loss: 146.990087, smooth loss: 157.148593\n",
      "iter 7270, loss: 112.480428, smooth loss: 157.084182\n",
      "iter 7280, loss: 136.239969, smooth loss: 157.041737\n",
      "iter 7290, loss: 153.997778, smooth loss: 157.108624\n",
      "iter 7300, loss: 148.204119, smooth loss: 157.031522\n",
      "iter 7310, loss: 158.567708, smooth loss: 157.017786\n",
      "iter 7320, loss: 150.247682, smooth loss: 156.903723\n",
      "iter 7330, loss: 127.504216, smooth loss: 156.767367\n",
      "iter 7340, loss: 147.168212, smooth loss: 156.681165\n",
      "iter 7350, loss: 144.601664, smooth loss: 156.592157\n",
      "iter 7360, loss: 177.761757, smooth loss: 156.696640\n",
      "iter 7370, loss: 179.795042, smooth loss: 156.682883\n",
      "iter 7380, loss: 134.172416, smooth loss: 156.529563\n",
      "iter 7390, loss: 140.047259, smooth loss: 156.505446\n",
      "iter 7400, loss: 140.487415, smooth loss: 156.398187\n",
      "iter 7410, loss: 166.493656, smooth loss: 156.459624\n",
      "iter 7420, loss: 148.696982, smooth loss: 156.495125\n",
      "iter 7430, loss: 160.824894, smooth loss: 156.495325\n",
      "iter 7440, loss: 167.866586, smooth loss: 156.386274\n",
      "iter 7450, loss: 191.142628, smooth loss: 156.429480\n",
      "iter 7460, loss: 179.279368, smooth loss: 156.561450\n",
      "iter 7470, loss: 163.234992, smooth loss: 156.520128\n",
      "iter 7480, loss: 134.736022, smooth loss: 156.449832\n",
      "iter 7490, loss: 141.693861, smooth loss: 156.335818\n",
      "iter 7500, loss: 160.111137, smooth loss: 156.437531\n",
      "iter 7510, loss: 144.687113, smooth loss: 156.306365\n",
      "iter 7520, loss: 136.098156, smooth loss: 156.235406\n",
      "iter 7530, loss: 157.084397, smooth loss: 156.165163\n",
      "iter 7540, loss: 168.828827, smooth loss: 156.074145\n",
      "iter 7550, loss: 153.427384, smooth loss: 156.141470\n",
      "iter 7560, loss: 125.829856, smooth loss: 156.101550\n",
      "iter 7570, loss: 139.839267, smooth loss: 156.019887\n",
      "iter 7580, loss: 148.318673, smooth loss: 155.996264\n",
      "iter 7590, loss: 141.569622, smooth loss: 155.871514\n",
      "iter 7600, loss: 178.766529, smooth loss: 155.916602\n",
      "iter 7610, loss: 179.157503, smooth loss: 155.963669\n",
      "iter 7620, loss: 132.204774, smooth loss: 155.910875\n",
      "iter 7630, loss: 112.667748, smooth loss: 155.802660\n",
      "iter 7640, loss: 134.621620, smooth loss: 155.751904\n",
      "iter 7650, loss: 136.976338, smooth loss: 155.746599\n",
      "iter 7660, loss: 139.091639, smooth loss: 155.746518\n",
      "iter 7670, loss: 159.885094, smooth loss: 155.786725\n",
      "iter 7680, loss: 122.571905, smooth loss: 155.763590\n",
      "iter 7690, loss: 130.698625, smooth loss: 155.581671\n",
      "iter 7700, loss: 158.716388, smooth loss: 155.594334\n",
      "iter 7710, loss: 153.480391, smooth loss: 155.658578\n",
      "iter 7720, loss: 128.842502, smooth loss: 155.683881\n",
      "iter 7730, loss: 146.188751, smooth loss: 155.638248\n",
      "iter 7740, loss: 148.340960, smooth loss: 155.546134\n",
      "iter 7750, loss: 157.190149, smooth loss: 155.636980\n",
      "iter 7760, loss: 138.492038, smooth loss: 155.512673\n",
      "iter 7770, loss: 163.918002, smooth loss: 155.444963\n",
      "iter 7780, loss: 151.816410, smooth loss: 155.352334\n",
      "iter 7790, loss: 145.605974, smooth loss: 155.354069\n",
      "iter 7800, loss: 153.648963, smooth loss: 155.321330\n",
      "iter 7810, loss: 145.206200, smooth loss: 155.281118\n",
      "iter 7820, loss: 131.752024, smooth loss: 155.253347\n",
      "iter 7830, loss: 172.902873, smooth loss: 155.192813\n",
      "iter 7840, loss: 172.335272, smooth loss: 155.176491\n",
      "iter 7850, loss: 145.329599, smooth loss: 155.115051\n",
      "iter 7860, loss: 184.266453, smooth loss: 155.189399\n",
      "iter 7870, loss: 151.999512, smooth loss: 155.147400\n",
      "iter 7880, loss: 167.667015, smooth loss: 155.099808\n",
      "iter 7890, loss: 161.218432, smooth loss: 155.015468\n",
      "iter 7900, loss: 128.459833, smooth loss: 154.900226\n",
      "iter 7910, loss: 153.704681, smooth loss: 154.811335\n",
      "iter 7920, loss: 133.474170, smooth loss: 154.681093\n",
      "iter 7930, loss: 195.407522, smooth loss: 154.724178\n",
      "iter 7940, loss: 140.578327, smooth loss: 154.745076\n",
      "iter 7950, loss: 131.812679, smooth loss: 154.660516\n",
      "iter 7960, loss: 148.927044, smooth loss: 154.640818\n",
      "iter 7970, loss: 117.351881, smooth loss: 154.513753\n",
      "iter 7980, loss: 143.733547, smooth loss: 154.468664\n",
      "iter 7990, loss: 186.998128, smooth loss: 154.641002\n",
      "iter 8000, loss: 154.901903, smooth loss: 154.621180\n",
      "iter 8010, loss: 144.605755, smooth loss: 154.500483\n",
      "iter 8020, loss: 163.408078, smooth loss: 154.481379\n",
      "iter 8030, loss: 151.062005, smooth loss: 154.630150\n",
      "iter 8040, loss: 152.608746, smooth loss: 154.638528\n",
      "iter 8050, loss: 160.360629, smooth loss: 154.631824\n",
      "iter 8060, loss: 142.208817, smooth loss: 154.488652\n",
      "iter 8070, loss: 159.253156, smooth loss: 154.573753\n",
      "iter 8080, loss: 143.209789, smooth loss: 154.479089\n",
      "iter 8090, loss: 144.420493, smooth loss: 154.400823\n",
      "iter 8100, loss: 169.539866, smooth loss: 154.331049\n",
      "iter 8110, loss: 126.441017, smooth loss: 154.190662\n",
      "iter 8120, loss: 149.820355, smooth loss: 154.254092\n",
      "iter 8130, loss: 133.076070, smooth loss: 154.292354\n",
      "iter 8140, loss: 148.678479, smooth loss: 154.176551\n",
      "iter 8150, loss: 168.403756, smooth loss: 154.138196\n",
      "iter 8160, loss: 165.387084, smooth loss: 154.064655\n",
      "iter 8170, loss: 167.677672, smooth loss: 153.979142\n",
      "iter 8180, loss: 156.980687, smooth loss: 154.058221\n",
      "iter 8190, loss: 135.729662, smooth loss: 154.102377\n",
      "iter 8200, loss: 177.781750, smooth loss: 154.011870\n",
      "iter 8210, loss: 154.617465, smooth loss: 153.924141\n",
      "iter 8220, loss: 142.024403, smooth loss: 153.876719\n",
      "iter 8230, loss: 148.339213, smooth loss: 153.924756\n",
      "iter 8240, loss: 184.827535, smooth loss: 153.938217\n",
      "iter 8250, loss: 150.807626, smooth loss: 153.954200\n",
      "iter 8260, loss: 140.217852, smooth loss: 153.779608\n",
      "iter 8270, loss: 136.245750, smooth loss: 153.730087\n",
      "iter 8280, loss: 160.409465, smooth loss: 153.841636\n",
      "iter 8290, loss: 142.019337, smooth loss: 153.886334\n",
      "iter 8300, loss: 169.963600, smooth loss: 153.814749\n",
      "iter 8310, loss: 154.242502, smooth loss: 153.736522\n",
      "iter 8320, loss: 234.573603, smooth loss: 153.788484\n",
      "iter 8330, loss: 155.401339, smooth loss: 153.719390\n",
      "iter 8340, loss: 142.494484, smooth loss: 153.595575\n",
      "iter 8350, loss: 169.681290, smooth loss: 153.541930\n",
      "iter 8360, loss: 142.566494, smooth loss: 153.548479\n",
      "iter 8370, loss: 156.590409, smooth loss: 153.515977\n",
      "iter 8380, loss: 147.524687, smooth loss: 153.480522\n",
      "iter 8390, loss: 161.812466, smooth loss: 153.487220\n",
      "iter 8400, loss: 183.212716, smooth loss: 153.367347\n",
      "iter 8410, loss: 114.951614, smooth loss: 153.368794\n",
      "iter 8420, loss: 143.569817, smooth loss: 153.343734\n",
      "iter 8430, loss: 157.860382, smooth loss: 153.347534\n",
      "iter 8440, loss: 135.306353, smooth loss: 153.379278\n",
      "iter 8450, loss: 143.343390, smooth loss: 153.297887\n",
      "iter 8460, loss: 147.282459, smooth loss: 153.215097\n",
      "iter 8470, loss: 143.881982, smooth loss: 153.152278\n",
      "iter 8480, loss: 135.679937, smooth loss: 153.042814\n",
      "iter 8490, loss: 156.200036, smooth loss: 152.913719\n",
      "iter 8500, loss: 152.587131, smooth loss: 152.907459\n",
      "iter 8510, loss: 134.078791, smooth loss: 152.988552\n",
      "iter 8520, loss: 149.903321, smooth loss: 152.920078\n",
      "iter 8530, loss: 174.275819, smooth loss: 152.844303\n",
      "iter 8540, loss: 187.614307, smooth loss: 152.794564\n",
      "iter 8550, loss: 166.886812, smooth loss: 152.702323\n",
      "iter 8560, loss: 164.186957, smooth loss: 152.842569\n",
      "iter 8570, loss: 146.096087, smooth loss: 152.843124\n",
      "iter 8580, loss: 159.758672, smooth loss: 152.733700\n",
      "iter 8590, loss: 138.822395, smooth loss: 152.659322\n",
      "iter 8600, loss: 165.929221, smooth loss: 152.860747\n",
      "iter 8610, loss: 138.833045, smooth loss: 152.907720\n",
      "iter 8620, loss: 157.267180, smooth loss: 152.877274\n",
      "iter 8630, loss: 125.325035, smooth loss: 152.746343\n",
      "iter 8640, loss: 174.613375, smooth loss: 152.809531\n",
      "iter 8650, loss: 134.982416, smooth loss: 152.765943\n",
      "iter 8660, loss: 148.338085, smooth loss: 152.682871\n",
      "iter 8670, loss: 157.370006, smooth loss: 152.579678\n",
      "iter 8680, loss: 114.559417, smooth loss: 152.512015\n",
      "iter 8690, loss: 164.579570, smooth loss: 152.508547\n",
      "iter 8700, loss: 182.658958, smooth loss: 152.568149\n",
      "iter 8710, loss: 147.776314, smooth loss: 152.452257\n",
      "iter 8720, loss: 148.928940, smooth loss: 152.410620\n",
      "iter 8730, loss: 128.259425, smooth loss: 152.336271\n",
      "iter 8740, loss: 166.249801, smooth loss: 152.208261\n",
      "iter 8750, loss: 158.310603, smooth loss: 152.348755\n",
      "iter 8760, loss: 138.557703, smooth loss: 152.387863\n",
      "iter 8770, loss: 129.134099, smooth loss: 152.269211\n",
      "iter 8780, loss: 143.403084, smooth loss: 152.218533\n",
      "iter 8790, loss: 145.278078, smooth loss: 152.165070\n",
      "iter 8800, loss: 151.202261, smooth loss: 152.205149\n",
      "iter 8810, loss: 132.630356, smooth loss: 152.163640\n",
      "iter 8820, loss: 152.230759, smooth loss: 152.258343\n",
      "iter 8830, loss: 119.257333, smooth loss: 152.104707\n",
      "iter 8840, loss: 158.689709, smooth loss: 152.018494\n",
      "iter 8850, loss: 166.681516, smooth loss: 152.136640\n",
      "iter 8860, loss: 153.586156, smooth loss: 152.149640\n",
      "iter 8870, loss: 139.459328, smooth loss: 152.099969\n",
      "iter 8880, loss: 148.376247, smooth loss: 152.030711\n",
      "iter 8890, loss: 149.776242, smooth loss: 151.994899\n",
      "iter 8900, loss: 138.880943, smooth loss: 152.032751\n",
      "iter 8910, loss: 150.024650, smooth loss: 151.938713\n",
      "iter 8920, loss: 120.375554, smooth loss: 151.849765\n",
      "iter 8930, loss: 175.173386, smooth loss: 151.887920\n",
      "iter 8940, loss: 114.860663, smooth loss: 151.840594\n",
      "iter 8950, loss: 164.715674, smooth loss: 151.798855\n",
      "iter 8960, loss: 148.954025, smooth loss: 151.784639\n",
      "iter 8970, loss: 148.481097, smooth loss: 151.664541\n",
      "iter 8980, loss: 135.825024, smooth loss: 151.711535\n",
      "iter 8990, loss: 176.883925, smooth loss: 151.666360\n",
      "iter 9000, loss: 136.361548, smooth loss: 151.637062\n",
      "iter 9010, loss: 152.323206, smooth loss: 151.710799\n",
      "iter 9020, loss: 143.070847, smooth loss: 151.635272\n",
      "iter 9030, loss: 150.267518, smooth loss: 151.575510\n",
      "iter 9040, loss: 130.603912, smooth loss: 151.513393\n",
      "iter 9050, loss: 166.314036, smooth loss: 151.407688\n",
      "iter 9060, loss: 138.912368, smooth loss: 151.269761\n",
      "iter 9070, loss: 155.919500, smooth loss: 151.192866\n",
      "iter 9080, loss: 164.352908, smooth loss: 151.335005\n",
      "iter 9090, loss: 136.159560, smooth loss: 151.250021\n",
      "iter 9100, loss: 142.426030, smooth loss: 151.185263\n",
      "iter 9110, loss: 136.426111, smooth loss: 151.130974\n",
      "iter 9120, loss: 149.225334, smooth loss: 151.008669\n",
      "iter 9130, loss: 151.952668, smooth loss: 151.186968\n",
      "iter 9140, loss: 131.433443, smooth loss: 151.214220\n",
      "iter 9150, loss: 121.543139, smooth loss: 151.036876\n",
      "iter 9160, loss: 148.666845, smooth loss: 151.005307\n",
      "iter 9170, loss: 178.830558, smooth loss: 151.196551\n",
      "iter 9180, loss: 145.439803, smooth loss: 151.271565\n",
      "iter 9190, loss: 179.149333, smooth loss: 151.237981\n",
      "iter 9200, loss: 130.702646, smooth loss: 151.154151\n",
      "iter 9210, loss: 165.844932, smooth loss: 151.130968\n",
      "iter 9220, loss: 145.083967, smooth loss: 151.113535\n",
      "iter 9230, loss: 126.139549, smooth loss: 151.044364\n",
      "iter 9240, loss: 176.016300, smooth loss: 150.926672\n",
      "iter 9250, loss: 156.660910, smooth loss: 150.909555\n",
      "iter 9260, loss: 137.171789, smooth loss: 150.837311\n",
      "iter 9270, loss: 143.706111, smooth loss: 150.881231\n",
      "iter 9280, loss: 130.949672, smooth loss: 150.809107\n",
      "iter 9290, loss: 148.721659, smooth loss: 150.768524\n",
      "iter 9300, loss: 150.953073, smooth loss: 150.718662\n",
      "iter 9310, loss: 117.336306, smooth loss: 150.547401\n",
      "iter 9320, loss: 126.023524, smooth loss: 150.677001\n",
      "iter 9330, loss: 154.699040, smooth loss: 150.766205\n",
      "iter 9340, loss: 164.542360, smooth loss: 150.645639\n",
      "iter 9350, loss: 130.817106, smooth loss: 150.580198\n",
      "iter 9360, loss: 168.183600, smooth loss: 150.517873\n",
      "iter 9370, loss: 163.341989, smooth loss: 150.553907\n",
      "iter 9380, loss: 170.629109, smooth loss: 150.534322\n",
      "iter 9390, loss: 178.231540, smooth loss: 150.641170\n",
      "iter 9400, loss: 147.601005, smooth loss: 150.527104\n",
      "iter 9410, loss: 141.245398, smooth loss: 150.377712\n",
      "iter 9420, loss: 157.686282, smooth loss: 150.469752\n",
      "iter 9430, loss: 167.457308, smooth loss: 150.500496\n",
      "iter 9440, loss: 155.165408, smooth loss: 150.475014\n",
      "iter 9450, loss: 167.517496, smooth loss: 150.407068\n",
      "iter 9460, loss: 140.610604, smooth loss: 150.393683\n",
      "iter 9470, loss: 130.853281, smooth loss: 150.409906\n",
      "iter 9480, loss: 149.537713, smooth loss: 150.333934\n",
      "iter 9490, loss: 146.919907, smooth loss: 150.279494\n",
      "iter 9500, loss: 171.119744, smooth loss: 150.256462\n",
      "iter 9510, loss: 155.227428, smooth loss: 150.260557\n",
      "iter 9520, loss: 129.447940, smooth loss: 150.195204\n",
      "iter 9530, loss: 172.510659, smooth loss: 150.179441\n",
      "iter 9540, loss: 124.571070, smooth loss: 150.052865\n",
      "iter 9550, loss: 144.138674, smooth loss: 150.129031\n",
      "iter 9560, loss: 110.917349, smooth loss: 150.055142\n",
      "iter 9570, loss: 171.909040, smooth loss: 150.078045\n",
      "iter 9580, loss: 137.419740, smooth loss: 150.131281\n",
      "iter 9590, loss: 128.915571, smooth loss: 150.040342\n",
      "iter 9600, loss: 126.912632, smooth loss: 150.016588\n",
      "iter 9610, loss: 160.079225, smooth loss: 149.955554\n",
      "iter 9620, loss: 157.108326, smooth loss: 149.831482\n",
      "iter 9630, loss: 135.883843, smooth loss: 149.732889\n",
      "iter 9640, loss: 132.277219, smooth loss: 149.593340\n",
      "iter 9650, loss: 130.515348, smooth loss: 149.739011\n",
      "iter 9660, loss: 114.005448, smooth loss: 149.724013\n",
      "iter 9670, loss: 162.926676, smooth loss: 149.651557\n",
      "iter 9680, loss: 163.272683, smooth loss: 149.590403\n",
      "iter 9690, loss: 143.802001, smooth loss: 149.456354\n",
      "iter 9700, loss: 173.927883, smooth loss: 149.566352\n",
      "iter 9710, loss: 174.637274, smooth loss: 149.672906\n",
      "iter 9720, loss: 107.779369, smooth loss: 149.528352\n",
      "iter 9730, loss: 126.354105, smooth loss: 149.467733\n",
      "iter 9740, loss: 183.471880, smooth loss: 149.601160\n",
      "iter 9750, loss: 165.646164, smooth loss: 149.704820\n",
      "iter 9760, loss: 144.112913, smooth loss: 149.660272\n",
      "iter 9770, loss: 134.074424, smooth loss: 149.629482\n",
      "iter 9780, loss: 158.240209, smooth loss: 149.541798\n",
      "iter 9790, loss: 124.615745, smooth loss: 149.597125\n",
      "iter 9800, loss: 179.498984, smooth loss: 149.526160\n",
      "iter 9810, loss: 102.164338, smooth loss: 149.355811\n",
      "iter 9820, loss: 157.515895, smooth loss: 149.354648\n",
      "iter 9830, loss: 160.970902, smooth loss: 149.277953\n",
      "iter 9840, loss: 147.660323, smooth loss: 149.341013\n",
      "iter 9850, loss: 138.060031, smooth loss: 149.289982\n",
      "iter 9860, loss: 150.466699, smooth loss: 149.229301\n",
      "iter 9870, loss: 148.651173, smooth loss: 149.201662\n",
      "iter 9880, loss: 114.469722, smooth loss: 149.057752\n",
      "iter 9890, loss: 136.584139, smooth loss: 149.146391\n",
      "iter 9900, loss: 160.125947, smooth loss: 149.223845\n",
      "iter 9910, loss: 146.371056, smooth loss: 149.086509\n",
      "iter 9920, loss: 195.552836, smooth loss: 149.087962\n",
      "iter 9930, loss: 131.193232, smooth loss: 148.954155\n",
      "iter 9940, loss: 175.757459, smooth loss: 148.994866\n",
      "iter 9950, loss: 162.295150, smooth loss: 148.996602\n",
      "iter 9960, loss: 160.669721, smooth loss: 149.046937\n",
      "iter 9970, loss: 131.274082, smooth loss: 148.979965\n",
      "iter 9980, loss: 133.194260, smooth loss: 148.838691\n",
      "iter 9990, loss: 177.320500, smooth loss: 148.940184\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 10000\n",
    "\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 10\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    \n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    if loss < 15:\n",
    "        break\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " ipsum naeticus constitur sasebul neque tulta, ipsa nihil\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['i']  # Set the seed character index\n",
    "num_predictions = 50  # Set the desired number of predictions\n",
    "predictions = sample(hprev, 'ipsum ', num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " legendum Phillitare, ut ditperia commata ab insos, ut omngeni nihitine altebunt. Fitbumem et nisi eff\n"
     ]
    }
   ],
   "source": [
    "def sample2(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['l']  # Set the seed character index\n",
    "num_predictions = 100  # Set the desired number of predictions\n",
    "predictions = sample2(hprev, seed_ix, num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
