{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 10996 characters, 64 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('pizza.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 100 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # ‚àÇloss/‚àÇy\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWy and ‚àÇloss/‚àÇby\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) \n",
    "         \n",
    "        # ‚àÇloss/‚àÇWh, ‚àÇloss/‚àÇUh and ‚àÇloss/‚àÇbh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) \n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWr, ‚àÇloss/‚àÇUr and ‚àÇloss/‚àÇbr\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWz, ‚àÇloss/‚àÇUz and ‚àÇloss/‚àÇbz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇhùë°‚Çã‚ÇÅ\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_string, n):\n",
    "    # Convert the seed string to a list of indices\n",
    "    seed_ix = [char_to_ix[ch] for ch in seed_string]\n",
    "    ixes = []\n",
    "\n",
    "    # Initialize the first word of sample as one-hot encoded vectors for each character in the seed\n",
    "    for ix in seed_ix:\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        # Append the index to the list of generated indices\n",
    "        ixes.append(ix)\n",
    "\n",
    "    # Now generate n characters\n",
    "    for t in range(n):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ixes[-1]] = 1  # Start with the last character from the seed\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        y = np.dot(Wy, h) + by\n",
    "        p = softmax(y)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        ixes.append(ix)\n",
    "\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 415.900449, smooth loss: 415.888320\n",
      "iter 10, loss: 288.623470, smooth loss: 415.131319\n",
      "iter 20, loss: 274.263925, smooth loss: 413.827941\n",
      "iter 30, loss: 259.649762, smooth loss: 412.292689\n",
      "iter 40, loss: 232.154458, smooth loss: 410.634563\n",
      "iter 50, loss: 239.176866, smooth loss: 408.921493\n",
      "iter 60, loss: 217.099266, smooth loss: 407.156857\n",
      "iter 70, loss: 225.413362, smooth loss: 405.437359\n",
      "iter 80, loss: 227.164043, smooth loss: 403.713075\n",
      "iter 90, loss: 216.485251, smooth loss: 401.933586\n",
      "iter 100, loss: 212.011335, smooth loss: 400.168355\n",
      "iter 110, loss: 220.035011, smooth loss: 398.444025\n",
      "iter 120, loss: 229.158327, smooth loss: 396.682558\n",
      "iter 130, loss: 209.990703, smooth loss: 394.889009\n",
      "iter 140, loss: 239.835605, smooth loss: 393.049194\n",
      "iter 150, loss: 207.408979, smooth loss: 391.141227\n",
      "iter 160, loss: 197.660518, smooth loss: 389.308286\n",
      "iter 170, loss: 202.233546, smooth loss: 387.443636\n",
      "iter 180, loss: 218.464090, smooth loss: 385.686480\n",
      "iter 190, loss: 205.644807, smooth loss: 383.875337\n",
      "iter 200, loss: 203.208026, smooth loss: 382.049005\n",
      "iter 210, loss: 209.775027, smooth loss: 380.285204\n",
      "iter 220, loss: 196.440759, smooth loss: 378.546845\n",
      "iter 230, loss: 190.752625, smooth loss: 376.780604\n",
      "iter 240, loss: 197.564456, smooth loss: 374.994739\n",
      "iter 250, loss: 176.973274, smooth loss: 373.126328\n",
      "iter 260, loss: 185.329984, smooth loss: 371.255528\n",
      "iter 270, loss: 182.898074, smooth loss: 369.450915\n",
      "iter 280, loss: 186.373813, smooth loss: 367.635029\n",
      "iter 290, loss: 174.683384, smooth loss: 365.920164\n",
      "iter 300, loss: 179.406731, smooth loss: 364.146997\n",
      "iter 310, loss: 181.286902, smooth loss: 362.366495\n",
      "iter 320, loss: 194.507211, smooth loss: 360.668296\n",
      "iter 330, loss: 193.652969, smooth loss: 358.984408\n",
      "iter 340, loss: 174.130838, smooth loss: 357.256518\n",
      "iter 350, loss: 168.163740, smooth loss: 355.507121\n",
      "iter 360, loss: 194.800073, smooth loss: 353.714365\n",
      "iter 370, loss: 169.141332, smooth loss: 351.891186\n",
      "iter 380, loss: 144.234937, smooth loss: 350.114168\n",
      "iter 390, loss: 182.926077, smooth loss: 348.404645\n",
      "iter 400, loss: 192.795256, smooth loss: 346.775181\n",
      "iter 410, loss: 175.665272, smooth loss: 345.044907\n",
      "iter 420, loss: 203.991950, smooth loss: 343.381551\n",
      "iter 430, loss: 172.119929, smooth loss: 341.710630\n",
      "iter 440, loss: 171.267970, smooth loss: 340.102628\n",
      "iter 450, loss: 156.448540, smooth loss: 338.432134\n",
      "iter 460, loss: 157.527170, smooth loss: 336.743536\n",
      "iter 470, loss: 178.538995, smooth loss: 335.037451\n",
      "iter 480, loss: 128.380021, smooth loss: 333.253550\n",
      "iter 490, loss: 189.419119, smooth loss: 331.603752\n",
      "iter 500, loss: 188.399592, smooth loss: 329.971350\n",
      "iter 510, loss: 178.770402, smooth loss: 328.414539\n",
      "iter 520, loss: 173.882603, smooth loss: 326.742434\n",
      "iter 530, loss: 166.561940, smooth loss: 325.163883\n",
      "iter 540, loss: 192.002718, smooth loss: 323.586224\n",
      "iter 550, loss: 169.087966, smooth loss: 322.031383\n",
      "iter 560, loss: 152.888529, smooth loss: 320.417610\n",
      "iter 570, loss: 147.655838, smooth loss: 318.794829\n",
      "iter 580, loss: 155.583313, smooth loss: 317.174118\n",
      "iter 590, loss: 156.247501, smooth loss: 315.484078\n",
      "iter 600, loss: 141.633395, smooth loss: 313.898274\n",
      "iter 610, loss: 170.777378, smooth loss: 312.376430\n",
      "iter 620, loss: 166.654057, smooth loss: 310.901054\n",
      "iter 630, loss: 198.914728, smooth loss: 309.340098\n",
      "iter 640, loss: 145.300129, smooth loss: 307.796184\n",
      "iter 650, loss: 152.331507, smooth loss: 306.307304\n",
      "iter 660, loss: 153.532829, smooth loss: 304.830502\n",
      "iter 670, loss: 136.120995, smooth loss: 303.269142\n",
      "iter 680, loss: 122.169380, smooth loss: 301.710353\n",
      "iter 690, loss: 140.927602, smooth loss: 300.198665\n",
      "iter 700, loss: 176.665482, smooth loss: 298.635740\n",
      "iter 710, loss: 158.259329, smooth loss: 297.106563\n",
      "iter 720, loss: 159.198139, smooth loss: 295.673850\n",
      "iter 730, loss: 138.850720, smooth loss: 294.274143\n",
      "iter 740, loss: 146.261405, smooth loss: 292.796783\n",
      "iter 750, loss: 150.041932, smooth loss: 291.343554\n",
      "iter 760, loss: 147.183091, smooth loss: 289.931517\n",
      "iter 770, loss: 176.820259, smooth loss: 288.565481\n",
      "iter 780, loss: 119.270465, smooth loss: 287.018477\n",
      "iter 790, loss: 127.309091, smooth loss: 285.543183\n",
      "iter 800, loss: 127.296965, smooth loss: 284.113609\n",
      "iter 810, loss: 152.658520, smooth loss: 282.665558\n",
      "iter 820, loss: 135.375191, smooth loss: 281.205202\n",
      "iter 830, loss: 145.959930, smooth loss: 279.873735\n",
      "iter 840, loss: 134.644810, smooth loss: 278.554865\n",
      "iter 850, loss: 107.505592, smooth loss: 277.136254\n",
      "iter 860, loss: 122.314657, smooth loss: 275.788813\n",
      "iter 870, loss: 151.432465, smooth loss: 274.483389\n",
      "iter 880, loss: 137.643631, smooth loss: 273.182724\n",
      "iter 890, loss: 126.808990, smooth loss: 271.693224\n",
      "iter 900, loss: 133.683482, smooth loss: 270.302834\n",
      "iter 910, loss: 106.166952, smooth loss: 268.917516\n",
      "iter 920, loss: 131.331736, smooth loss: 267.574421\n",
      "iter 930, loss: 138.072517, smooth loss: 266.213324\n",
      "iter 940, loss: 139.769230, smooth loss: 264.979033\n",
      "iter 950, loss: 121.166170, smooth loss: 263.729472\n",
      "iter 960, loss: 130.830388, smooth loss: 262.415812\n",
      "iter 970, loss: 157.484391, smooth loss: 261.174765\n",
      "iter 980, loss: 127.672939, smooth loss: 259.933101\n",
      "iter 990, loss: 121.734490, smooth loss: 258.726781\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 1000\n",
    "\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 10\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    \n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    if loss < 15:\n",
    "        break\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " pizza will undoubtedly oproucing with phopoustitic indiuristed the worke \n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['i']  # Set the seed character index\n",
    "num_predictions = 50  # Set the desired number of predictions\n",
    "predictions = sample(hprev, 'pizza will undoubtedly ', num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " pions in the exporn a for kistping cans, pizza, chusf on the wrirt hos icsnocing colustrar. \n",
      "nt yrust\n"
     ]
    }
   ],
   "source": [
    "def sample2(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['p']  # Set the seed character index\n",
    "num_predictions = 100  # Set the desired number of predictions\n",
    "predictions = sample2(hprev, seed_ix, num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
