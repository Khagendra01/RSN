{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 10996 characters, 64 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('pizza.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 100 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # ‚àÇloss/‚àÇy\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWy and ‚àÇloss/‚àÇby\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) \n",
    "         \n",
    "        # ‚àÇloss/‚àÇWh, ‚àÇloss/‚àÇUh and ‚àÇloss/‚àÇbh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) \n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWr, ‚àÇloss/‚àÇUr and ‚àÇloss/‚àÇbr\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇWz, ‚àÇloss/‚àÇUz and ‚àÇloss/‚àÇbz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # ‚àÇloss/‚àÇhùë°‚Çã‚ÇÅ\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_string, n):\n",
    "    # Convert the seed string to a list of indices\n",
    "    seed_ix = [char_to_ix[ch] for ch in seed_string]\n",
    "    ixes = []\n",
    "\n",
    "    # Initialize the first word of sample as one-hot encoded vectors for each character in the seed\n",
    "    for ix in seed_ix:\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        # Append the index to the list of generated indices\n",
    "        ixes.append(ix)\n",
    "\n",
    "    # Now generate n characters\n",
    "    for t in range(n):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ixes[-1]] = 1  # Start with the last character from the seed\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        y = np.dot(Wy, h) + by\n",
    "        p = softmax(y)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        ixes.append(ix)\n",
    "\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 415.900449, smooth loss: 415.888320\n",
      "iter 10, loss: 288.623470, smooth loss: 415.131319\n",
      "iter 20, loss: 274.263925, smooth loss: 413.827941\n",
      "iter 30, loss: 259.649762, smooth loss: 412.292689\n",
      "iter 40, loss: 232.154458, smooth loss: 410.634563\n",
      "iter 50, loss: 239.176866, smooth loss: 408.921493\n",
      "iter 60, loss: 217.099266, smooth loss: 407.156857\n",
      "iter 70, loss: 225.413362, smooth loss: 405.437359\n",
      "iter 80, loss: 227.164043, smooth loss: 403.713075\n",
      "iter 90, loss: 216.485251, smooth loss: 401.933586\n",
      "iter 100, loss: 212.011335, smooth loss: 400.168355\n",
      "iter 110, loss: 220.035011, smooth loss: 398.444025\n",
      "iter 120, loss: 229.158327, smooth loss: 396.682558\n",
      "iter 130, loss: 209.990703, smooth loss: 394.889009\n",
      "iter 140, loss: 239.835605, smooth loss: 393.049194\n",
      "iter 150, loss: 207.408979, smooth loss: 391.141227\n",
      "iter 160, loss: 197.660518, smooth loss: 389.308286\n",
      "iter 170, loss: 202.233546, smooth loss: 387.443636\n",
      "iter 180, loss: 218.464090, smooth loss: 385.686480\n",
      "iter 190, loss: 205.644807, smooth loss: 383.875337\n",
      "iter 200, loss: 203.208026, smooth loss: 382.049005\n",
      "iter 210, loss: 209.775027, smooth loss: 380.285204\n",
      "iter 220, loss: 196.440759, smooth loss: 378.546845\n",
      "iter 230, loss: 190.752625, smooth loss: 376.780604\n",
      "iter 240, loss: 197.564456, smooth loss: 374.994739\n",
      "iter 250, loss: 176.973274, smooth loss: 373.126328\n",
      "iter 260, loss: 185.329984, smooth loss: 371.255528\n",
      "iter 270, loss: 182.898074, smooth loss: 369.450915\n",
      "iter 280, loss: 186.373813, smooth loss: 367.635029\n",
      "iter 290, loss: 174.683384, smooth loss: 365.920164\n",
      "iter 300, loss: 179.406731, smooth loss: 364.146997\n",
      "iter 310, loss: 181.286902, smooth loss: 362.366495\n",
      "iter 320, loss: 194.507211, smooth loss: 360.668296\n",
      "iter 330, loss: 193.652969, smooth loss: 358.984408\n",
      "iter 340, loss: 174.130838, smooth loss: 357.256518\n",
      "iter 350, loss: 168.163740, smooth loss: 355.507121\n",
      "iter 360, loss: 194.800073, smooth loss: 353.714365\n",
      "iter 370, loss: 169.141332, smooth loss: 351.891186\n",
      "iter 380, loss: 144.234937, smooth loss: 350.114168\n",
      "iter 390, loss: 182.926077, smooth loss: 348.404645\n",
      "iter 400, loss: 192.795256, smooth loss: 346.775181\n",
      "iter 410, loss: 175.665272, smooth loss: 345.044907\n",
      "iter 420, loss: 203.991950, smooth loss: 343.381551\n",
      "iter 430, loss: 172.119929, smooth loss: 341.710630\n",
      "iter 440, loss: 171.267970, smooth loss: 340.102628\n",
      "iter 450, loss: 156.448540, smooth loss: 338.432134\n",
      "iter 460, loss: 157.527170, smooth loss: 336.743536\n",
      "iter 470, loss: 178.538995, smooth loss: 335.037451\n",
      "iter 480, loss: 128.380021, smooth loss: 333.253550\n",
      "iter 490, loss: 189.419119, smooth loss: 331.603752\n",
      "iter 500, loss: 188.399592, smooth loss: 329.971350\n",
      "iter 510, loss: 178.770402, smooth loss: 328.414539\n",
      "iter 520, loss: 173.882603, smooth loss: 326.742434\n",
      "iter 530, loss: 166.561940, smooth loss: 325.163883\n",
      "iter 540, loss: 192.002718, smooth loss: 323.586224\n",
      "iter 550, loss: 169.087966, smooth loss: 322.031383\n",
      "iter 560, loss: 152.888529, smooth loss: 320.417610\n",
      "iter 570, loss: 147.655838, smooth loss: 318.794829\n",
      "iter 580, loss: 155.583313, smooth loss: 317.174118\n",
      "iter 590, loss: 156.247501, smooth loss: 315.484078\n",
      "iter 600, loss: 141.633395, smooth loss: 313.898274\n",
      "iter 610, loss: 170.777378, smooth loss: 312.376430\n",
      "iter 620, loss: 166.654057, smooth loss: 310.901054\n",
      "iter 630, loss: 198.914728, smooth loss: 309.340098\n",
      "iter 640, loss: 145.300129, smooth loss: 307.796184\n",
      "iter 650, loss: 152.331507, smooth loss: 306.307304\n",
      "iter 660, loss: 153.532829, smooth loss: 304.830502\n",
      "iter 670, loss: 136.120995, smooth loss: 303.269142\n",
      "iter 680, loss: 122.169380, smooth loss: 301.710353\n",
      "iter 690, loss: 140.927602, smooth loss: 300.198665\n",
      "iter 700, loss: 176.665482, smooth loss: 298.635740\n",
      "iter 710, loss: 158.259329, smooth loss: 297.106563\n",
      "iter 720, loss: 159.198139, smooth loss: 295.673850\n",
      "iter 730, loss: 138.850720, smooth loss: 294.274143\n",
      "iter 740, loss: 146.261405, smooth loss: 292.796783\n",
      "iter 750, loss: 150.041932, smooth loss: 291.343554\n",
      "iter 760, loss: 147.183091, smooth loss: 289.931517\n",
      "iter 770, loss: 176.820259, smooth loss: 288.565481\n",
      "iter 780, loss: 119.270465, smooth loss: 287.018477\n",
      "iter 790, loss: 127.309091, smooth loss: 285.543183\n",
      "iter 800, loss: 127.296965, smooth loss: 284.113609\n",
      "iter 810, loss: 152.658520, smooth loss: 282.665558\n",
      "iter 820, loss: 135.375191, smooth loss: 281.205202\n",
      "iter 830, loss: 145.959930, smooth loss: 279.873735\n",
      "iter 840, loss: 134.644810, smooth loss: 278.554865\n",
      "iter 850, loss: 107.505592, smooth loss: 277.136254\n",
      "iter 860, loss: 122.314657, smooth loss: 275.788813\n",
      "iter 870, loss: 151.432465, smooth loss: 274.483389\n",
      "iter 880, loss: 137.643631, smooth loss: 273.182724\n",
      "iter 890, loss: 126.808990, smooth loss: 271.693224\n",
      "iter 900, loss: 133.683482, smooth loss: 270.302834\n",
      "iter 910, loss: 106.166952, smooth loss: 268.917516\n",
      "iter 920, loss: 131.331736, smooth loss: 267.574421\n",
      "iter 930, loss: 138.072517, smooth loss: 266.213324\n",
      "iter 940, loss: 139.769230, smooth loss: 264.979033\n",
      "iter 950, loss: 121.166170, smooth loss: 263.729472\n",
      "iter 960, loss: 130.830388, smooth loss: 262.415812\n",
      "iter 970, loss: 157.484391, smooth loss: 261.174765\n",
      "iter 980, loss: 127.672939, smooth loss: 259.933101\n",
      "iter 990, loss: 121.734490, smooth loss: 258.726781\n",
      "iter 1000, loss: 150.622829, smooth loss: 257.357747\n",
      "iter 1010, loss: 131.488188, smooth loss: 256.036133\n",
      "iter 1020, loss: 116.486224, smooth loss: 254.734725\n",
      "iter 1030, loss: 100.079055, smooth loss: 253.488810\n",
      "iter 1040, loss: 157.694146, smooth loss: 252.291627\n",
      "iter 1050, loss: 131.624068, smooth loss: 251.116032\n",
      "iter 1060, loss: 91.777889, smooth loss: 249.911275\n",
      "iter 1070, loss: 130.933935, smooth loss: 248.714520\n",
      "iter 1080, loss: 139.628746, smooth loss: 247.584803\n",
      "iter 1090, loss: 138.344561, smooth loss: 246.424850\n",
      "iter 1100, loss: 122.928295, smooth loss: 245.270380\n",
      "iter 1110, loss: 150.077259, smooth loss: 244.045032\n",
      "iter 1120, loss: 120.217244, smooth loss: 242.793841\n",
      "iter 1130, loss: 114.332483, smooth loss: 241.575486\n",
      "iter 1140, loss: 127.455201, smooth loss: 240.427498\n",
      "iter 1150, loss: 121.799789, smooth loss: 239.293672\n",
      "iter 1160, loss: 153.589125, smooth loss: 238.241956\n",
      "iter 1170, loss: 110.881529, smooth loss: 237.070672\n",
      "iter 1180, loss: 133.203483, smooth loss: 235.995747\n",
      "iter 1190, loss: 115.804035, smooth loss: 234.925800\n",
      "iter 1200, loss: 112.025235, smooth loss: 233.837011\n",
      "iter 1210, loss: 104.208233, smooth loss: 232.785203\n",
      "iter 1220, loss: 111.202284, smooth loss: 231.646956\n",
      "iter 1230, loss: 127.689258, smooth loss: 230.492224\n",
      "iter 1240, loss: 148.457773, smooth loss: 229.381447\n",
      "iter 1250, loss: 93.755024, smooth loss: 228.264702\n",
      "iter 1260, loss: 118.816769, smooth loss: 227.229467\n",
      "iter 1270, loss: 113.308799, smooth loss: 226.268401\n",
      "iter 1280, loss: 136.994344, smooth loss: 225.199409\n",
      "iter 1290, loss: 119.842562, smooth loss: 224.171832\n",
      "iter 1300, loss: 110.764213, smooth loss: 223.158593\n",
      "iter 1310, loss: 123.422851, smooth loss: 222.142423\n",
      "iter 1320, loss: 110.692895, smooth loss: 221.114824\n",
      "iter 1330, loss: 99.498054, smooth loss: 220.009698\n",
      "iter 1340, loss: 111.549079, smooth loss: 218.914696\n",
      "iter 1350, loss: 126.210391, smooth loss: 217.895357\n",
      "iter 1360, loss: 123.288407, smooth loss: 216.852912\n",
      "iter 1370, loss: 111.011804, smooth loss: 215.877620\n",
      "iter 1380, loss: 112.827982, smooth loss: 214.979433\n",
      "iter 1390, loss: 108.710748, smooth loss: 213.970971\n",
      "iter 1400, loss: 127.032808, smooth loss: 213.054090\n",
      "iter 1410, loss: 108.555804, smooth loss: 212.113640\n",
      "iter 1420, loss: 130.644429, smooth loss: 211.174368\n",
      "iter 1430, loss: 109.301898, smooth loss: 210.212126\n",
      "iter 1440, loss: 115.079272, smooth loss: 209.198965\n",
      "iter 1450, loss: 116.604047, smooth loss: 208.196586\n",
      "iter 1460, loss: 109.341365, smooth loss: 207.253653\n",
      "iter 1470, loss: 89.681966, smooth loss: 206.249508\n",
      "iter 1480, loss: 126.672542, smooth loss: 205.367225\n",
      "iter 1490, loss: 133.796692, smooth loss: 204.569712\n",
      "iter 1500, loss: 115.418216, smooth loss: 203.613440\n",
      "iter 1510, loss: 139.790319, smooth loss: 202.777808\n",
      "iter 1520, loss: 99.043877, smooth loss: 201.888160\n",
      "iter 1530, loss: 103.709702, smooth loss: 201.019348\n",
      "iter 1540, loss: 79.017507, smooth loss: 200.099214\n",
      "iter 1550, loss: 105.594294, smooth loss: 199.182811\n",
      "iter 1560, loss: 125.471815, smooth loss: 198.267253\n",
      "iter 1570, loss: 90.466864, smooth loss: 197.364678\n",
      "iter 1580, loss: 131.475473, smooth loss: 196.472379\n",
      "iter 1590, loss: 139.717080, smooth loss: 195.669429\n",
      "iter 1600, loss: 128.837509, smooth loss: 194.916510\n",
      "iter 1610, loss: 124.704939, smooth loss: 194.023823\n",
      "iter 1620, loss: 112.866751, smooth loss: 193.226254\n",
      "iter 1630, loss: 136.110146, smooth loss: 192.412157\n",
      "iter 1640, loss: 122.273549, smooth loss: 191.603911\n",
      "iter 1650, loss: 99.664208, smooth loss: 190.722932\n",
      "iter 1660, loss: 97.879762, smooth loss: 189.856972\n",
      "iter 1670, loss: 117.629618, smooth loss: 189.036724\n",
      "iter 1680, loss: 106.326020, smooth loss: 188.168743\n",
      "iter 1690, loss: 94.725686, smooth loss: 187.328452\n",
      "iter 1700, loss: 117.067529, smooth loss: 186.612101\n",
      "iter 1710, loss: 115.196789, smooth loss: 185.911883\n",
      "iter 1720, loss: 138.927740, smooth loss: 185.101160\n",
      "iter 1730, loss: 107.685445, smooth loss: 184.344084\n",
      "iter 1740, loss: 103.311873, smooth loss: 183.592809\n",
      "iter 1750, loss: 102.166212, smooth loss: 182.824860\n",
      "iter 1760, loss: 87.132947, smooth loss: 181.980041\n",
      "iter 1770, loss: 85.994566, smooth loss: 181.156034\n",
      "iter 1780, loss: 96.561750, smooth loss: 180.394071\n",
      "iter 1790, loss: 121.755517, smooth loss: 179.622431\n",
      "iter 1800, loss: 117.230180, smooth loss: 178.877073\n",
      "iter 1810, loss: 111.000833, smooth loss: 178.205177\n",
      "iter 1820, loss: 101.071856, smooth loss: 177.545712\n",
      "iter 1830, loss: 101.638748, smooth loss: 176.801651\n",
      "iter 1840, loss: 109.380867, smooth loss: 176.101683\n",
      "iter 1850, loss: 104.136431, smooth loss: 175.386888\n",
      "iter 1860, loss: 136.714275, smooth loss: 174.719513\n",
      "iter 1870, loss: 77.553976, smooth loss: 173.897462\n",
      "iter 1880, loss: 91.668359, smooth loss: 173.160432\n",
      "iter 1890, loss: 91.981927, smooth loss: 172.458887\n",
      "iter 1900, loss: 120.012330, smooth loss: 171.796012\n",
      "iter 1910, loss: 103.997870, smooth loss: 171.100400\n",
      "iter 1920, loss: 118.027485, smooth loss: 170.524536\n",
      "iter 1930, loss: 81.661818, smooth loss: 169.895516\n",
      "iter 1940, loss: 88.633374, smooth loss: 169.207552\n",
      "iter 1950, loss: 87.529984, smooth loss: 168.571533\n",
      "iter 1960, loss: 115.870889, smooth loss: 167.949865\n",
      "iter 1970, loss: 107.276230, smooth loss: 167.333723\n",
      "iter 1980, loss: 96.319853, smooth loss: 166.574914\n",
      "iter 1990, loss: 96.904186, smooth loss: 165.895130\n",
      "iter 2000, loss: 83.960596, smooth loss: 165.230821\n",
      "iter 2010, loss: 102.593133, smooth loss: 164.624492\n",
      "iter 2020, loss: 97.546009, smooth loss: 163.964447\n",
      "iter 2030, loss: 97.195397, smooth loss: 163.430137\n",
      "iter 2040, loss: 80.164685, smooth loss: 162.827955\n",
      "iter 2050, loss: 97.872465, smooth loss: 162.180940\n",
      "iter 2060, loss: 120.906626, smooth loss: 161.620234\n",
      "iter 2070, loss: 96.266659, smooth loss: 161.010882\n",
      "iter 2080, loss: 89.028147, smooth loss: 160.418538\n",
      "iter 2090, loss: 105.508782, smooth loss: 159.695984\n",
      "iter 2100, loss: 98.252337, smooth loss: 159.005743\n",
      "iter 2110, loss: 86.018851, smooth loss: 158.370963\n",
      "iter 2120, loss: 77.532358, smooth loss: 157.803860\n",
      "iter 2130, loss: 133.526633, smooth loss: 157.242342\n",
      "iter 2140, loss: 106.524241, smooth loss: 156.691603\n",
      "iter 2150, loss: 70.010743, smooth loss: 156.098141\n",
      "iter 2160, loss: 100.446577, smooth loss: 155.550788\n",
      "iter 2170, loss: 104.882682, smooth loss: 155.014258\n",
      "iter 2180, loss: 98.491420, smooth loss: 154.428773\n",
      "iter 2190, loss: 95.667594, smooth loss: 153.884911\n",
      "iter 2200, loss: 115.827641, smooth loss: 153.238072\n",
      "iter 2210, loss: 100.350406, smooth loss: 152.610140\n",
      "iter 2220, loss: 90.794510, smooth loss: 152.006664\n",
      "iter 2230, loss: 100.155264, smooth loss: 151.471259\n",
      "iter 2240, loss: 90.765665, smooth loss: 150.919915\n",
      "iter 2250, loss: 127.109847, smooth loss: 150.464342\n",
      "iter 2260, loss: 81.782506, smooth loss: 149.845990\n",
      "iter 2270, loss: 99.151334, smooth loss: 149.321946\n",
      "iter 2280, loss: 86.351705, smooth loss: 148.803136\n",
      "iter 2290, loss: 78.024911, smooth loss: 148.229590\n",
      "iter 2300, loss: 96.685303, smooth loss: 147.747680\n",
      "iter 2310, loss: 85.432817, smooth loss: 147.164919\n",
      "iter 2320, loss: 89.584141, smooth loss: 146.555831\n",
      "iter 2330, loss: 119.412029, smooth loss: 146.043379\n",
      "iter 2340, loss: 69.959057, smooth loss: 145.472841\n",
      "iter 2350, loss: 91.195101, smooth loss: 144.991272\n",
      "iter 2360, loss: 84.066324, smooth loss: 144.558920\n",
      "iter 2370, loss: 114.029167, smooth loss: 144.034479\n",
      "iter 2380, loss: 86.786687, smooth loss: 143.529964\n",
      "iter 2390, loss: 77.405077, smooth loss: 143.052491\n",
      "iter 2400, loss: 95.083751, smooth loss: 142.547357\n",
      "iter 2410, loss: 95.063362, smooth loss: 142.078395\n",
      "iter 2420, loss: 67.907559, smooth loss: 141.481984\n",
      "iter 2430, loss: 92.637158, smooth loss: 140.933667\n",
      "iter 2440, loss: 105.776888, smooth loss: 140.467134\n",
      "iter 2450, loss: 99.467369, smooth loss: 139.951752\n",
      "iter 2460, loss: 91.025062, smooth loss: 139.497328\n",
      "iter 2470, loss: 88.485685, smooth loss: 139.139784\n",
      "iter 2480, loss: 83.019259, smooth loss: 138.643162\n",
      "iter 2490, loss: 99.267638, smooth loss: 138.192309\n",
      "iter 2500, loss: 81.843742, smooth loss: 137.746036\n",
      "iter 2510, loss: 102.547743, smooth loss: 137.307351\n",
      "iter 2520, loss: 85.112447, smooth loss: 136.855474\n",
      "iter 2530, loss: 87.563481, smooth loss: 136.338262\n",
      "iter 2540, loss: 92.540897, smooth loss: 135.817144\n",
      "iter 2550, loss: 90.093014, smooth loss: 135.389894\n",
      "iter 2560, loss: 67.263108, smooth loss: 134.901534\n",
      "iter 2570, loss: 106.055163, smooth loss: 134.524187\n",
      "iter 2580, loss: 113.104849, smooth loss: 134.225684\n",
      "iter 2590, loss: 89.036888, smooth loss: 133.751390\n",
      "iter 2600, loss: 109.661054, smooth loss: 133.353736\n",
      "iter 2610, loss: 82.595696, smooth loss: 132.923619\n",
      "iter 2620, loss: 83.234685, smooth loss: 132.517781\n",
      "iter 2630, loss: 58.505576, smooth loss: 132.052046\n",
      "iter 2640, loss: 83.045523, smooth loss: 131.592144\n",
      "iter 2650, loss: 100.006439, smooth loss: 131.119734\n",
      "iter 2660, loss: 72.012969, smooth loss: 130.681495\n",
      "iter 2670, loss: 112.657005, smooth loss: 130.256898\n",
      "iter 2680, loss: 108.485277, smooth loss: 129.912015\n",
      "iter 2690, loss: 99.628519, smooth loss: 129.604769\n",
      "iter 2700, loss: 99.672528, smooth loss: 129.162505\n",
      "iter 2710, loss: 90.447265, smooth loss: 128.768212\n",
      "iter 2720, loss: 111.814542, smooth loss: 128.409721\n",
      "iter 2730, loss: 101.084638, smooth loss: 128.047588\n",
      "iter 2740, loss: 79.612698, smooth loss: 127.608572\n",
      "iter 2750, loss: 75.377350, smooth loss: 127.170057\n",
      "iter 2760, loss: 105.149587, smooth loss: 126.763616\n",
      "iter 2770, loss: 81.545216, smooth loss: 126.337319\n",
      "iter 2780, loss: 81.354951, smooth loss: 125.947823\n",
      "iter 2790, loss: 97.236435, smooth loss: 125.658101\n",
      "iter 2800, loss: 86.476008, smooth loss: 125.365026\n",
      "iter 2810, loss: 106.261567, smooth loss: 124.964314\n",
      "iter 2820, loss: 87.282862, smooth loss: 124.616234\n",
      "iter 2830, loss: 92.471759, smooth loss: 124.253482\n",
      "iter 2840, loss: 77.442479, smooth loss: 123.896571\n",
      "iter 2850, loss: 74.427947, smooth loss: 123.478909\n",
      "iter 2860, loss: 78.419486, smooth loss: 123.060257\n",
      "iter 2870, loss: 75.780120, smooth loss: 122.683576\n",
      "iter 2880, loss: 93.234786, smooth loss: 122.319491\n",
      "iter 2890, loss: 102.633634, smooth loss: 121.958310\n",
      "iter 2900, loss: 93.376062, smooth loss: 121.684553\n",
      "iter 2910, loss: 80.883379, smooth loss: 121.392192\n",
      "iter 2920, loss: 76.267601, smooth loss: 121.001902\n",
      "iter 2930, loss: 82.521660, smooth loss: 120.657443\n",
      "iter 2940, loss: 83.189696, smooth loss: 120.304225\n",
      "iter 2950, loss: 111.893940, smooth loss: 119.971215\n",
      "iter 2960, loss: 57.624105, smooth loss: 119.510700\n",
      "iter 2970, loss: 69.936605, smooth loss: 119.119589\n",
      "iter 2980, loss: 65.309383, smooth loss: 118.755525\n",
      "iter 2990, loss: 89.134825, smooth loss: 118.415797\n",
      "iter 3000, loss: 97.072815, smooth loss: 118.062160\n",
      "iter 3010, loss: 98.747915, smooth loss: 117.815006\n",
      "iter 3020, loss: 61.263376, smooth loss: 117.517272\n",
      "iter 3030, loss: 69.420315, smooth loss: 117.145139\n",
      "iter 3040, loss: 76.156698, smooth loss: 116.864506\n",
      "iter 3050, loss: 84.288063, smooth loss: 116.533992\n",
      "iter 3060, loss: 83.950130, smooth loss: 116.250532\n",
      "iter 3070, loss: 87.824694, smooth loss: 115.830119\n",
      "iter 3080, loss: 82.615641, smooth loss: 115.473002\n",
      "iter 3090, loss: 68.281057, smooth loss: 115.134688\n",
      "iter 3100, loss: 79.727454, smooth loss: 114.839318\n",
      "iter 3110, loss: 73.385906, smooth loss: 114.522743\n",
      "iter 3120, loss: 72.459323, smooth loss: 114.290033\n",
      "iter 3130, loss: 68.018522, smooth loss: 114.029371\n",
      "iter 3140, loss: 86.433371, smooth loss: 113.715350\n",
      "iter 3150, loss: 96.702134, smooth loss: 113.439046\n",
      "iter 3160, loss: 74.078798, smooth loss: 113.121733\n",
      "iter 3170, loss: 71.965586, smooth loss: 112.848158\n",
      "iter 3180, loss: 91.488947, smooth loss: 112.464134\n",
      "iter 3190, loss: 81.520651, smooth loss: 112.096186\n",
      "iter 3200, loss: 75.124042, smooth loss: 111.792177\n",
      "iter 3210, loss: 64.217428, smooth loss: 111.507568\n",
      "iter 3220, loss: 127.506412, smooth loss: 111.290396\n",
      "iter 3230, loss: 84.953492, smooth loss: 111.088051\n",
      "iter 3240, loss: 55.739375, smooth loss: 110.815793\n",
      "iter 3250, loss: 74.314064, smooth loss: 110.530899\n",
      "iter 3260, loss: 90.647887, smooth loss: 110.291331\n",
      "iter 3270, loss: 80.198003, smooth loss: 109.989720\n",
      "iter 3280, loss: 75.540480, smooth loss: 109.722767\n",
      "iter 3290, loss: 97.023310, smooth loss: 109.374761\n",
      "iter 3300, loss: 77.234139, smooth loss: 109.008431\n",
      "iter 3310, loss: 69.477614, smooth loss: 108.676937\n",
      "iter 3320, loss: 82.638735, smooth loss: 108.420066\n",
      "iter 3330, loss: 79.213195, smooth loss: 108.199935\n",
      "iter 3340, loss: 114.828209, smooth loss: 108.043057\n",
      "iter 3350, loss: 71.898178, smooth loss: 107.735015\n",
      "iter 3360, loss: 92.128679, smooth loss: 107.506055\n",
      "iter 3370, loss: 79.064702, smooth loss: 107.297176\n",
      "iter 3380, loss: 67.604388, smooth loss: 107.008419\n",
      "iter 3390, loss: 62.188189, smooth loss: 106.763396\n",
      "iter 3400, loss: 69.239617, smooth loss: 106.469109\n",
      "iter 3410, loss: 69.383094, smooth loss: 106.137705\n",
      "iter 3420, loss: 103.124146, smooth loss: 105.893200\n",
      "iter 3430, loss: 59.301406, smooth loss: 105.645997\n",
      "iter 3440, loss: 76.396760, smooth loss: 105.439050\n",
      "iter 3450, loss: 66.003454, smooth loss: 105.272482\n",
      "iter 3460, loss: 95.464669, smooth loss: 104.998787\n",
      "iter 3470, loss: 73.185245, smooth loss: 104.748669\n",
      "iter 3480, loss: 65.934746, smooth loss: 104.544275\n",
      "iter 3490, loss: 83.365154, smooth loss: 104.330220\n",
      "iter 3500, loss: 89.816300, smooth loss: 104.104811\n",
      "iter 3510, loss: 60.157098, smooth loss: 103.783635\n",
      "iter 3520, loss: 78.626178, smooth loss: 103.473228\n",
      "iter 3530, loss: 91.996245, smooth loss: 103.242039\n",
      "iter 3540, loss: 85.555745, smooth loss: 102.960614\n",
      "iter 3550, loss: 83.757410, smooth loss: 102.774126\n",
      "iter 3560, loss: 78.113979, smooth loss: 102.620886\n",
      "iter 3570, loss: 68.426886, smooth loss: 102.370606\n",
      "iter 3580, loss: 81.890171, smooth loss: 102.124029\n",
      "iter 3590, loss: 66.729562, smooth loss: 101.905388\n",
      "iter 3600, loss: 82.445283, smooth loss: 101.667545\n",
      "iter 3610, loss: 71.846008, smooth loss: 101.436605\n",
      "iter 3620, loss: 75.471739, smooth loss: 101.117898\n",
      "iter 3630, loss: 77.717781, smooth loss: 100.817410\n",
      "iter 3640, loss: 78.074171, smooth loss: 100.572612\n",
      "iter 3650, loss: 63.913413, smooth loss: 100.316316\n",
      "iter 3660, loss: 89.747317, smooth loss: 100.164584\n",
      "iter 3670, loss: 99.199933, smooth loss: 100.037272\n",
      "iter 3680, loss: 75.305171, smooth loss: 99.761575\n",
      "iter 3690, loss: 97.217198, smooth loss: 99.547480\n",
      "iter 3700, loss: 67.292388, smooth loss: 99.293428\n",
      "iter 3710, loss: 69.247946, smooth loss: 99.072378\n",
      "iter 3720, loss: 50.451067, smooth loss: 98.835375\n",
      "iter 3730, loss: 70.470477, smooth loss: 98.551698\n",
      "iter 3740, loss: 84.784917, smooth loss: 98.285035\n",
      "iter 3750, loss: 59.072691, smooth loss: 98.014082\n",
      "iter 3760, loss: 93.186988, smooth loss: 97.788440\n",
      "iter 3770, loss: 96.569475, smooth loss: 97.629038\n",
      "iter 3780, loss: 99.282387, smooth loss: 97.496696\n",
      "iter 3790, loss: 87.123987, smooth loss: 97.239337\n",
      "iter 3800, loss: 71.999136, smooth loss: 97.050961\n",
      "iter 3810, loss: 93.747116, smooth loss: 96.833080\n",
      "iter 3820, loss: 88.836178, smooth loss: 96.639061\n",
      "iter 3830, loss: 71.224837, smooth loss: 96.370922\n",
      "iter 3840, loss: 67.830540, smooth loss: 96.088131\n",
      "iter 3850, loss: 75.345116, smooth loss: 95.818548\n",
      "iter 3860, loss: 69.465910, smooth loss: 95.559834\n",
      "iter 3870, loss: 68.293719, smooth loss: 95.350345\n",
      "iter 3880, loss: 90.957176, smooth loss: 95.231094\n",
      "iter 3890, loss: 81.374592, smooth loss: 95.105472\n",
      "iter 3900, loss: 90.427296, smooth loss: 94.875765\n",
      "iter 3910, loss: 73.686398, smooth loss: 94.677647\n",
      "iter 3920, loss: 75.682100, smooth loss: 94.459304\n",
      "iter 3930, loss: 60.751854, smooth loss: 94.255516\n",
      "iter 3940, loss: 61.315472, smooth loss: 93.985376\n",
      "iter 3950, loss: 66.054214, smooth loss: 93.735176\n",
      "iter 3960, loss: 68.349990, smooth loss: 93.507913\n",
      "iter 3970, loss: 77.992486, smooth loss: 93.288417\n",
      "iter 3980, loss: 85.318217, smooth loss: 93.103501\n",
      "iter 3990, loss: 82.516332, smooth loss: 92.981348\n",
      "iter 4000, loss: 79.122351, smooth loss: 92.885135\n",
      "iter 4010, loss: 64.420607, smooth loss: 92.641341\n",
      "iter 4020, loss: 70.879245, smooth loss: 92.473153\n",
      "iter 4030, loss: 72.422732, smooth loss: 92.290807\n",
      "iter 4040, loss: 100.089895, smooth loss: 92.166352\n",
      "iter 4050, loss: 50.356900, smooth loss: 91.886092\n",
      "iter 4060, loss: 61.007962, smooth loss: 91.641363\n",
      "iter 4070, loss: 57.840889, smooth loss: 91.435813\n",
      "iter 4080, loss: 82.443509, smooth loss: 91.281378\n",
      "iter 4090, loss: 86.728731, smooth loss: 91.132868\n",
      "iter 4100, loss: 84.851841, smooth loss: 91.041691\n",
      "iter 4110, loss: 58.104620, smooth loss: 90.922925\n",
      "iter 4120, loss: 63.134597, smooth loss: 90.708962\n",
      "iter 4130, loss: 69.267091, smooth loss: 90.547928\n",
      "iter 4140, loss: 76.719257, smooth loss: 90.389984\n",
      "iter 4150, loss: 70.416731, smooth loss: 90.279082\n",
      "iter 4160, loss: 76.098344, smooth loss: 90.024475\n",
      "iter 4170, loss: 69.343975, smooth loss: 89.777531\n",
      "iter 4180, loss: 59.159406, smooth loss: 89.562936\n",
      "iter 4190, loss: 84.064086, smooth loss: 89.406363\n",
      "iter 4200, loss: 65.340896, smooth loss: 89.262889\n",
      "iter 4210, loss: 67.424852, smooth loss: 89.188675\n",
      "iter 4220, loss: 61.454176, smooth loss: 89.059062\n",
      "iter 4230, loss: 70.691896, smooth loss: 88.868468\n",
      "iter 4240, loss: 85.177951, smooth loss: 88.756735\n",
      "iter 4250, loss: 64.235525, smooth loss: 88.564613\n",
      "iter 4260, loss: 57.291538, smooth loss: 88.421059\n",
      "iter 4270, loss: 72.393619, smooth loss: 88.174385\n",
      "iter 4280, loss: 61.818878, smooth loss: 87.936782\n",
      "iter 4290, loss: 61.510552, smooth loss: 87.718082\n",
      "iter 4300, loss: 56.402831, smooth loss: 87.546956\n",
      "iter 4310, loss: 94.491014, smooth loss: 87.407665\n",
      "iter 4320, loss: 76.644441, smooth loss: 87.307141\n",
      "iter 4330, loss: 43.418906, smooth loss: 87.146646\n",
      "iter 4340, loss: 62.750382, smooth loss: 86.957996\n",
      "iter 4350, loss: 74.656416, smooth loss: 86.821280\n",
      "iter 4360, loss: 72.028568, smooth loss: 86.640863\n",
      "iter 4370, loss: 64.844358, smooth loss: 86.507091\n",
      "iter 4380, loss: 79.656892, smooth loss: 86.282027\n",
      "iter 4390, loss: 69.117071, smooth loss: 86.049515\n",
      "iter 4400, loss: 62.710074, smooth loss: 85.846958\n",
      "iter 4410, loss: 74.700105, smooth loss: 85.695762\n",
      "iter 4420, loss: 71.046270, smooth loss: 85.567270\n",
      "iter 4430, loss: 102.523069, smooth loss: 85.515477\n",
      "iter 4440, loss: 60.232703, smooth loss: 85.339832\n",
      "iter 4450, loss: 79.893996, smooth loss: 85.193950\n",
      "iter 4460, loss: 65.288085, smooth loss: 85.089492\n",
      "iter 4470, loss: 69.140713, smooth loss: 84.931902\n",
      "iter 4480, loss: 59.765919, smooth loss: 84.805487\n",
      "iter 4490, loss: 52.884511, smooth loss: 84.596777\n",
      "iter 4500, loss: 54.091904, smooth loss: 84.363142\n",
      "iter 4510, loss: 93.667761, smooth loss: 84.206875\n",
      "iter 4520, loss: 52.723502, smooth loss: 84.073299\n",
      "iter 4530, loss: 68.949713, smooth loss: 83.984298\n",
      "iter 4540, loss: 58.234436, smooth loss: 83.930295\n",
      "iter 4550, loss: 94.123914, smooth loss: 83.791589\n",
      "iter 4560, loss: 60.369331, smooth loss: 83.649038\n",
      "iter 4570, loss: 51.188356, smooth loss: 83.533681\n",
      "iter 4580, loss: 78.974456, smooth loss: 83.399927\n",
      "iter 4590, loss: 66.644496, smooth loss: 83.288969\n",
      "iter 4600, loss: 53.213696, smooth loss: 83.076885\n",
      "iter 4610, loss: 69.052986, smooth loss: 82.867900\n",
      "iter 4620, loss: 79.691876, smooth loss: 82.720581\n",
      "iter 4630, loss: 76.295796, smooth loss: 82.544126\n",
      "iter 4640, loss: 65.213230, smooth loss: 82.431378\n",
      "iter 4650, loss: 65.310523, smooth loss: 82.364357\n",
      "iter 4660, loss: 58.764487, smooth loss: 82.201573\n",
      "iter 4670, loss: 67.123539, smooth loss: 82.083548\n",
      "iter 4680, loss: 56.801029, smooth loss: 81.958050\n",
      "iter 4690, loss: 79.160247, smooth loss: 81.855740\n",
      "iter 4700, loss: 60.053711, smooth loss: 81.725516\n",
      "iter 4710, loss: 62.173280, smooth loss: 81.514404\n",
      "iter 4720, loss: 73.252408, smooth loss: 81.333407\n",
      "iter 4730, loss: 72.609649, smooth loss: 81.217901\n",
      "iter 4740, loss: 50.717488, smooth loss: 81.050619\n",
      "iter 4750, loss: 83.272288, smooth loss: 80.994742\n",
      "iter 4760, loss: 85.923995, smooth loss: 80.928664\n",
      "iter 4770, loss: 65.760650, smooth loss: 80.749237\n",
      "iter 4780, loss: 90.241869, smooth loss: 80.650621\n",
      "iter 4790, loss: 58.531374, smooth loss: 80.505885\n",
      "iter 4800, loss: 60.363190, smooth loss: 80.415664\n",
      "iter 4810, loss: 48.987048, smooth loss: 80.279103\n",
      "iter 4820, loss: 72.971129, smooth loss: 80.113017\n",
      "iter 4830, loss: 73.733863, smooth loss: 79.940336\n",
      "iter 4840, loss: 59.207787, smooth loss: 79.802377\n",
      "iter 4850, loss: 93.419399, smooth loss: 79.652008\n",
      "iter 4860, loss: 92.455172, smooth loss: 79.603716\n",
      "iter 4870, loss: 72.403548, smooth loss: 79.567376\n",
      "iter 4880, loss: 73.199348, smooth loss: 79.402536\n",
      "iter 4890, loss: 68.128038, smooth loss: 79.295971\n",
      "iter 4900, loss: 94.860727, smooth loss: 79.164860\n",
      "iter 4910, loss: 78.394886, smooth loss: 79.069166\n",
      "iter 4920, loss: 63.793070, smooth loss: 78.911386\n",
      "iter 4930, loss: 59.568426, smooth loss: 78.751993\n",
      "iter 4940, loss: 70.112208, smooth loss: 78.589115\n",
      "iter 4950, loss: 63.038800, smooth loss: 78.454600\n",
      "iter 4960, loss: 67.353180, smooth loss: 78.359621\n",
      "iter 4970, loss: 90.083591, smooth loss: 78.327630\n",
      "iter 4980, loss: 72.215534, smooth loss: 78.282811\n",
      "iter 4990, loss: 73.529960, smooth loss: 78.136588\n",
      "iter 5000, loss: 64.072672, smooth loss: 78.030842\n",
      "iter 5010, loss: 66.790358, smooth loss: 77.918483\n",
      "iter 5020, loss: 67.277826, smooth loss: 77.817161\n",
      "iter 5030, loss: 56.383837, smooth loss: 77.665449\n",
      "iter 5040, loss: 56.760549, smooth loss: 77.483403\n",
      "iter 5050, loss: 59.645695, smooth loss: 77.344124\n",
      "iter 5060, loss: 64.897619, smooth loss: 77.220860\n",
      "iter 5070, loss: 81.471749, smooth loss: 77.125463\n",
      "iter 5080, loss: 67.020712, smooth loss: 77.098840\n",
      "iter 5090, loss: 63.213710, smooth loss: 77.031068\n",
      "iter 5100, loss: 55.241109, smooth loss: 76.858129\n",
      "iter 5110, loss: 60.915814, smooth loss: 76.767242\n",
      "iter 5120, loss: 61.810606, smooth loss: 76.648701\n",
      "iter 5130, loss: 98.625561, smooth loss: 76.564902\n",
      "iter 5140, loss: 47.443893, smooth loss: 76.378135\n",
      "iter 5150, loss: 55.112045, smooth loss: 76.199458\n",
      "iter 5160, loss: 57.494930, smooth loss: 76.049174\n",
      "iter 5170, loss: 78.641286, smooth loss: 75.943007\n",
      "iter 5180, loss: 74.839110, smooth loss: 75.819849\n",
      "iter 5190, loss: 74.817756, smooth loss: 75.780108\n",
      "iter 5200, loss: 48.568021, smooth loss: 75.684730\n",
      "iter 5210, loss: 53.215576, smooth loss: 75.522002\n",
      "iter 5220, loss: 57.289715, smooth loss: 75.418889\n",
      "iter 5230, loss: 65.290779, smooth loss: 75.307997\n",
      "iter 5240, loss: 67.129774, smooth loss: 75.225384\n",
      "iter 5250, loss: 70.809808, smooth loss: 75.039953\n",
      "iter 5260, loss: 59.410750, smooth loss: 74.893490\n",
      "iter 5270, loss: 54.199635, smooth loss: 74.739265\n",
      "iter 5280, loss: 60.612848, smooth loss: 74.616623\n",
      "iter 5290, loss: 56.661641, smooth loss: 74.500621\n",
      "iter 5300, loss: 45.751375, smooth loss: 74.439984\n",
      "iter 5310, loss: 51.365360, smooth loss: 74.367826\n",
      "iter 5320, loss: 62.695964, smooth loss: 74.218108\n",
      "iter 5330, loss: 74.150734, smooth loss: 74.127563\n",
      "iter 5340, loss: 59.351033, smooth loss: 73.986793\n",
      "iter 5350, loss: 51.534474, smooth loss: 73.902354\n",
      "iter 5360, loss: 69.045854, smooth loss: 73.718742\n",
      "iter 5370, loss: 59.023716, smooth loss: 73.536586\n",
      "iter 5380, loss: 56.851521, smooth loss: 73.412576\n",
      "iter 5390, loss: 51.959818, smooth loss: 73.305894\n",
      "iter 5400, loss: 83.284647, smooth loss: 73.241514\n",
      "iter 5410, loss: 62.731592, smooth loss: 73.179852\n",
      "iter 5420, loss: 36.484620, smooth loss: 73.064404\n",
      "iter 5430, loss: 54.351577, smooth loss: 72.932552\n",
      "iter 5440, loss: 64.428299, smooth loss: 72.853991\n",
      "iter 5450, loss: 59.122228, smooth loss: 72.734109\n",
      "iter 5460, loss: 60.581831, smooth loss: 72.667225\n",
      "iter 5470, loss: 66.183629, smooth loss: 72.511605\n",
      "iter 5480, loss: 64.734957, smooth loss: 72.321743\n",
      "iter 5490, loss: 56.916761, smooth loss: 72.203657\n",
      "iter 5500, loss: 68.082995, smooth loss: 72.099984\n",
      "iter 5510, loss: 57.218191, smooth loss: 72.012265\n",
      "iter 5520, loss: 95.273387, smooth loss: 71.985384\n",
      "iter 5530, loss: 52.913592, smooth loss: 71.860448\n",
      "iter 5540, loss: 67.376009, smooth loss: 71.749179\n",
      "iter 5550, loss: 55.810226, smooth loss: 71.655204\n",
      "iter 5560, loss: 69.362734, smooth loss: 71.530111\n",
      "iter 5570, loss: 44.521002, smooth loss: 71.463522\n",
      "iter 5580, loss: 47.991881, smooth loss: 71.316225\n",
      "iter 5590, loss: 54.554547, smooth loss: 71.142419\n",
      "iter 5600, loss: 80.252549, smooth loss: 71.049701\n",
      "iter 5610, loss: 48.023790, smooth loss: 70.949365\n",
      "iter 5620, loss: 59.030912, smooth loss: 70.897814\n",
      "iter 5630, loss: 47.781992, smooth loss: 70.856923\n",
      "iter 5640, loss: 77.837332, smooth loss: 70.775317\n",
      "iter 5650, loss: 52.725479, smooth loss: 70.659682\n",
      "iter 5660, loss: 43.717564, smooth loss: 70.580580\n",
      "iter 5670, loss: 64.544708, smooth loss: 70.501941\n",
      "iter 5680, loss: 68.768233, smooth loss: 70.461349\n",
      "iter 5690, loss: 45.907140, smooth loss: 70.327452\n",
      "iter 5700, loss: 62.622024, smooth loss: 70.214623\n",
      "iter 5710, loss: 77.701996, smooth loss: 70.148372\n",
      "iter 5720, loss: 70.347777, smooth loss: 70.047977\n",
      "iter 5730, loss: 58.166862, smooth loss: 69.996016\n",
      "iter 5740, loss: 65.947718, smooth loss: 69.993531\n",
      "iter 5750, loss: 52.313941, smooth loss: 69.902301\n",
      "iter 5760, loss: 67.959400, smooth loss: 69.840724\n",
      "iter 5770, loss: 51.569705, smooth loss: 69.765152\n",
      "iter 5780, loss: 67.675062, smooth loss: 69.718766\n",
      "iter 5790, loss: 52.970645, smooth loss: 69.637166\n",
      "iter 5800, loss: 64.760824, smooth loss: 69.496067\n",
      "iter 5810, loss: 69.299510, smooth loss: 69.363743\n",
      "iter 5820, loss: 65.277010, smooth loss: 69.278444\n",
      "iter 5830, loss: 45.411980, smooth loss: 69.131925\n",
      "iter 5840, loss: 72.330086, smooth loss: 69.076869\n",
      "iter 5850, loss: 76.628867, smooth loss: 69.082030\n",
      "iter 5860, loss: 59.636281, smooth loss: 68.958419\n",
      "iter 5870, loss: 77.865794, smooth loss: 68.861482\n",
      "iter 5880, loss: 49.101665, smooth loss: 68.743174\n",
      "iter 5890, loss: 46.469767, smooth loss: 68.678606\n",
      "iter 5900, loss: 40.838018, smooth loss: 68.579867\n",
      "iter 5910, loss: 54.119318, smooth loss: 68.430629\n",
      "iter 5920, loss: 68.180249, smooth loss: 68.294750\n",
      "iter 5930, loss: 49.731992, smooth loss: 68.195970\n",
      "iter 5940, loss: 69.416722, smooth loss: 68.083957\n",
      "iter 5950, loss: 76.844603, smooth loss: 68.033524\n",
      "iter 5960, loss: 62.357421, smooth loss: 68.006220\n",
      "iter 5970, loss: 63.616398, smooth loss: 67.872779\n",
      "iter 5980, loss: 64.807935, smooth loss: 67.796798\n",
      "iter 5990, loss: 73.953553, smooth loss: 67.681207\n",
      "iter 6000, loss: 66.936776, smooth loss: 67.592448\n",
      "iter 6010, loss: 58.424904, smooth loss: 67.473704\n",
      "iter 6020, loss: 53.557848, smooth loss: 67.377200\n",
      "iter 6030, loss: 68.841232, smooth loss: 67.269540\n",
      "iter 6040, loss: 49.754161, smooth loss: 67.151399\n",
      "iter 6050, loss: 55.465618, smooth loss: 67.062658\n",
      "iter 6060, loss: 80.881816, smooth loss: 67.055476\n",
      "iter 6070, loss: 66.145040, smooth loss: 67.012325\n",
      "iter 6080, loss: 64.722867, smooth loss: 66.907213\n",
      "iter 6090, loss: 56.365182, smooth loss: 66.821463\n",
      "iter 6100, loss: 57.885057, smooth loss: 66.747862\n",
      "iter 6110, loss: 55.476255, smooth loss: 66.686754\n",
      "iter 6120, loss: 49.434533, smooth loss: 66.581732\n",
      "iter 6130, loss: 44.674635, smooth loss: 66.482202\n",
      "iter 6140, loss: 50.820864, smooth loss: 66.387879\n",
      "iter 6150, loss: 61.571478, smooth loss: 66.315527\n",
      "iter 6160, loss: 72.766390, smooth loss: 66.271464\n",
      "iter 6170, loss: 56.782401, smooth loss: 66.262024\n",
      "iter 6180, loss: 56.629377, smooth loss: 66.245484\n",
      "iter 6190, loss: 56.591274, smooth loss: 66.156898\n",
      "iter 6200, loss: 55.794691, smooth loss: 66.088419\n",
      "iter 6210, loss: 75.726118, smooth loss: 66.027664\n",
      "iter 6220, loss: 86.819711, smooth loss: 66.010197\n",
      "iter 6230, loss: 47.916043, smooth loss: 65.864949\n",
      "iter 6240, loss: 61.737078, smooth loss: 65.762028\n",
      "iter 6250, loss: 51.725705, smooth loss: 65.691306\n",
      "iter 6260, loss: 62.104096, smooth loss: 65.634893\n",
      "iter 6270, loss: 73.029285, smooth loss: 65.593873\n",
      "iter 6280, loss: 66.091814, smooth loss: 65.587142\n",
      "iter 6290, loss: 46.422804, smooth loss: 65.563586\n",
      "iter 6300, loss: 50.383014, smooth loss: 65.440281\n",
      "iter 6310, loss: 49.942885, smooth loss: 65.381766\n",
      "iter 6320, loss: 61.835077, smooth loss: 65.312791\n",
      "iter 6330, loss: 52.945778, smooth loss: 65.255537\n",
      "iter 6340, loss: 65.674951, smooth loss: 65.128039\n",
      "iter 6350, loss: 59.414789, smooth loss: 65.026688\n",
      "iter 6360, loss: 52.579669, smooth loss: 64.924906\n",
      "iter 6370, loss: 55.411490, smooth loss: 64.856690\n",
      "iter 6380, loss: 48.113686, smooth loss: 64.798556\n",
      "iter 6390, loss: 41.299694, smooth loss: 64.764449\n",
      "iter 6400, loss: 45.226962, smooth loss: 64.714106\n",
      "iter 6410, loss: 51.231694, smooth loss: 64.589592\n",
      "iter 6420, loss: 70.524267, smooth loss: 64.541272\n",
      "iter 6430, loss: 56.562753, smooth loss: 64.451329\n",
      "iter 6440, loss: 48.016825, smooth loss: 64.435314\n",
      "iter 6450, loss: 80.604772, smooth loss: 64.332291\n",
      "iter 6460, loss: 56.915486, smooth loss: 64.236214\n",
      "iter 6470, loss: 51.245975, smooth loss: 64.136457\n",
      "iter 6480, loss: 50.633850, smooth loss: 64.073493\n",
      "iter 6490, loss: 83.696528, smooth loss: 64.052839\n",
      "iter 6500, loss: 63.647364, smooth loss: 64.017996\n",
      "iter 6510, loss: 37.360532, smooth loss: 63.947504\n",
      "iter 6520, loss: 50.111516, smooth loss: 63.850431\n",
      "iter 6530, loss: 58.311709, smooth loss: 63.809566\n",
      "iter 6540, loss: 54.396281, smooth loss: 63.730708\n",
      "iter 6550, loss: 50.733314, smooth loss: 63.685172\n",
      "iter 6560, loss: 64.716411, smooth loss: 63.581227\n",
      "iter 6570, loss: 50.370597, smooth loss: 63.466243\n",
      "iter 6580, loss: 52.678448, smooth loss: 63.362948\n",
      "iter 6590, loss: 70.233297, smooth loss: 63.304658\n",
      "iter 6600, loss: 54.138934, smooth loss: 63.266072\n",
      "iter 6610, loss: 89.574000, smooth loss: 63.246084\n",
      "iter 6620, loss: 48.337689, smooth loss: 63.133935\n",
      "iter 6630, loss: 63.562365, smooth loss: 63.037330\n",
      "iter 6640, loss: 52.725956, smooth loss: 62.971508\n",
      "iter 6650, loss: 59.375252, smooth loss: 62.877907\n",
      "iter 6660, loss: 39.189638, smooth loss: 62.794872\n",
      "iter 6670, loss: 43.639086, smooth loss: 62.700281\n",
      "iter 6680, loss: 41.308018, smooth loss: 62.556033\n",
      "iter 6690, loss: 64.781978, smooth loss: 62.468175\n",
      "iter 6700, loss: 48.817473, smooth loss: 62.401781\n",
      "iter 6710, loss: 54.598421, smooth loss: 62.375421\n",
      "iter 6720, loss: 44.436566, smooth loss: 62.364463\n",
      "iter 6730, loss: 68.720084, smooth loss: 62.297836\n",
      "iter 6740, loss: 47.987145, smooth loss: 62.196117\n",
      "iter 6750, loss: 39.960494, smooth loss: 62.143505\n",
      "iter 6760, loss: 60.064694, smooth loss: 62.087043\n",
      "iter 6770, loss: 59.589583, smooth loss: 62.013450\n",
      "iter 6780, loss: 44.881467, smooth loss: 61.883672\n",
      "iter 6790, loss: 55.777172, smooth loss: 61.745568\n",
      "iter 6800, loss: 62.224055, smooth loss: 61.673516\n",
      "iter 6810, loss: 67.247459, smooth loss: 61.602591\n",
      "iter 6820, loss: 51.026766, smooth loss: 61.554068\n",
      "iter 6830, loss: 52.596022, smooth loss: 61.554761\n",
      "iter 6840, loss: 41.651849, smooth loss: 61.453169\n",
      "iter 6850, loss: 56.975323, smooth loss: 61.362581\n",
      "iter 6860, loss: 48.377882, smooth loss: 61.307763\n",
      "iter 6870, loss: 60.449289, smooth loss: 61.251010\n",
      "iter 6880, loss: 53.571188, smooth loss: 61.176361\n",
      "iter 6890, loss: 50.241219, smooth loss: 61.040055\n",
      "iter 6900, loss: 56.874517, smooth loss: 60.920168\n",
      "iter 6910, loss: 60.100255, smooth loss: 60.851762\n",
      "iter 6920, loss: 39.103137, smooth loss: 60.746260\n",
      "iter 6930, loss: 71.335558, smooth loss: 60.763453\n",
      "iter 6940, loss: 71.720467, smooth loss: 60.754441\n",
      "iter 6950, loss: 49.129147, smooth loss: 60.645643\n",
      "iter 6960, loss: 77.904364, smooth loss: 60.572467\n",
      "iter 6970, loss: 44.954716, smooth loss: 60.480733\n",
      "iter 6980, loss: 51.706111, smooth loss: 60.437223\n",
      "iter 6990, loss: 33.669089, smooth loss: 60.371673\n",
      "iter 7000, loss: 46.752148, smooth loss: 60.269405\n",
      "iter 7010, loss: 59.912675, smooth loss: 60.168145\n",
      "iter 7020, loss: 47.118955, smooth loss: 60.107579\n",
      "iter 7030, loss: 69.218132, smooth loss: 60.066586\n",
      "iter 7040, loss: 73.019633, smooth loss: 60.088458\n",
      "iter 7050, loss: 58.166211, smooth loss: 60.071741\n",
      "iter 7060, loss: 54.986438, smooth loss: 59.994483\n",
      "iter 7070, loss: 55.644261, smooth loss: 59.933704\n",
      "iter 7080, loss: 70.024859, smooth loss: 59.892358\n",
      "iter 7090, loss: 64.727028, smooth loss: 59.853050\n",
      "iter 7100, loss: 57.613725, smooth loss: 59.786964\n",
      "iter 7110, loss: 47.936486, smooth loss: 59.684368\n",
      "iter 7120, loss: 54.470183, smooth loss: 59.595524\n",
      "iter 7130, loss: 43.750608, smooth loss: 59.512753\n",
      "iter 7140, loss: 53.946408, smooth loss: 59.442476\n",
      "iter 7150, loss: 66.953686, smooth loss: 59.457719\n",
      "iter 7160, loss: 64.908471, smooth loss: 59.429300\n",
      "iter 7170, loss: 49.730202, smooth loss: 59.329873\n",
      "iter 7180, loss: 45.671890, smooth loss: 59.270225\n",
      "iter 7190, loss: 53.598951, smooth loss: 59.211502\n",
      "iter 7200, loss: 44.322815, smooth loss: 59.149032\n",
      "iter 7210, loss: 40.241143, smooth loss: 59.075230\n",
      "iter 7220, loss: 35.466866, smooth loss: 58.957282\n",
      "iter 7230, loss: 51.908672, smooth loss: 58.896349\n",
      "iter 7240, loss: 51.489760, smooth loss: 58.844083\n",
      "iter 7250, loss: 61.662793, smooth loss: 58.822213\n",
      "iter 7260, loss: 55.573615, smooth loss: 58.837270\n",
      "iter 7270, loss: 55.331705, smooth loss: 58.823510\n",
      "iter 7280, loss: 43.824040, smooth loss: 58.734018\n",
      "iter 7290, loss: 54.488013, smooth loss: 58.692301\n",
      "iter 7300, loss: 55.478437, smooth loss: 58.636025\n",
      "iter 7310, loss: 79.434930, smooth loss: 58.635494\n",
      "iter 7320, loss: 33.819816, smooth loss: 58.513205\n",
      "iter 7330, loss: 47.758423, smooth loss: 58.412960\n",
      "iter 7340, loss: 49.251983, smooth loss: 58.353264\n",
      "iter 7350, loss: 54.902918, smooth loss: 58.292765\n",
      "iter 7360, loss: 61.036011, smooth loss: 58.267285\n",
      "iter 7370, loss: 58.718529, smooth loss: 58.255040\n",
      "iter 7380, loss: 40.284128, smooth loss: 58.210776\n",
      "iter 7390, loss: 44.912211, smooth loss: 58.112141\n",
      "iter 7400, loss: 43.571691, smooth loss: 58.044807\n",
      "iter 7410, loss: 55.417376, smooth loss: 58.013317\n",
      "iter 7420, loss: 50.724866, smooth loss: 57.964912\n",
      "iter 7430, loss: 55.453525, smooth loss: 57.853553\n",
      "iter 7440, loss: 51.676211, smooth loss: 57.769953\n",
      "iter 7450, loss: 49.634456, smooth loss: 57.702368\n",
      "iter 7460, loss: 68.055710, smooth loss: 57.685955\n",
      "iter 7470, loss: 39.556869, smooth loss: 57.635394\n",
      "iter 7480, loss: 29.527652, smooth loss: 57.628169\n",
      "iter 7490, loss: 41.500194, smooth loss: 57.600590\n",
      "iter 7500, loss: 50.875334, smooth loss: 57.500142\n",
      "iter 7510, loss: 75.597510, smooth loss: 57.468486\n",
      "iter 7520, loss: 46.934536, smooth loss: 57.385900\n",
      "iter 7530, loss: 41.355520, smooth loss: 57.366784\n",
      "iter 7540, loss: 57.610611, smooth loss: 57.259429\n",
      "iter 7550, loss: 47.224030, smooth loss: 57.180604\n",
      "iter 7560, loss: 45.153469, smooth loss: 57.109008\n",
      "iter 7570, loss: 43.952664, smooth loss: 57.082481\n",
      "iter 7580, loss: 70.460471, smooth loss: 57.077790\n",
      "iter 7590, loss: 50.032106, smooth loss: 57.071491\n",
      "iter 7600, loss: 31.189983, smooth loss: 57.027669\n",
      "iter 7610, loss: 41.913884, smooth loss: 56.929075\n",
      "iter 7620, loss: 48.330908, smooth loss: 56.891798\n",
      "iter 7630, loss: 51.114216, smooth loss: 56.822133\n",
      "iter 7640, loss: 46.191804, smooth loss: 56.790519\n",
      "iter 7650, loss: 57.804297, smooth loss: 56.691322\n",
      "iter 7660, loss: 46.808582, smooth loss: 56.580459\n",
      "iter 7670, loss: 42.390500, smooth loss: 56.504979\n",
      "iter 7680, loss: 58.203608, smooth loss: 56.478217\n",
      "iter 7690, loss: 51.082900, smooth loss: 56.466766\n",
      "iter 7700, loss: 76.537927, smooth loss: 56.490086\n",
      "iter 7710, loss: 42.509619, smooth loss: 56.420422\n",
      "iter 7720, loss: 58.047673, smooth loss: 56.336161\n",
      "iter 7730, loss: 48.171393, smooth loss: 56.288315\n",
      "iter 7740, loss: 57.176116, smooth loss: 56.256665\n",
      "iter 7750, loss: 39.319459, smooth loss: 56.232209\n",
      "iter 7760, loss: 39.706142, smooth loss: 56.134893\n",
      "iter 7770, loss: 36.533554, smooth loss: 56.009124\n",
      "iter 7780, loss: 56.040993, smooth loss: 55.947609\n",
      "iter 7790, loss: 41.421065, smooth loss: 55.910593\n",
      "iter 7800, loss: 48.939775, smooth loss: 55.905876\n",
      "iter 7810, loss: 40.417800, smooth loss: 55.925207\n",
      "iter 7820, loss: 66.996237, smooth loss: 55.891034\n",
      "iter 7830, loss: 43.352906, smooth loss: 55.800132\n",
      "iter 7840, loss: 42.058559, smooth loss: 55.783485\n",
      "iter 7850, loss: 49.245171, smooth loss: 55.752666\n",
      "iter 7860, loss: 60.328017, smooth loss: 55.716247\n",
      "iter 7870, loss: 38.424936, smooth loss: 55.653119\n",
      "iter 7880, loss: 53.214505, smooth loss: 55.562085\n",
      "iter 7890, loss: 58.806731, smooth loss: 55.524121\n",
      "iter 7900, loss: 58.433317, smooth loss: 55.484939\n",
      "iter 7910, loss: 46.997676, smooth loss: 55.476814\n",
      "iter 7920, loss: 52.606467, smooth loss: 55.519579\n",
      "iter 7930, loss: 41.761153, smooth loss: 55.458078\n",
      "iter 7940, loss: 52.233969, smooth loss: 55.397320\n",
      "iter 7950, loss: 38.077520, smooth loss: 55.343539\n",
      "iter 7960, loss: 50.115457, smooth loss: 55.311770\n",
      "iter 7970, loss: 50.566334, smooth loss: 55.300272\n",
      "iter 7980, loss: 52.542607, smooth loss: 55.236436\n",
      "iter 7990, loss: 48.962046, smooth loss: 55.144691\n",
      "iter 8000, loss: 54.642186, smooth loss: 55.096173\n",
      "iter 8010, loss: 41.731605, smooth loss: 55.027466\n",
      "iter 8020, loss: 66.336993, smooth loss: 55.043914\n",
      "iter 8030, loss: 76.818728, smooth loss: 55.069005\n",
      "iter 8040, loss: 46.144008, smooth loss: 54.999730\n",
      "iter 8050, loss: 67.973681, smooth loss: 54.949297\n",
      "iter 8060, loss: 44.726363, smooth loss: 54.857896\n",
      "iter 8070, loss: 41.466122, smooth loss: 54.809476\n",
      "iter 8080, loss: 29.608267, smooth loss: 54.723657\n",
      "iter 8090, loss: 41.538799, smooth loss: 54.635075\n",
      "iter 8100, loss: 60.353272, smooth loss: 54.549749\n",
      "iter 8110, loss: 39.816032, smooth loss: 54.489184\n",
      "iter 8120, loss: 63.712044, smooth loss: 54.458137\n",
      "iter 8130, loss: 64.167693, smooth loss: 54.450933\n",
      "iter 8140, loss: 54.004928, smooth loss: 54.463562\n",
      "iter 8150, loss: 53.625611, smooth loss: 54.366644\n",
      "iter 8160, loss: 50.543938, smooth loss: 54.328002\n",
      "iter 8170, loss: 65.139157, smooth loss: 54.292109\n",
      "iter 8180, loss: 54.743882, smooth loss: 54.271388\n",
      "iter 8190, loss: 51.246084, smooth loss: 54.202030\n",
      "iter 8200, loss: 39.379290, smooth loss: 54.117157\n",
      "iter 8210, loss: 48.531618, smooth loss: 54.027056\n",
      "iter 8220, loss: 42.678239, smooth loss: 53.974240\n",
      "iter 8230, loss: 45.394186, smooth loss: 53.900795\n",
      "iter 8240, loss: 67.347167, smooth loss: 53.927210\n",
      "iter 8250, loss: 47.354264, smooth loss: 53.898498\n",
      "iter 8260, loss: 49.836433, smooth loss: 53.807466\n",
      "iter 8270, loss: 41.158808, smooth loss: 53.722808\n",
      "iter 8280, loss: 52.744089, smooth loss: 53.694314\n",
      "iter 8290, loss: 39.487873, smooth loss: 53.642316\n",
      "iter 8300, loss: 42.034739, smooth loss: 53.575734\n",
      "iter 8310, loss: 30.406292, smooth loss: 53.475827\n",
      "iter 8320, loss: 40.097031, smooth loss: 53.403753\n",
      "iter 8330, loss: 43.117461, smooth loss: 53.346014\n",
      "iter 8340, loss: 57.379043, smooth loss: 53.298086\n",
      "iter 8350, loss: 45.716012, smooth loss: 53.285991\n",
      "iter 8360, loss: 45.231801, smooth loss: 53.260078\n",
      "iter 8370, loss: 42.238362, smooth loss: 53.178719\n",
      "iter 8380, loss: 45.446942, smooth loss: 53.134350\n",
      "iter 8390, loss: 44.570177, smooth loss: 53.116654\n",
      "iter 8400, loss: 72.291944, smooth loss: 53.107687\n",
      "iter 8410, loss: 32.146266, smooth loss: 53.005947\n",
      "iter 8420, loss: 47.056978, smooth loss: 52.928413\n",
      "iter 8430, loss: 42.429006, smooth loss: 52.863377\n",
      "iter 8440, loss: 48.480196, smooth loss: 52.814938\n",
      "iter 8450, loss: 57.866558, smooth loss: 52.779157\n",
      "iter 8460, loss: 51.052788, smooth loss: 52.785246\n",
      "iter 8470, loss: 35.896220, smooth loss: 52.738414\n",
      "iter 8480, loss: 37.808925, smooth loss: 52.639997\n",
      "iter 8490, loss: 38.749810, smooth loss: 52.573981\n",
      "iter 8500, loss: 51.781708, smooth loss: 52.540135\n",
      "iter 8510, loss: 59.216629, smooth loss: 52.538913\n",
      "iter 8520, loss: 54.713187, smooth loss: 52.422490\n",
      "iter 8530, loss: 49.685610, smooth loss: 52.317069\n",
      "iter 8540, loss: 44.161326, smooth loss: 52.238699\n",
      "iter 8550, loss: 43.195733, smooth loss: 52.195431\n",
      "iter 8560, loss: 37.533586, smooth loss: 52.155931\n",
      "iter 8570, loss: 30.756531, smooth loss: 52.135954\n",
      "iter 8580, loss: 39.441823, smooth loss: 52.088319\n",
      "iter 8590, loss: 44.748653, smooth loss: 51.986298\n",
      "iter 8600, loss: 54.236019, smooth loss: 51.934627\n",
      "iter 8610, loss: 47.405152, smooth loss: 51.878556\n",
      "iter 8620, loss: 37.275511, smooth loss: 51.890087\n",
      "iter 8630, loss: 56.150869, smooth loss: 51.808506\n",
      "iter 8640, loss: 45.738408, smooth loss: 51.710884\n",
      "iter 8650, loss: 47.008722, smooth loss: 51.655333\n",
      "iter 8660, loss: 41.528247, smooth loss: 51.622710\n",
      "iter 8670, loss: 63.967919, smooth loss: 51.604929\n",
      "iter 8680, loss: 46.182333, smooth loss: 51.591734\n",
      "iter 8690, loss: 30.067060, smooth loss: 51.539154\n",
      "iter 8700, loss: 37.014998, smooth loss: 51.478676\n",
      "iter 8710, loss: 53.093983, smooth loss: 51.451606\n",
      "iter 8720, loss: 44.985657, smooth loss: 51.391807\n",
      "iter 8730, loss: 52.481442, smooth loss: 51.390552\n",
      "iter 8740, loss: 50.028699, smooth loss: 51.297042\n",
      "iter 8750, loss: 40.241834, smooth loss: 51.181385\n",
      "iter 8760, loss: 39.714851, smooth loss: 51.121490\n",
      "iter 8770, loss: 57.111420, smooth loss: 51.098081\n",
      "iter 8780, loss: 43.553497, smooth loss: 51.054240\n",
      "iter 8790, loss: 71.645028, smooth loss: 51.068091\n",
      "iter 8800, loss: 46.104083, smooth loss: 51.001186\n",
      "iter 8810, loss: 54.505915, smooth loss: 50.938120\n",
      "iter 8820, loss: 47.365627, smooth loss: 50.909291\n",
      "iter 8830, loss: 51.029791, smooth loss: 50.890562\n",
      "iter 8840, loss: 31.368397, smooth loss: 50.841243\n",
      "iter 8850, loss: 33.754531, smooth loss: 50.752604\n",
      "iter 8860, loss: 28.395987, smooth loss: 50.634111\n",
      "iter 8870, loss: 57.718770, smooth loss: 50.577380\n",
      "iter 8880, loss: 33.674017, smooth loss: 50.542593\n",
      "iter 8890, loss: 42.114358, smooth loss: 50.516083\n",
      "iter 8900, loss: 40.162402, smooth loss: 50.563874\n",
      "iter 8910, loss: 59.077852, smooth loss: 50.518594\n",
      "iter 8920, loss: 32.284879, smooth loss: 50.451124\n",
      "iter 8930, loss: 29.741232, smooth loss: 50.416728\n",
      "iter 8940, loss: 50.094523, smooth loss: 50.416776\n",
      "iter 8950, loss: 44.901258, smooth loss: 50.370426\n",
      "iter 8960, loss: 32.347371, smooth loss: 50.269520\n",
      "iter 8970, loss: 48.675011, smooth loss: 50.175472\n",
      "iter 8980, loss: 60.369559, smooth loss: 50.128522\n",
      "iter 8990, loss: 55.751678, smooth loss: 50.080405\n",
      "iter 9000, loss: 48.924778, smooth loss: 50.076757\n",
      "iter 9010, loss: 46.097183, smooth loss: 50.098527\n",
      "iter 9020, loss: 31.744914, smooth loss: 50.041850\n",
      "iter 9030, loss: 48.686040, smooth loss: 49.982042\n",
      "iter 9040, loss: 39.220624, smooth loss: 49.962946\n",
      "iter 9050, loss: 44.918202, smooth loss: 49.957885\n",
      "iter 9060, loss: 46.201567, smooth loss: 49.913612\n",
      "iter 9070, loss: 44.460845, smooth loss: 49.815969\n",
      "iter 9080, loss: 45.085791, smooth loss: 49.742616\n",
      "iter 9090, loss: 52.624791, smooth loss: 49.731067\n",
      "iter 9100, loss: 33.139107, smooth loss: 49.679141\n",
      "iter 9110, loss: 63.890474, smooth loss: 49.681209\n",
      "iter 9120, loss: 63.613383, smooth loss: 49.711699\n",
      "iter 9130, loss: 44.361290, smooth loss: 49.651583\n",
      "iter 9140, loss: 66.091739, smooth loss: 49.625374\n",
      "iter 9150, loss: 44.580650, smooth loss: 49.573291\n",
      "iter 9160, loss: 39.196349, smooth loss: 49.557701\n",
      "iter 9170, loss: 27.917950, smooth loss: 49.495570\n",
      "iter 9180, loss: 50.449755, smooth loss: 49.418241\n",
      "iter 9190, loss: 53.311347, smooth loss: 49.327923\n",
      "iter 9200, loss: 37.880644, smooth loss: 49.297656\n",
      "iter 9210, loss: 51.551094, smooth loss: 49.237703\n",
      "iter 9220, loss: 60.728596, smooth loss: 49.237598\n",
      "iter 9230, loss: 50.375906, smooth loss: 49.237457\n",
      "iter 9240, loss: 50.355284, smooth loss: 49.166797\n",
      "iter 9250, loss: 48.794173, smooth loss: 49.112922\n",
      "iter 9260, loss: 65.109990, smooth loss: 49.083980\n",
      "iter 9270, loss: 50.440493, smooth loss: 49.049528\n",
      "iter 9280, loss: 43.061279, smooth loss: 48.990788\n",
      "iter 9290, loss: 38.006804, smooth loss: 48.894254\n",
      "iter 9300, loss: 44.919900, smooth loss: 48.812882\n",
      "iter 9310, loss: 37.897480, smooth loss: 48.759575\n",
      "iter 9320, loss: 49.738853, smooth loss: 48.712572\n",
      "iter 9330, loss: 58.203234, smooth loss: 48.732909\n",
      "iter 9340, loss: 46.095147, smooth loss: 48.709521\n",
      "iter 9350, loss: 40.880494, smooth loss: 48.610726\n",
      "iter 9360, loss: 37.654089, smooth loss: 48.560069\n",
      "iter 9370, loss: 44.259672, smooth loss: 48.494886\n",
      "iter 9380, loss: 41.417209, smooth loss: 48.446633\n",
      "iter 9390, loss: 37.376144, smooth loss: 48.381710\n",
      "iter 9400, loss: 26.503091, smooth loss: 48.277304\n",
      "iter 9410, loss: 34.975861, smooth loss: 48.197196\n",
      "iter 9420, loss: 33.435370, smooth loss: 48.148502\n",
      "iter 9430, loss: 49.820357, smooth loss: 48.110660\n",
      "iter 9440, loss: 42.006591, smooth loss: 48.109691\n",
      "iter 9450, loss: 38.800351, smooth loss: 48.092009\n",
      "iter 9460, loss: 34.305037, smooth loss: 48.002377\n",
      "iter 9470, loss: 43.880607, smooth loss: 47.961027\n",
      "iter 9480, loss: 53.929404, smooth loss: 47.917716\n",
      "iter 9490, loss: 71.787689, smooth loss: 47.889971\n",
      "iter 9500, loss: 30.046878, smooth loss: 47.787599\n",
      "iter 9510, loss: 40.531775, smooth loss: 47.720279\n",
      "iter 9520, loss: 39.115043, smooth loss: 47.667198\n",
      "iter 9530, loss: 45.176886, smooth loss: 47.624390\n",
      "iter 9540, loss: 50.847013, smooth loss: 47.592369\n",
      "iter 9550, loss: 47.489796, smooth loss: 47.592010\n",
      "iter 9560, loss: 30.780032, smooth loss: 47.556189\n",
      "iter 9570, loss: 37.447252, smooth loss: 47.479191\n",
      "iter 9580, loss: 36.110742, smooth loss: 47.449644\n",
      "iter 9590, loss: 41.556560, smooth loss: 47.403186\n",
      "iter 9600, loss: 50.755604, smooth loss: 47.403441\n",
      "iter 9610, loss: 41.185290, smooth loss: 47.308913\n",
      "iter 9620, loss: 41.890111, smooth loss: 47.238053\n",
      "iter 9630, loss: 44.425740, smooth loss: 47.181370\n",
      "iter 9640, loss: 44.244203, smooth loss: 47.157467\n",
      "iter 9650, loss: 35.775184, smooth loss: 47.123365\n",
      "iter 9660, loss: 27.149532, smooth loss: 47.128408\n",
      "iter 9670, loss: 36.638017, smooth loss: 47.116242\n",
      "iter 9680, loss: 44.453518, smooth loss: 47.048086\n",
      "iter 9690, loss: 55.768088, smooth loss: 47.021736\n",
      "iter 9700, loss: 42.213995, smooth loss: 46.974372\n",
      "iter 9710, loss: 40.050016, smooth loss: 46.950609\n",
      "iter 9720, loss: 47.517023, smooth loss: 46.860947\n",
      "iter 9730, loss: 40.786618, smooth loss: 46.811414\n",
      "iter 9740, loss: 36.567498, smooth loss: 46.758862\n",
      "iter 9750, loss: 38.556036, smooth loss: 46.732505\n",
      "iter 9760, loss: 64.915217, smooth loss: 46.726271\n",
      "iter 9770, loss: 42.502888, smooth loss: 46.722194\n",
      "iter 9780, loss: 25.045392, smooth loss: 46.698275\n",
      "iter 9790, loss: 36.435980, smooth loss: 46.642001\n",
      "iter 9800, loss: 39.098064, smooth loss: 46.631106\n",
      "iter 9810, loss: 39.533922, smooth loss: 46.618105\n",
      "iter 9820, loss: 33.050958, smooth loss: 46.596191\n",
      "iter 9830, loss: 46.480697, smooth loss: 46.532439\n",
      "iter 9840, loss: 39.527854, smooth loss: 46.465937\n",
      "iter 9850, loss: 37.388048, smooth loss: 46.425327\n",
      "iter 9860, loss: 45.897499, smooth loss: 46.393171\n",
      "iter 9870, loss: 40.703531, smooth loss: 46.385203\n",
      "iter 9880, loss: 66.487983, smooth loss: 46.414865\n",
      "iter 9890, loss: 33.761754, smooth loss: 46.355803\n",
      "iter 9900, loss: 46.545476, smooth loss: 46.303796\n",
      "iter 9910, loss: 35.506186, smooth loss: 46.271474\n",
      "iter 9920, loss: 48.137262, smooth loss: 46.232089\n",
      "iter 9930, loss: 34.765664, smooth loss: 46.227228\n",
      "iter 9940, loss: 37.886759, smooth loss: 46.159589\n",
      "iter 9950, loss: 31.210043, smooth loss: 46.087749\n",
      "iter 9960, loss: 46.880829, smooth loss: 46.047030\n",
      "iter 9970, loss: 29.605213, smooth loss: 46.007338\n",
      "iter 9980, loss: 39.200613, smooth loss: 46.005155\n",
      "iter 9990, loss: 38.874861, smooth loss: 46.003304\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 10000\n",
    "\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 10\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    \n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    if loss < 15:\n",
    "        break\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " Pizza have different as a which bedorsy, teath its sullarinnts of the M\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['i']  # Set the seed character index\n",
    "num_predictions = 50  # Set the desired number of predictions\n",
    "predictions = sample(hprev, 'Pizza have different ', num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
