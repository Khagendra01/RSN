{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "data has 10996 characters, 64 unique.\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import regex as re \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('pizza.txt', 'r') as file: \n",
    "\tdata = file.read()\n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "vocab_size = total_words\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index.get(\"pizza\")\n",
    "# tokenizer.index_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 25 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (687,687) and (1,10) not aligned: 687 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m [char_to_ix[ch] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m data[p\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:p\u001b[38;5;241m+\u001b[39mseq_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get gradients for current model based on input and target sequences\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev \u001b[38;5;241m=\u001b[39m \u001b[43mlossFun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m smooth_loss \u001b[38;5;241m=\u001b[39m smooth_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Occasionally print loss information\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m, in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     50\u001b[0m x[t] \u001b[38;5;241m=\u001b[39m embedding_layer(tf\u001b[38;5;241m.\u001b[39mexpand_dims(inputs[t], \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate update and reset gates\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m z[t] \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Uz, h[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m bz)\n\u001b[0;32m     54\u001b[0m r[t] \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(Wr, x[t]) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Ur, h[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m br)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate hidden units\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (687,687) and (1,10) not aligned: 687 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 1000\n",
    "\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 100\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0\n",
    "\n",
    "    # Get input and target sequence\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " panpeaadania  tyaea gsgimrir,imnr te z natnraasusr,ipp ratrpedsgakaareapmavan mcmnaaarsardrrizzsssdipttsasirla, aara giareotiiemgeaaiangryaemectasil,at repae alaaaemsrreimsnrdap pagyrny rapshsmgnz ebata rtpegru.aems uarys imlm stf,lruultrpstgg ominpppat ande aus iper aaancapssoars cia eamr isrp aaegnanoahdaaaiyglpieeso rdcss p-aruancsseasm stm a srimnzspor  o  s grbat s l taamccareuge,diyicupam tt ie g ps.raeu pip fg csmn s  aeirsaaIaisurgtsim ectlzamy csg msar r spaaisiarrsmtpata  ereasa.c adsIrsreyuaoagbnnesaa o etra asastt lctesmt rpru   arma,t paaae eemcsrseiarsi eporhrpssrtdrdauz  aasmim len saet vusnuspetn re grtpsptpagtsraameelerapgegyp snaalrmt,aaieeanadltaeraitmvdpuyr  zmtsnnrofmrypra rismt  imremcyavtis tuacprciara is aaaten s ccaaap rrr lre  tusdpvvamaasrumaivmasralaitrpdurgsa dos mimstslasetesuznrsg oitrs poz gsofaarueel szuislcy a a fdgie c t,pzhgatsaeteaege mlapsiepariturrczleb nrfsssaaaaaaorprumn sp   aagsyaea   sltc area rsdeesnsgcrcpais e tz ratavyrsegvtosgifagsgoaaapaf\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['p']  # Set the seed character index\n",
    "num_predictions = 1000  # Set the desired number of predictions\n",
    "predictions = sample(hprev, seed_ix, num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "x[0] = np.zeros((vocab_size, 1))\n",
    "x[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
