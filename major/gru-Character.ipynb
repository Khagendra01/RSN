{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Embedding\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors  \n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('pizza.txt', 'r') as file: \n",
    "\tdata = file.read()\n",
    "    \n",
    "word2vec_model = KeyedVectors.load_word2vec_format('word2vec.bin', binary=True)\n",
    "\n",
    "# Directly access the vocabulary and embedding dimension\n",
    "vocab = word2vec_model.key_to_index  # Accessing the vocabulary\n",
    "embedding_dim = word2vec_model.vector_size  # Getting the embedding dimension\n",
    "\n",
    "# Create a mapping from words to indices\n",
    "word_to_ix = {word: idx for idx, word in enumerate(vocab)}\n",
    "ix_to_word = {idx: word for word, idx in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 0,\n",
       " 'for': 1,\n",
       " 'that': 2,\n",
       " 'is': 3,\n",
       " 'on': 4,\n",
       " 'The': 5,\n",
       " 'with': 6,\n",
       " 'said': 7,\n",
       " 'was': 8,\n",
       " 'the': 9,\n",
       " 'at': 10,\n",
       " 'not': 11,\n",
       " 'as': 12,\n",
       " 'it': 13,\n",
       " 'be': 14,\n",
       " 'from': 15,\n",
       " 'by': 16,\n",
       " 'are': 17,\n",
       " 'I': 18,\n",
       " 'have': 19,\n",
       " 'he': 20,\n",
       " 'will': 21,\n",
       " 'has': 22,\n",
       " 'his': 23,\n",
       " 'an': 24,\n",
       " 'this': 25,\n",
       " 'or': 26,\n",
       " 'their': 27,\n",
       " 'who': 28,\n",
       " 'they': 29,\n",
       " 'but': 30,\n",
       " 'had': 31,\n",
       " 'year': 32,\n",
       " 'were': 33,\n",
       " 'we': 34,\n",
       " 'more': 35,\n",
       " 'up': 36,\n",
       " 'been': 37,\n",
       " 'you': 38,\n",
       " 'its': 39,\n",
       " 'one': 40,\n",
       " 'about': 41,\n",
       " 'would': 42,\n",
       " 'which': 43,\n",
       " 'out': 44,\n",
       " 'can': 45,\n",
       " 'It': 46,\n",
       " 'all': 47,\n",
       " 'also': 48,\n",
       " 'two': 49,\n",
       " 'after': 50,\n",
       " 'first': 51,\n",
       " 'He': 52,\n",
       " 'do': 53,\n",
       " 'time': 54,\n",
       " 'than': 55,\n",
       " 'when': 56,\n",
       " 'We': 57,\n",
       " 'over': 58,\n",
       " 'last': 59,\n",
       " 'new': 60,\n",
       " 'other': 61,\n",
       " 'her': 62,\n",
       " 'people': 63,\n",
       " 'into': 64,\n",
       " 'In': 65,\n",
       " 'our': 66,\n",
       " 'there': 67,\n",
       " 'A': 68,\n",
       " 'she': 69,\n",
       " 'could': 70,\n",
       " 'just': 71,\n",
       " 'years': 72,\n",
       " 'some': 73,\n",
       " 'U.S.': 74,\n",
       " 'three': 75,\n",
       " 'million': 76,\n",
       " 'them': 77,\n",
       " 'what': 78,\n",
       " 'But': 79,\n",
       " 'so': 80,\n",
       " 'no': 81,\n",
       " 'like': 82,\n",
       " 'if': 83,\n",
       " 'only': 84,\n",
       " 'percent': 85,\n",
       " 'get': 86,\n",
       " 'did': 87,\n",
       " 'him': 88,\n",
       " 'game': 89,\n",
       " 'back': 90,\n",
       " 'because': 91,\n",
       " 'now': 92,\n",
       " 'before': 93,\n",
       " 'company': 94,\n",
       " 'any': 95,\n",
       " 'team': 96,\n",
       " 'against': 97,\n",
       " 'off': 98,\n",
       " 'This': 99,\n",
       " 'most': 100,\n",
       " 'made': 101,\n",
       " 'through': 102,\n",
       " 'make': 103,\n",
       " 'second': 104,\n",
       " 'state': 105,\n",
       " 'well': 106,\n",
       " 'day': 107,\n",
       " 'season': 108,\n",
       " 'says': 109,\n",
       " 'week': 110,\n",
       " 'where': 111,\n",
       " 'while': 112,\n",
       " 'down': 113,\n",
       " 'being': 114,\n",
       " 'government': 115,\n",
       " 'your': 116,\n",
       " 'home': 117,\n",
       " 'going': 118,\n",
       " 'my': 119,\n",
       " 'good': 120,\n",
       " 'They': 121,\n",
       " \"'re\": 122,\n",
       " 'should': 123,\n",
       " 'many': 124,\n",
       " 'way': 125,\n",
       " 'those': 126,\n",
       " 'four': 127,\n",
       " 'during': 128,\n",
       " 'such': 129,\n",
       " 'may': 130,\n",
       " 'very': 131,\n",
       " 'how': 132,\n",
       " 'since': 133,\n",
       " 'work': 134,\n",
       " 'take': 135,\n",
       " 'including': 136,\n",
       " 'high': 137,\n",
       " 'then': 138,\n",
       " 'next': 139,\n",
       " 'By': 140,\n",
       " 'much': 141,\n",
       " 'still': 142,\n",
       " 'go': 143,\n",
       " 'think': 144,\n",
       " 'old': 145,\n",
       " 'even': 146,\n",
       " 'world': 147,\n",
       " 'see': 148,\n",
       " 'say': 149,\n",
       " 'business': 150,\n",
       " 'five': 151,\n",
       " 'told': 152,\n",
       " 'under': 153,\n",
       " 'us': 154,\n",
       " 'these': 155,\n",
       " 'If': 156,\n",
       " 'right': 157,\n",
       " 'And': 158,\n",
       " 'me': 159,\n",
       " 'between': 160,\n",
       " 'play': 161,\n",
       " 'help': 162,\n",
       " 'market': 163,\n",
       " 'That': 164,\n",
       " 'know': 165,\n",
       " 'end': 166,\n",
       " 'AP': 167,\n",
       " 'long': 168,\n",
       " 'information': 169,\n",
       " 'points': 170,\n",
       " 'does': 171,\n",
       " 'both': 172,\n",
       " 'There': 173,\n",
       " 'part': 174,\n",
       " 'around': 175,\n",
       " 'police': 176,\n",
       " 'want': 177,\n",
       " \"'ve\": 178,\n",
       " 'based': 179,\n",
       " 'For': 180,\n",
       " 'got': 181,\n",
       " 'third': 182,\n",
       " 'school': 183,\n",
       " 'left': 184,\n",
       " 'another': 185,\n",
       " 'country': 186,\n",
       " 'need': 187,\n",
       " '2': 188,\n",
       " 'best': 189,\n",
       " 'win': 190,\n",
       " 'quarter': 191,\n",
       " 'use': 192,\n",
       " 'today': 193,\n",
       " 'same': 194,\n",
       " 'public': 195,\n",
       " 'run': 196,\n",
       " 'Friday': 197,\n",
       " 'set': 198,\n",
       " 'month': 199,\n",
       " 'top': 200,\n",
       " 'billion': 201,\n",
       " 'Tuesday': 202,\n",
       " 'come': 203,\n",
       " 'Monday': 204,\n",
       " 'She': 205,\n",
       " 'city': 206,\n",
       " 'place': 207,\n",
       " 'night': 208,\n",
       " 'six': 209,\n",
       " 'each': 210,\n",
       " 'Thursday': 211,\n",
       " 'Wednesday': 212,\n",
       " 'here': 213,\n",
       " 'You': 214,\n",
       " 'group': 215,\n",
       " 'really': 216,\n",
       " 'found': 217,\n",
       " 'As': 218,\n",
       " 'used': 219,\n",
       " 'lot': 220,\n",
       " \"'m\": 221,\n",
       " 'money': 222,\n",
       " 'put': 223,\n",
       " 'games': 224,\n",
       " 'support': 225,\n",
       " 'program': 226,\n",
       " 'half': 227,\n",
       " 'report': 228,\n",
       " 'family': 229,\n",
       " 'months': 230,\n",
       " 'number': 231,\n",
       " 'officials': 232,\n",
       " 'am': 233,\n",
       " 'former': 234,\n",
       " 'own': 235,\n",
       " 'man': 236,\n",
       " 'Saturday': 237,\n",
       " 'too': 238,\n",
       " 'better': 239,\n",
       " 'days': 240,\n",
       " 'came': 241,\n",
       " 'lead': 242,\n",
       " 'life': 243,\n",
       " 'American': 244,\n",
       " 'show': 245,\n",
       " 'past': 246,\n",
       " 'took': 247,\n",
       " 'added': 248,\n",
       " 'expected': 249,\n",
       " 'called': 250,\n",
       " 'great': 251,\n",
       " 'State': 252,\n",
       " 'services': 253,\n",
       " 'children': 254,\n",
       " 'hit': 255,\n",
       " 'area': 256,\n",
       " 'system': 257,\n",
       " 'every': 258,\n",
       " 'pm': 259,\n",
       " 'big': 260,\n",
       " 'service': 261,\n",
       " 'few': 262,\n",
       " 'per': 263,\n",
       " 'members': 264,\n",
       " 'Sunday': 265,\n",
       " 'early': 266,\n",
       " 'point': 267,\n",
       " 'start': 268,\n",
       " 'companies': 269,\n",
       " 'little': 270,\n",
       " 'case': 271,\n",
       " 'ago': 272,\n",
       " 'local': 273,\n",
       " 'according': 274,\n",
       " 'never': 275,\n",
       " 'without': 276,\n",
       " 'sales': 277,\n",
       " 'until': 278,\n",
       " 'went': 279,\n",
       " 'players': 280,\n",
       " 'won': 281,\n",
       " 'financial': 282,\n",
       " 'news': 283,\n",
       " 'When': 284,\n",
       " 'share': 285,\n",
       " 'several': 286,\n",
       " 'free': 287,\n",
       " 'away': 288,\n",
       " 'already': 289,\n",
       " 'On': 290,\n",
       " 'industry': 291,\n",
       " \"'ll\": 292,\n",
       " 'call': 293,\n",
       " 'With': 294,\n",
       " 'students': 295,\n",
       " 'line': 296,\n",
       " 'available': 297,\n",
       " 'County': 298,\n",
       " 'making': 299,\n",
       " 'held': 300,\n",
       " 'final': 301,\n",
       " 'power': 302,\n",
       " 'plan': 303,\n",
       " 'might': 304,\n",
       " 'least': 305,\n",
       " 'look': 306,\n",
       " 'forward': 307,\n",
       " 'give': 308,\n",
       " 'At': 309,\n",
       " 'again': 310,\n",
       " 'later': 311,\n",
       " 'full': 312,\n",
       " 'must': 313,\n",
       " 'things': 314,\n",
       " 'major': 315,\n",
       " 'community': 316,\n",
       " 'announced': 317,\n",
       " 'open': 318,\n",
       " 'record': 319,\n",
       " 'reported': 320,\n",
       " 'court': 321,\n",
       " 'working': 322,\n",
       " 'able': 323,\n",
       " 'something': 324,\n",
       " 'president': 325,\n",
       " 'meeting': 326,\n",
       " 'keep': 327,\n",
       " 'March': 328,\n",
       " 'future': 329,\n",
       " 'far': 330,\n",
       " 'deal': 331,\n",
       " 'City': 332,\n",
       " 'May': 333,\n",
       " 'development': 334,\n",
       " 'University': 335,\n",
       " 'find': 336,\n",
       " 'times': 337,\n",
       " 'After': 338,\n",
       " 'office': 339,\n",
       " 'led': 340,\n",
       " 'among': 341,\n",
       " 'June': 342,\n",
       " 'increase': 343,\n",
       " 'China': 344,\n",
       " 'John': 345,\n",
       " 'whether': 346,\n",
       " 'cost': 347,\n",
       " 'security': 348,\n",
       " 'job': 349,\n",
       " 'less': 350,\n",
       " 'head': 351,\n",
       " 'seven': 352,\n",
       " 'growth': 353,\n",
       " 'lost': 354,\n",
       " 'pay': 355,\n",
       " 'looking': 356,\n",
       " 'provide': 357,\n",
       " 'To': 358,\n",
       " 'plans': 359,\n",
       " 'products': 360,\n",
       " 'car': 361,\n",
       " 'recent': 362,\n",
       " 'hard': 363,\n",
       " 'always': 364,\n",
       " 'include': 365,\n",
       " 'women': 366,\n",
       " 'across': 367,\n",
       " 'tax': 368,\n",
       " 'water': 369,\n",
       " 'April': 370,\n",
       " 'continue': 371,\n",
       " 'important': 372,\n",
       " 'different': 373,\n",
       " 'close': 374,\n",
       " 'One': 375,\n",
       " 'late': 376,\n",
       " 'decision': 377,\n",
       " 'current': 378,\n",
       " 'law': 379,\n",
       " 'within': 380,\n",
       " 'along': 381,\n",
       " 'played': 382,\n",
       " 'move': 383,\n",
       " 'enough': 384,\n",
       " 'become': 385,\n",
       " 'side': 386,\n",
       " 'national': 387,\n",
       " 'Inc.': 388,\n",
       " 'results': 389,\n",
       " 'level': 390,\n",
       " 'loss': 391,\n",
       " 'economic': 392,\n",
       " 'coach': 393,\n",
       " 'near': 394,\n",
       " 'getting': 395,\n",
       " 'price': 396,\n",
       " 'Department': 397,\n",
       " 'event': 398,\n",
       " 'fourth': 399,\n",
       " 'change': 400,\n",
       " 'All': 401,\n",
       " 'small': 402,\n",
       " 'board': 403,\n",
       " 'National': 404,\n",
       " 'So': 405,\n",
       " 'goal': 406,\n",
       " 'taken': 407,\n",
       " 'field': 408,\n",
       " 'prices': 409,\n",
       " 'weeks': 410,\n",
       " 'men': 411,\n",
       " 'asked': 412,\n",
       " 'eight': 413,\n",
       " 'data': 414,\n",
       " 'shot': 415,\n",
       " 'New': 416,\n",
       " 'started': 417,\n",
       " 'July': 418,\n",
       " 'director': 419,\n",
       " 'President': 420,\n",
       " 'party': 421,\n",
       " 'federal': 422,\n",
       " 'done': 423,\n",
       " 'political': 424,\n",
       " 'minutes': 425,\n",
       " 'taking': 426,\n",
       " 'Company': 427,\n",
       " 'technology': 428,\n",
       " 'project': 429,\n",
       " 'center': 430,\n",
       " 'leading': 431,\n",
       " 'issue': 432,\n",
       " 'though': 433,\n",
       " 'having': 434,\n",
       " 'period': 435,\n",
       " 'likely': 436,\n",
       " 'scored': 437,\n",
       " 'strong': 438,\n",
       " 'series': 439,\n",
       " 'military': 440,\n",
       " 'seen': 441,\n",
       " 'trying': 442,\n",
       " 'What': 443,\n",
       " 'coming': 444,\n",
       " 'process': 445,\n",
       " 'building': 446,\n",
       " 'behind': 447,\n",
       " 'performance': 448,\n",
       " 'management': 449,\n",
       " 'Iraq': 450,\n",
       " 'saying': 451,\n",
       " 'earlier': 452,\n",
       " 'believe': 453,\n",
       " 'oil': 454,\n",
       " 'given': 455,\n",
       " 'Police': 456,\n",
       " 'customers': 457,\n",
       " 'due': 458,\n",
       " 'following': 459,\n",
       " 'term': 460,\n",
       " 'others': 461,\n",
       " 'statement': 462,\n",
       " 'international': 463,\n",
       " 'economy': 464,\n",
       " 'health': 465,\n",
       " 'thing': 466,\n",
       " 'Obama': 467,\n",
       " 'return': 468,\n",
       " 'killed': 469,\n",
       " 'Washington': 470,\n",
       " 'further': 471,\n",
       " 'However': 472,\n",
       " 'doing': 473,\n",
       " 'face': 474,\n",
       " 'low': 475,\n",
       " 'higher': 476,\n",
       " 'site': 477,\n",
       " 'once': 478,\n",
       " 'yet': 479,\n",
       " 'hours': 480,\n",
       " 'America': 481,\n",
       " 'control': 482,\n",
       " 'received': 483,\n",
       " 'rate': 484,\n",
       " 'career': 485,\n",
       " 'Bush': 486,\n",
       " 'teams': 487,\n",
       " 'known': 488,\n",
       " 'offer': 489,\n",
       " 'race': 490,\n",
       " 'ever': 491,\n",
       " 'experience': 492,\n",
       " 'playing': 493,\n",
       " 'name': 494,\n",
       " 'possible': 495,\n",
       " 'countries': 496,\n",
       " 'Mr.': 497,\n",
       " 'average': 498,\n",
       " 'together': 499,\n",
       " 'using': 500,\n",
       " 'cut': 501,\n",
       " 'While': 502,\n",
       " 'total': 503,\n",
       " 'round': 504,\n",
       " 'young': 505,\n",
       " 'nearly': 506,\n",
       " 'shares': 507,\n",
       " 'member': 508,\n",
       " 'campaign': 509,\n",
       " 'media': 510,\n",
       " 'needs': 511,\n",
       " 'why': 512,\n",
       " 'house': 513,\n",
       " 'issues': 514,\n",
       " 'costs': 515,\n",
       " 'fire': 516,\n",
       " 'victory': 517,\n",
       " 'player': 518,\n",
       " 'began': 519,\n",
       " 'sure': 520,\n",
       " 'story': 521,\n",
       " 'North': 522,\n",
       " 'His': 523,\n",
       " 'staff': 524,\n",
       " 'order': 525,\n",
       " 'war': 526,\n",
       " 'large': 527,\n",
       " 'interest': 528,\n",
       " 'stock': 529,\n",
       " 'food': 530,\n",
       " 'research': 531,\n",
       " 'key': 532,\n",
       " 'India': 533,\n",
       " 'South': 534,\n",
       " 'morning': 535,\n",
       " 'conference': 536,\n",
       " 'senior': 537,\n",
       " 'global': 538,\n",
       " 'Center': 539,\n",
       " 'death': 540,\n",
       " 'person': 541,\n",
       " 'thought': 542,\n",
       " 'gave': 543,\n",
       " 'feel': 544,\n",
       " 'energy': 545,\n",
       " 'history': 546,\n",
       " 'recently': 547,\n",
       " 'largest': 548,\n",
       " 'general': 549,\n",
       " 'official': 550,\n",
       " 'released': 551,\n",
       " 'wanted': 552,\n",
       " 'meet': 553,\n",
       " 'short': 554,\n",
       " 'outside': 555,\n",
       " 'running': 556,\n",
       " 'live': 557,\n",
       " 'ball': 558,\n",
       " 'online': 559,\n",
       " 'real': 560,\n",
       " 'position': 561,\n",
       " 'fact': 562,\n",
       " 'fell': 563,\n",
       " 'nine': 564,\n",
       " 'December': 565,\n",
       " 'front': 566,\n",
       " 'action': 567,\n",
       " 'defense': 568,\n",
       " 'problem': 569,\n",
       " 'problems': 570,\n",
       " 'Mr': 571,\n",
       " 'nation': 572,\n",
       " 'needed': 573,\n",
       " 'special': 574,\n",
       " 'January': 575,\n",
       " 'almost': 576,\n",
       " 'chance': 577,\n",
       " \"'d\": 578,\n",
       " 'result': 579,\n",
       " 'West': 580,\n",
       " 'September': 581,\n",
       " 'reports': 582,\n",
       " 'leader': 583,\n",
       " 'investment': 584,\n",
       " 'yesterday': 585,\n",
       " 'Some': 586,\n",
       " 'leaders': 587,\n",
       " 'ahead': 588,\n",
       " 'production': 589,\n",
       " 'comes': 590,\n",
       " 'No': 591,\n",
       " 'runs': 592,\n",
       " 'match': 593,\n",
       " 'role': 594,\n",
       " 'kind': 595,\n",
       " 'try': 596,\n",
       " 'ended': 597,\n",
       " 'risk': 598,\n",
       " 'areas': 599,\n",
       " 'election': 600,\n",
       " 'workers': 601,\n",
       " 'visit': 602,\n",
       " 'bring': 603,\n",
       " 'road': 604,\n",
       " 'music': 605,\n",
       " 'study': 606,\n",
       " 'makes': 607,\n",
       " 'often': 608,\n",
       " 'release': 609,\n",
       " 'woman': 610,\n",
       " 'vote': 611,\n",
       " 'care': 612,\n",
       " 'town': 613,\n",
       " 'clear': 614,\n",
       " 'comment': 615,\n",
       " 'budget': 616,\n",
       " 'potential': 617,\n",
       " 'single': 618,\n",
       " 'markets': 619,\n",
       " 'policy': 620,\n",
       " 'capital': 621,\n",
       " 'saw': 622,\n",
       " 'access': 623,\n",
       " 'weekend': 624,\n",
       " 'operations': 625,\n",
       " 'whose': 626,\n",
       " 'net': 627,\n",
       " 'House': 628,\n",
       " 'hand': 629,\n",
       " 'increased': 630,\n",
       " 'charges': 631,\n",
       " 'winning': 632,\n",
       " 'trade': 633,\n",
       " 'These': 634,\n",
       " 'income': 635,\n",
       " 'value': 636,\n",
       " 'involved': 637,\n",
       " 'Bank': 638,\n",
       " 'November': 639,\n",
       " 'bill': 640,\n",
       " 'compared': 641,\n",
       " 'anything': 642,\n",
       " 'manager': 643,\n",
       " 'Texas': 644,\n",
       " 'property': 645,\n",
       " 'stop': 646,\n",
       " 'annual': 647,\n",
       " 'private': 648,\n",
       " 'contract': 649,\n",
       " 'died': 650,\n",
       " 'Now': 651,\n",
       " 'hope': 652,\n",
       " 'product': 653,\n",
       " 'fans': 654,\n",
       " 'lower': 655,\n",
       " 'demand': 656,\n",
       " 'News': 657,\n",
       " 'David': 658,\n",
       " 'club': 659,\n",
       " 'comments': 660,\n",
       " 'film': 661,\n",
       " 'yards': 662,\n",
       " 'quality': 663,\n",
       " 'currently': 664,\n",
       " 'events': 665,\n",
       " 'addition': 666,\n",
       " 'couple': 667,\n",
       " 'schools': 668,\n",
       " 'attack': 669,\n",
       " 'region': 670,\n",
       " 'latest': 671,\n",
       " 'opportunity': 672,\n",
       " 'worked': 673,\n",
       " 'course': 674,\n",
       " 'bad': 675,\n",
       " 'fall': 676,\n",
       " 'Group': 677,\n",
       " 'October': 678,\n",
       " 'jobs': 679,\n",
       " 'list': 680,\n",
       " 'let': 681,\n",
       " 'however': 682,\n",
       " 'chief': 683,\n",
       " 'summer': 684,\n",
       " 'programs': 685,\n",
       " 'According': 686,\n",
       " 'revenue': 687,\n",
       " 'Our': 688,\n",
       " 'rose': 689,\n",
       " 'previous': 690,\n",
       " 'TV': 691,\n",
       " 'football': 692,\n",
       " 'biggest': 693,\n",
       " 'employees': 694,\n",
       " 'changes': 695,\n",
       " 'residents': 696,\n",
       " 'means': 697,\n",
       " 'agreement': 698,\n",
       " 'includes': 699,\n",
       " 'post': 700,\n",
       " 'Canada': 701,\n",
       " 'probably': 702,\n",
       " 'related': 703,\n",
       " 'training': 704,\n",
       " 'allowed': 705,\n",
       " 'class': 706,\n",
       " 'bit': 707,\n",
       " 'video': 708,\n",
       " 'Michael': 709,\n",
       " 'An': 710,\n",
       " 'sent': 711,\n",
       " 'education': 712,\n",
       " 'states': 713,\n",
       " 'straight': 714,\n",
       " 'love': 715,\n",
       " 'beat': 716,\n",
       " 'hold': 717,\n",
       " 'turn': 718,\n",
       " 'finished': 719,\n",
       " 'network': 720,\n",
       " 'Smith': 721,\n",
       " 'buy': 722,\n",
       " 'foreign': 723,\n",
       " 'especially': 724,\n",
       " 'groups': 725,\n",
       " 'wants': 726,\n",
       " 'title': 727,\n",
       " 'included': 728,\n",
       " 'turned': 729,\n",
       " 'bank': 730,\n",
       " 'Florida': 731,\n",
       " 'efforts': 732,\n",
       " 'personal': 733,\n",
       " 'businesses': 734,\n",
       " 'August': 735,\n",
       " 'California': 736,\n",
       " 'situation': 737,\n",
       " 'district': 738,\n",
       " 'allow': 739,\n",
       " 'helped': 740,\n",
       " 'body': 741,\n",
       " 'nothing': 742,\n",
       " 'soon': 743,\n",
       " 'safety': 744,\n",
       " 'officer': 745,\n",
       " 'cents': 746,\n",
       " 'Europe': 747,\n",
       " 'St.': 748,\n",
       " 'additional': 749,\n",
       " 'spokesman': 750,\n",
       " 'February': 751,\n",
       " 'wife': 752,\n",
       " 'showed': 753,\n",
       " 'leave': 754,\n",
       " 'investors': 755,\n",
       " 'parents': 756,\n",
       " 'medical': 757,\n",
       " 'spending': 758,\n",
       " 'non': 759,\n",
       " 'London': 760,\n",
       " 'Council': 761,\n",
       " 'matter': 762,\n",
       " 'spent': 763,\n",
       " 'child': 764,\n",
       " 'World': 765,\n",
       " 'effort': 766,\n",
       " 'opening': 767,\n",
       " 'either': 768,\n",
       " 'range': 769,\n",
       " 'question': 770,\n",
       " 'European': 771,\n",
       " 'goals': 772,\n",
       " 'administration': 773,\n",
       " 'friends': 774,\n",
       " 'himself': 775,\n",
       " 'shows': 776,\n",
       " 'difficult': 777,\n",
       " 'kids': 778,\n",
       " 'paid': 779,\n",
       " 'create': 780,\n",
       " 'cash': 781,\n",
       " 'age': 782,\n",
       " 'league': 783,\n",
       " 'form': 784,\n",
       " 'impact': 785,\n",
       " 'drive': 786,\n",
       " 'someone': 787,\n",
       " 'became': 788,\n",
       " 'stay': 789,\n",
       " 'fight': 790,\n",
       " 'significant': 791,\n",
       " 'firm': 792,\n",
       " 'Senate': 793,\n",
       " 'hospital': 794,\n",
       " 'charged': 795,\n",
       " 'operating': 796,\n",
       " 'main': 797,\n",
       " 'book': 798,\n",
       " 'success': 799,\n",
       " 'son': 800,\n",
       " 'trading': 801,\n",
       " 'focus': 802,\n",
       " 'room': 803,\n",
       " 'continued': 804,\n",
       " 'Congress': 805,\n",
       " 'everything': 806,\n",
       " 'Park': 807,\n",
       " 'agency': 808,\n",
       " 'brought': 809,\n",
       " 'talk': 810,\n",
       " 'break': 811,\n",
       " 'air': 812,\n",
       " 'software': 813,\n",
       " 'decided': 814,\n",
       " 'Do': 815,\n",
       " 'ready': 816,\n",
       " 'arrested': 817,\n",
       " 'track': 818,\n",
       " 'provides': 819,\n",
       " 'mother': 820,\n",
       " 'base': 821,\n",
       " 'trial': 822,\n",
       " 'phone': 823,\n",
       " 'My': 824,\n",
       " 'build': 825,\n",
       " 'conditions': 826,\n",
       " 'rest': 827,\n",
       " 'Johnson': 828,\n",
       " 'terms': 829,\n",
       " 'expect': 830,\n",
       " 'England': 831,\n",
       " 'Israel': 832,\n",
       " 'despite': 833,\n",
       " 'closed': 834,\n",
       " 'starting': 835,\n",
       " 'provided': 836,\n",
       " 'pressure': 837,\n",
       " 'lives': 838,\n",
       " 'step': 839,\n",
       " 'remain': 840,\n",
       " 'similar': 841,\n",
       " 'charge': 842,\n",
       " 'date': 843,\n",
       " 'whole': 844,\n",
       " 'land': 845,\n",
       " 'growing': 846,\n",
       " 'James': 847,\n",
       " 'Internet': 848,\n",
       " 'projects': 849,\n",
       " 'British': 850,\n",
       " 'cases': 851,\n",
       " 'ground': 852,\n",
       " 'legal': 853,\n",
       " 'International': 854,\n",
       " 'agreed': 855,\n",
       " 'tell': 856,\n",
       " 'test': 857,\n",
       " 'everyone': 858,\n",
       " 'pretty': 859,\n",
       " 'authorities': 860,\n",
       " 'Two': 861,\n",
       " 'above': 862,\n",
       " 'moved': 863,\n",
       " 'profit': 864,\n",
       " 'throughout': 865,\n",
       " 'inside': 866,\n",
       " 'ability': 867,\n",
       " 'overall': 868,\n",
       " 'pass': 869,\n",
       " 'officers': 870,\n",
       " 'rather': 871,\n",
       " 'Australia': 872,\n",
       " 'actually': 873,\n",
       " 'county': 874,\n",
       " 'amount': 875,\n",
       " 'scheduled': 876,\n",
       " 'themselves': 877,\n",
       " 'organization': 878,\n",
       " 'giving': 879,\n",
       " 'credit': 880,\n",
       " 'father': 881,\n",
       " 'drug': 882,\n",
       " 'investigation': 883,\n",
       " 'families': 884,\n",
       " 'Republican': 885,\n",
       " 'funds': 886,\n",
       " 'patients': 887,\n",
       " 'takes': 888,\n",
       " 'systems': 889,\n",
       " 'Japan': 890,\n",
       " 'complete': 891,\n",
       " 'sold': 892,\n",
       " 'practice': 893,\n",
       " 'calls': 894,\n",
       " 'UK': 895,\n",
       " 'force': 896,\n",
       " 'student': 897,\n",
       " 'idea': 898,\n",
       " 'reached': 899,\n",
       " 'reason': 900,\n",
       " 'levels': 901,\n",
       " 'space': 902,\n",
       " 'competition': 903,\n",
       " 'forces': 904,\n",
       " 'sector': 905,\n",
       " 'Last': 906,\n",
       " 'tried': 907,\n",
       " 'common': 908,\n",
       " 'homes': 909,\n",
       " 'stage': 910,\n",
       " 'department': 911,\n",
       " 'named': 912,\n",
       " 'earnings': 913,\n",
       " 'offers': 914,\n",
       " 'star': 915,\n",
       " 'certain': 916,\n",
       " 'double': 917,\n",
       " 'longer': 918,\n",
       " 'followed': 919,\n",
       " 'cause': 920,\n",
       " 'Association': 921,\n",
       " 'signed': 922,\n",
       " 'committee': 923,\n",
       " 'hour': 924,\n",
       " 'college': 925,\n",
       " 'Pakistan': 926,\n",
       " 'users': 927,\n",
       " 'Iran': 928,\n",
       " 'sign': 929,\n",
       " 'living': 930,\n",
       " 'failed': 931,\n",
       " 'reach': 932,\n",
       " 'quickly': 933,\n",
       " 'receive': 934,\n",
       " 'debt': 935,\n",
       " 'sale': 936,\n",
       " 'Board': 937,\n",
       " 'Americans': 938,\n",
       " 'Road': 939,\n",
       " 'Brown': 940,\n",
       " 'insurance': 941,\n",
       " 'anyone': 942,\n",
       " 'tournament': 943,\n",
       " 'More': 944,\n",
       " 'gas': 945,\n",
       " 'talks': 946,\n",
       " 'serious': 947,\n",
       " 'required': 948,\n",
       " 'sell': 949,\n",
       " 'construction': 950,\n",
       " 'evidence': 951,\n",
       " 'remains': 952,\n",
       " 'black': 953,\n",
       " 'below': 954,\n",
       " 'improve': 955,\n",
       " 'crisis': 956,\n",
       " 'address': 957,\n",
       " 'questions': 958,\n",
       " 'easy': 959,\n",
       " 'begin': 960,\n",
       " 'view': 961,\n",
       " 'School': 962,\n",
       " 'heard': 963,\n",
       " 'executive': 964,\n",
       " 'raised': 965,\n",
       " 'Paul': 966,\n",
       " 'store': 967,\n",
       " 'gets': 968,\n",
       " 'filed': 969,\n",
       " 'huge': 970,\n",
       " 'moving': 971,\n",
       " 'seems': 972,\n",
       " 'met': 973,\n",
       " 'thousands': 974,\n",
       " 'CEO': 975,\n",
       " 'solutions': 976,\n",
       " 'score': 977,\n",
       " 'content': 978,\n",
       " 'treatment': 979,\n",
       " 'offered': 980,\n",
       " 'built': 981,\n",
       " 'hits': 982,\n",
       " 'present': 983,\n",
       " 'Chicago': 984,\n",
       " 'attacks': 985,\n",
       " 'although': 986,\n",
       " 'social': 987,\n",
       " 'afternoon': 988,\n",
       " 'vehicle': 989,\n",
       " 'read': 990,\n",
       " 'example': 991,\n",
       " 'rise': 992,\n",
       " 'minute': 993,\n",
       " 'banks': 994,\n",
       " 'council': 995,\n",
       " 'troops': 996,\n",
       " 'rates': 997,\n",
       " 'tough': 998,\n",
       " 'Chinese': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "vocab_size = total_words\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index.get(\"pizza\")\n",
    "# tokenizer.index_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = embedding_dim, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 25 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {0: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x = word2vec_model.wv[ix_to_word[inputs[t]]]\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n",
    "        dbh += dh_hat_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        dr = np.multiply(drhp, h[t-1])\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        dUr += np.dot(dr_l, h[t-1].T)\n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t])\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        dUz += np.dot(dz_l, h[t-1].T)\n",
    "        dbz += dz_l\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l)\n",
    "        dh_fz = np.multiply(dh, z[t])\n",
    "        dh_fhh = np.multiply(drhp, r[t])\n",
    "        dh_fr = np.dot(Ur.T, dr_l)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    ixes = [seed_ix]\n",
    "    x = word2vec_model.wv[ix_to_word[seed_ix]]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = word2vec_model.wv[ix_to_word[ix]]\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m targets \u001b[38;5;241m=\u001b[39m [word_to_ix[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m seq[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get gradients for current model based on input and target sequences\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev \u001b[38;5;241m=\u001b[39m \u001b[43mlossFun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m smooth_loss \u001b[38;5;241m=\u001b[39m smooth_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Occasionally print loss information\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 124\u001b[0m, in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     dhnext \u001b[38;5;241m=\u001b[39m dh_fz_inner \u001b[38;5;241m+\u001b[39m dh_fz \u001b[38;5;241m+\u001b[39m dh_fhh \u001b[38;5;241m+\u001b[39m dh_fr\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, \u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of iterations\n",
    "max_iters = 1000\n",
    "n = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "print_interval = 100\n",
    "\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "\n",
    "while n < max_iters:\n",
    "    # Reset memory\n",
    "    hprev = np.zeros((h_size, 1))\n",
    "\n",
    "    # Get input and target sequences\n",
    "    for seq in data:\n",
    "        inputs = [word_to_ix[word] for word in seq[:-1]]\n",
    "        targets = [word_to_ix[word] for word in seq[1:]]\n",
    "\n",
    "        # Get gradients for current model based on input and target sequences\n",
    "        loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "        # Occasionally print loss information\n",
    "        if n % print_interval == 0:\n",
    "            print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "        # Update model with adagrad (stochastic) gradient descent\n",
    "        for param, dparam, mem in zip([Wy, Wh, Wr, Wz, Uh, Ur, Uz, by, bh, br, bz],\n",
    "                                      [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                      [mdWy, mdWh, mdWr, mdWz, mdUh, mdUr, mdUz, mdby, mdbh, mdbr, mdbz]):\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # Small added term for numerical stability\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:\n",
      " panpeaadania  tyaea gsgimrir,imnr te z natnraasusr,ipp ratrpedsgakaareapmavan mcmnaaarsardrrizzsssdipttsasirla, aara giareotiiemgeaaiangryaemectasil,at repae alaaaemsrreimsnrdap pagyrny rapshsmgnz ebata rtpegru.aems uarys imlm stf,lruultrpstgg ominpppat ande aus iper aaancapssoars cia eamr isrp aaegnanoahdaaaiyglpieeso rdcss p-aruancsseasm stm a srimnzspor  o  s grbat s l taamccareuge,diyicupam tt ie g ps.raeu pip fg csmn s  aeirsaaIaisurgtsim ectlzamy csg msar r spaaisiarrsmtpata  ereasa.c adsIrsreyuaoagbnnesaa o etra asastt lctesmt rpru   arma,t paaae eemcsrseiarsi eporhrpssrtdrdauz  aasmim len saet vusnuspetn re grtpsptpagtsraameelerapgegyp snaalrmt,aaieeanadltaeraitmvdpuyr  zmtsnnrofmrypra rismt  imremcyavtis tuacprciara is aaaten s ccaaap rrr lre  tusdpvvamaasrumaivmasralaitrpdurgsa dos mimstslasetesuznrsg oitrs poz gsofaarueel szuislcy a a fdgie c t,pzhgatsaeteaege mlapsiepariturrczleb nrfsssaaaaaaorprumn sp   aagsyaea   sltc area rsdeesnsgcrcpais e tz ratavyrsegvtosgifagsgoaaapaf\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the sample function to generate predictions\n",
    "seed_ix = char_to_ix['p']  # Set the seed character index\n",
    "num_predictions = 1000  # Set the desired number of predictions\n",
    "predictions = sample(hprev, seed_ix, num_predictions)\n",
    "predicted_text = ''.join(ix_to_char[ix] for ix in predictions)\n",
    "print('Predicted text:\\n', predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "x[0] = np.zeros((vocab_size, 1))\n",
    "x[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
