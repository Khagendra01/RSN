{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read()\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_onehot(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_vocab = get_vocab('input.txt', lower = False)\n",
    "onehot = embed_to_onehot(text, text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "sequence_length = 12\n",
    "epoch = 1000\n",
    "num_layers = 2\n",
    "size_layer = 128\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "dimension = onehot.shape[1]\n",
    "epsilon = 1e-8\n",
    "\n",
    "U = np.random.randn(size_layer, dimension) / np.sqrt(size_layer)\n",
    "U_g = np.zeros(U.shape)\n",
    "Wf = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wf_g = np.zeros(Wf.shape)\n",
    "Wi = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wi_g = np.zeros(Wi.shape)\n",
    "Wc = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wc_g = np.zeros(Wc.shape)\n",
    "Wo = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wo_g = np.zeros(Wo.shape)\n",
    "V = np.random.randn(dimension, size_layer) / np.sqrt(dimension)\n",
    "V_g = np.zeros(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "def derivative_softmax_cross_entropy(x, y):\n",
    "    delta = softmax(x)\n",
    "    delta[range(X.shape[0]), y] -= 1\n",
    "    return delta\n",
    "\n",
    "def forward_multiply_gate(w, x):\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def forward_add_gate(x1, x2):\n",
    "    return x1 + x2\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat+1e-9))) / N\n",
    "\n",
    "def forward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V):\n",
    "    mul_u = forward_multiply_gate(x, U.T)\n",
    "    mul_Wf = forward_multiply_gate(h_state, Wf.T)\n",
    "    add_Wf = forward_add_gate(mul_u, mul_Wf)\n",
    "    f = sigmoid(add_Wf)\n",
    "    mul_Wi = forward_multiply_gate(h_state, Wi.T)\n",
    "    add_Wi = forward_add_gate(mul_u, mul_Wi)\n",
    "    i = sigmoid(add_Wi)\n",
    "    mul_Wc = forward_multiply_gate(h_state, Wc.T)\n",
    "    add_Wc = forward_add_gate(mul_u, mul_Wc)\n",
    "    c_hat = tanh(add_Wc)\n",
    "    C = c_state * f + i * c_hat\n",
    "    mul_Wo = forward_multiply_gate(h_state, Wo.T)\n",
    "    add_Wo = forward_add_gate(mul_u, mul_Wo)\n",
    "    o = sigmoid(add_Wo)\n",
    "    h = o * tanh(C)\n",
    "    mul_v = forward_multiply_gate(h, V.T)\n",
    "    return (mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat)\n",
    "\n",
    "def backward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dC = tanh(C, True) * o * dh.T\n",
    "    do = tanh(C) * dh.T\n",
    "    dadd_Wo = sigmoid(add_Wo, True) * do\n",
    "    dmul_u1, dmul_Wo = backward_add_gate(mul_u, mul_Wo, dadd_Wo)\n",
    "    dWo, dprev_state = backward_multiply_gate(Wo, h_state, dmul_Wo)\n",
    "    dc_hat = dC * i\n",
    "    dadd_Wc = tanh(add_Wc, True) * dc_hat\n",
    "    dmul_u2, dmul_Wc = backward_add_gate(mul_u, mul_Wc, dadd_Wc)\n",
    "    dWc, dprev_state = backward_multiply_gate(Wc, h_state, dmul_Wc)\n",
    "    di = dC * c_hat\n",
    "    dadd_Wi = sigmoid(add_Wi, True) * di\n",
    "    dmul_u3, dmul_Wi = backward_add_gate(mul_u, mul_Wi, dadd_Wi)\n",
    "    dWi, dprev_state = backward_multiply_gate(Wi, h_state, dmul_Wi)\n",
    "    df = dC * c_state\n",
    "    dadd_Wf = sigmoid(add_Wf, True) * df\n",
    "    dmul_u4, dmul_Wf = backward_add_gate(mul_u, mul_Wf, dadd_Wf)\n",
    "    dWf, dprev_state = backward_multiply_gate(Wf, h_state, dmul_Wf)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u4)\n",
    "    return (dU, dWf, dWi, dWc, dWo, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 3.051963, accuracy 0.151042\n",
      "epoch 100, loss 2.894775, accuracy 0.192708\n",
      "epoch 150, loss 2.793510, accuracy 0.210938\n",
      "epoch 200, loss 2.709813, accuracy 0.246094\n",
      "epoch 250, loss 2.655929, accuracy 0.205729\n",
      "epoch 300, loss 2.610033, accuracy 0.252604\n",
      "epoch 350, loss 2.605667, accuracy 0.251302\n",
      "epoch 400, loss 2.503175, accuracy 0.273438\n",
      "epoch 450, loss 2.497157, accuracy 0.277344\n",
      "epoch 500, loss 2.481875, accuracy 0.274740\n",
      "epoch 550, loss 2.431780, accuracy 0.290365\n",
      "epoch 600, loss 2.421123, accuracy 0.278646\n",
      "epoch 650, loss 2.452505, accuracy 0.290365\n",
      "epoch 700, loss 2.457527, accuracy 0.277344\n",
      "epoch 750, loss 2.481294, accuracy 0.264323\n",
      "epoch 800, loss 2.462105, accuracy 0.264323\n",
      "epoch 850, loss 2.401954, accuracy 0.283854\n",
      "epoch 900, loss 2.373719, accuracy 0.316406\n",
      "epoch 950, loss 2.375636, accuracy 0.268229\n",
      "epoch 1000, loss 2.395788, accuracy 0.300781\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:,n,:] = onehot[id1, :]\n",
    "        batch_y[:,n,:] = onehot[id2, :]\n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, Wc, Wo, V))\n",
    "        prev_c = layers[-1][7]\n",
    "        prev_h = layers[-1][10]\n",
    "        out_logits[:, n, :] = layers[-1][-4]\n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)),axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWf = np.zeros(Wf.shape)\n",
    "    dWi = np.zeros(Wi.shape)\n",
    "    dWc = np.zeros(Wc.shape)\n",
    "    dWo = np.zeros(Wo.shape)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWf_t, dWi_t, dWc_t, dWo_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, \n",
    "                                                                    Wc, Wo, V, d_mul_v, layers[n])\n",
    "        prev_c = layers[n][7]\n",
    "        prev_h = layers[n][10]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWf += dWf_t\n",
    "        dWi += dWi_t\n",
    "        dWc += dWc_t\n",
    "        dWo += dWo_t\n",
    "    U_g += dU ** 2\n",
    "    U += -learning_rate * dU / np.sqrt(U_g + epsilon)\n",
    "    V_g += dV ** 2\n",
    "    V += -learning_rate * dV / np.sqrt(V_g + epsilon)\n",
    "    Wf_g += dWf ** 2\n",
    "    Wf += -learning_rate * dWf / np.sqrt(Wf_g + epsilon)\n",
    "    Wi_g += dWi ** 2\n",
    "    Wi += -learning_rate * dWi / np.sqrt(Wi_g + epsilon)\n",
    "    Wc_g += dWc ** 2\n",
    "    Wc += -learning_rate * dWc / np.sqrt(Wc_g + epsilon)\n",
    "    Wo_g += dWo ** 2\n",
    "    Wo += -learning_rate * dWo / np.sqrt(Wo_g + epsilon)\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch %d, loss %f, accuracy %f'%(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.235248, accuracy 0.352865\n",
      "loss 2.267506, accuracy 0.315104\n",
      "loss 2.305853, accuracy 0.312500\n",
      "loss 2.283595, accuracy 0.329427\n",
      "loss 2.295583, accuracy 0.302083\n",
      "loss 2.232932, accuracy 0.324219\n",
      "loss 2.201061, accuracy 0.356771\n",
      "loss 2.313902, accuracy 0.320312\n",
      "loss 2.249889, accuracy 0.322917\n",
      "loss 2.235962, accuracy 0.335938\n",
      "loss 2.267121, accuracy 0.330729\n",
      "loss 2.280180, accuracy 0.321615\n",
      "loss 2.245110, accuracy 0.333333\n",
      "loss 2.188351, accuracy 0.351562\n",
      "loss 2.337424, accuracy 0.289062\n",
      "loss 2.234379, accuracy 0.299479\n",
      "loss 2.238207, accuracy 0.329427\n",
      "loss 2.242689, accuracy 0.319010\n",
      "loss 2.239369, accuracy 0.325521\n",
      "loss 2.316315, accuracy 0.305990\n",
      "loss 2.232966, accuracy 0.325521\n",
      "loss 2.272183, accuracy 0.305990\n",
      "loss 2.213686, accuracy 0.342448\n",
      "loss 2.219568, accuracy 0.308594\n",
      "loss 2.214912, accuracy 0.324219\n",
      "loss 2.321370, accuracy 0.312500\n",
      "loss 2.201151, accuracy 0.329427\n",
      "loss 2.189939, accuracy 0.339844\n",
      "loss 2.291823, accuracy 0.312500\n",
      "loss 2.247198, accuracy 0.329427\n",
      "loss 2.277403, accuracy 0.302083\n",
      "loss 2.300860, accuracy 0.312500\n",
      "loss 2.283928, accuracy 0.305990\n",
      "loss 2.355998, accuracy 0.313802\n",
      "loss 2.284512, accuracy 0.335938\n",
      "loss 2.271770, accuracy 0.326823\n",
      "loss 2.270557, accuracy 0.335938\n",
      "loss 2.274933, accuracy 0.311198\n",
      "loss 2.176194, accuracy 0.320312\n",
      "loss 2.200479, accuracy 0.338542\n",
      "loss 2.200496, accuracy 0.333333\n",
      "loss 2.266912, accuracy 0.308594\n",
      "loss 2.205424, accuracy 0.360677\n",
      "loss 2.231974, accuracy 0.326823\n",
      "loss 2.285010, accuracy 0.328125\n",
      "loss 2.312552, accuracy 0.317708\n",
      "loss 2.270425, accuracy 0.329427\n",
      "loss 2.294218, accuracy 0.326823\n",
      "loss 2.289105, accuracy 0.321615\n",
      "loss 2.227865, accuracy 0.338542\n",
      "loss 2.309509, accuracy 0.324219\n",
      "loss 2.286981, accuracy 0.312500\n",
      "loss 2.327068, accuracy 0.291667\n",
      "loss 2.229519, accuracy 0.300781\n",
      "loss 2.333675, accuracy 0.307292\n",
      "loss 2.335630, accuracy 0.304688\n",
      "loss 2.361444, accuracy 0.303385\n",
      "loss 2.245945, accuracy 0.330729\n",
      "loss 2.235746, accuracy 0.292969\n",
      "loss 2.240905, accuracy 0.325521\n",
      "loss 2.207663, accuracy 0.346354\n",
      "loss 2.236773, accuracy 0.330729\n",
      "loss 2.228804, accuracy 0.320312\n",
      "loss 2.253058, accuracy 0.332031\n",
      "loss 2.333388, accuracy 0.333333\n",
      "loss 2.304595, accuracy 0.311198\n",
      "loss 2.242449, accuracy 0.332031\n",
      "loss 2.229642, accuracy 0.326823\n",
      "loss 2.218802, accuracy 0.360677\n",
      "loss 2.277766, accuracy 0.311198\n",
      "loss 2.281449, accuracy 0.308594\n",
      "loss 2.303040, accuracy 0.313802\n",
      "loss 2.180130, accuracy 0.369792\n",
      "loss 2.281678, accuracy 0.305990\n",
      "loss 2.387841, accuracy 0.281250\n",
      "loss 2.253875, accuracy 0.338542\n",
      "loss 2.271507, accuracy 0.315104\n",
      "loss 2.251899, accuracy 0.321615\n",
      "loss 2.255331, accuracy 0.308594\n",
      "loss 2.164517, accuracy 0.319010\n",
      "loss 2.273066, accuracy 0.350260\n",
      "loss 2.289082, accuracy 0.330729\n",
      "loss 2.252307, accuracy 0.333333\n",
      "loss 2.273379, accuracy 0.316406\n",
      "loss 2.319291, accuracy 0.312500\n",
      "loss 2.303932, accuracy 0.307292\n",
      "loss 2.311870, accuracy 0.299479\n",
      "loss 2.192702, accuracy 0.348958\n",
      "loss 2.236902, accuracy 0.320312\n",
      "loss 2.300388, accuracy 0.309896\n",
      "loss 2.242041, accuracy 0.325521\n",
      "loss 2.252442, accuracy 0.337240\n",
      "loss 2.315403, accuracy 0.320312\n",
      "loss 2.278269, accuracy 0.341146\n",
      "loss 2.247448, accuracy 0.312500\n",
      "loss 2.284699, accuracy 0.322917\n",
      "loss 2.207682, accuracy 0.320312\n",
      "loss 2.252992, accuracy 0.311198\n",
      "loss 2.226247, accuracy 0.333333\n",
      "loss 2.220396, accuracy 0.319010\n",
      "loss 2.277490, accuracy 0.302083\n",
      "loss 2.232462, accuracy 0.307292\n",
      "loss 2.254598, accuracy 0.333333\n",
      "loss 2.239012, accuracy 0.334635\n",
      "loss 2.271640, accuracy 0.316406\n",
      "loss 2.181159, accuracy 0.375000\n",
      "loss 2.240668, accuracy 0.317708\n",
      "loss 2.253331, accuracy 0.329427\n",
      "loss 2.230696, accuracy 0.330729\n",
      "loss 2.323211, accuracy 0.304688\n",
      "loss 2.268538, accuracy 0.312500\n",
      "loss 2.285785, accuracy 0.311198\n",
      "loss 2.205205, accuracy 0.341146\n",
      "loss 2.205557, accuracy 0.341146\n",
      "loss 2.281726, accuracy 0.328125\n",
      "loss 2.263733, accuracy 0.317708\n",
      "loss 2.287162, accuracy 0.316406\n",
      "loss 2.207496, accuracy 0.328125\n",
      "loss 2.308116, accuracy 0.328125\n",
      "loss 2.252890, accuracy 0.333333\n",
      "loss 2.268188, accuracy 0.312500\n",
      "loss 2.226757, accuracy 0.354167\n",
      "loss 2.252405, accuracy 0.328125\n",
      "loss 2.238113, accuracy 0.335938\n",
      "loss 2.298775, accuracy 0.303385\n",
      "loss 2.304531, accuracy 0.300781\n",
      "loss 2.218888, accuracy 0.324219\n",
      "loss 2.288435, accuracy 0.311198\n",
      "loss 2.210479, accuracy 0.342448\n",
      "loss 2.298436, accuracy 0.316406\n",
      "loss 2.232725, accuracy 0.305990\n",
      "loss 2.283988, accuracy 0.322917\n",
      "loss 2.273608, accuracy 0.320312\n",
      "loss 2.247812, accuracy 0.328125\n",
      "loss 2.278032, accuracy 0.335938\n",
      "loss 2.255060, accuracy 0.330729\n",
      "loss 2.196718, accuracy 0.337240\n",
      "loss 2.243596, accuracy 0.338542\n",
      "loss 2.284306, accuracy 0.311198\n",
      "loss 2.239888, accuracy 0.324219\n",
      "loss 2.284092, accuracy 0.300781\n",
      "loss 2.253978, accuracy 0.319010\n",
      "loss 2.182564, accuracy 0.341146\n",
      "loss 2.255118, accuracy 0.298177\n",
      "loss 2.182613, accuracy 0.346354\n",
      "loss 2.281317, accuracy 0.300781\n",
      "loss 2.293322, accuracy 0.307292\n",
      "loss 2.143173, accuracy 0.352865\n",
      "loss 2.246560, accuracy 0.302083\n",
      "loss 2.312683, accuracy 0.322917\n",
      "loss 2.203779, accuracy 0.363281\n",
      "loss 2.261872, accuracy 0.296875\n",
      "loss 2.226198, accuracy 0.332031\n",
      "loss 2.222587, accuracy 0.329427\n",
      "loss 2.244943, accuracy 0.335938\n",
      "loss 2.318385, accuracy 0.292969\n",
      "loss 2.220120, accuracy 0.325521\n",
      "loss 2.249905, accuracy 0.324219\n",
      "loss 2.251107, accuracy 0.325521\n",
      "loss 2.203623, accuracy 0.335938\n",
      "loss 2.213843, accuracy 0.338542\n",
      "loss 2.280761, accuracy 0.320312\n",
      "loss 2.241338, accuracy 0.343750\n",
      "loss 2.186877, accuracy 0.358073\n",
      "loss 2.245625, accuracy 0.309896\n",
      "loss 2.298046, accuracy 0.320312\n",
      "loss 2.228392, accuracy 0.333333\n",
      "loss 2.252586, accuracy 0.346354\n",
      "loss 2.309850, accuracy 0.307292\n",
      "loss 2.299459, accuracy 0.295573\n",
      "loss 2.314441, accuracy 0.342448\n",
      "loss 2.313620, accuracy 0.303385\n",
      "loss 2.205008, accuracy 0.355469\n",
      "loss 2.242528, accuracy 0.338542\n",
      "loss 2.255394, accuracy 0.311198\n",
      "loss 2.225191, accuracy 0.352865\n",
      "loss 2.284691, accuracy 0.299479\n",
      "loss 2.248864, accuracy 0.334635\n",
      "loss 2.218536, accuracy 0.330729\n",
      "loss 2.245995, accuracy 0.322917\n",
      "loss 2.270739, accuracy 0.329427\n",
      "loss 2.240203, accuracy 0.337240\n",
      "loss 2.314389, accuracy 0.303385\n",
      "loss 2.295913, accuracy 0.322917\n",
      "loss 2.197036, accuracy 0.339844\n",
      "loss 2.174009, accuracy 0.330729\n",
      "loss 2.282086, accuracy 0.308594\n",
      "loss 2.227463, accuracy 0.338542\n",
      "loss 2.270440, accuracy 0.311198\n",
      "loss 2.265059, accuracy 0.346354\n",
      "loss 2.188657, accuracy 0.338542\n",
      "loss 2.255691, accuracy 0.334635\n",
      "loss 2.273277, accuracy 0.322917\n",
      "loss 2.278866, accuracy 0.339844\n",
      "loss 2.210536, accuracy 0.322917\n",
      "loss 2.292913, accuracy 0.309896\n",
      "loss 2.274941, accuracy 0.329427\n",
      "loss 2.237616, accuracy 0.329427\n",
      "loss 2.220514, accuracy 0.317708\n",
      "loss 2.169881, accuracy 0.341146\n",
      "loss 2.230355, accuracy 0.324219\n",
      "loss 2.267239, accuracy 0.295573\n",
      "loss 2.190884, accuracy 0.360677\n",
      "loss 2.260767, accuracy 0.330729\n",
      "loss 2.239000, accuracy 0.319010\n",
      "loss 2.248906, accuracy 0.337240\n",
      "loss 2.287547, accuracy 0.339844\n",
      "loss 2.235923, accuracy 0.324219\n",
      "loss 2.197108, accuracy 0.348958\n",
      "loss 2.222556, accuracy 0.315104\n",
      "loss 2.207039, accuracy 0.334635\n",
      "loss 2.193016, accuracy 0.358073\n",
      "loss 2.194577, accuracy 0.352865\n",
      "loss 2.261285, accuracy 0.326823\n",
      "loss 2.265815, accuracy 0.317708\n",
      "loss 2.278892, accuracy 0.317708\n",
      "loss 2.310750, accuracy 0.324219\n",
      "loss 2.195152, accuracy 0.326823\n",
      "loss 2.221275, accuracy 0.358073\n",
      "loss 2.253506, accuracy 0.311198\n",
      "loss 2.271009, accuracy 0.300781\n",
      "loss 2.225944, accuracy 0.351562\n",
      "loss 2.227256, accuracy 0.304688\n",
      "loss 2.244569, accuracy 0.322917\n",
      "loss 2.202512, accuracy 0.352865\n",
      "loss 2.295666, accuracy 0.322917\n",
      "loss 2.219081, accuracy 0.330729\n",
      "loss 2.229541, accuracy 0.350260\n",
      "loss 2.205021, accuracy 0.346354\n",
      "loss 2.250192, accuracy 0.308594\n",
      "loss 2.251181, accuracy 0.298177\n",
      "loss 2.240271, accuracy 0.328125\n",
      "loss 2.207986, accuracy 0.333333\n",
      "loss 2.261990, accuracy 0.330729\n",
      "loss 2.278646, accuracy 0.319010\n",
      "loss 2.191825, accuracy 0.365885\n",
      "loss 2.197438, accuracy 0.328125\n",
      "loss 2.203316, accuracy 0.339844\n",
      "loss 2.295206, accuracy 0.308594\n",
      "loss 2.200931, accuracy 0.346354\n",
      "loss 2.279677, accuracy 0.291667\n",
      "loss 2.251337, accuracy 0.303385\n",
      "loss 2.240300, accuracy 0.334635\n",
      "loss 2.202553, accuracy 0.348958\n",
      "loss 2.202570, accuracy 0.332031\n",
      "loss 2.264959, accuracy 0.341146\n",
      "loss 2.173633, accuracy 0.332031\n",
      "loss 2.322488, accuracy 0.291667\n",
      "loss 2.179697, accuracy 0.345052\n",
      "loss 2.259176, accuracy 0.321615\n",
      "loss 2.220213, accuracy 0.332031\n",
      "loss 2.180268, accuracy 0.360677\n",
      "loss 2.279999, accuracy 0.305990\n",
      "loss 2.236442, accuracy 0.326823\n",
      "loss 2.264101, accuracy 0.338542\n",
      "loss 2.231152, accuracy 0.343750\n",
      "loss 2.179896, accuracy 0.355469\n",
      "loss 2.298758, accuracy 0.304688\n",
      "loss 2.242775, accuracy 0.326823\n",
      "loss 2.267416, accuracy 0.312500\n",
      "loss 2.225983, accuracy 0.335938\n",
      "loss 2.222100, accuracy 0.351562\n",
      "loss 2.265934, accuracy 0.329427\n",
      "loss 2.274228, accuracy 0.313802\n",
      "loss 2.251781, accuracy 0.305990\n",
      "loss 2.244912, accuracy 0.319010\n",
      "loss 2.240538, accuracy 0.330729\n",
      "loss 2.333399, accuracy 0.259115\n",
      "loss 2.268110, accuracy 0.313802\n",
      "loss 2.249580, accuracy 0.334635\n",
      "loss 2.213055, accuracy 0.317708\n",
      "loss 2.214288, accuracy 0.333333\n",
      "loss 2.239327, accuracy 0.335938\n",
      "loss 2.193100, accuracy 0.346354\n",
      "loss 2.233441, accuracy 0.334635\n",
      "loss 2.200557, accuracy 0.332031\n",
      "loss 2.225095, accuracy 0.348958\n",
      "loss 2.211358, accuracy 0.342448\n",
      "loss 2.277465, accuracy 0.304688\n",
      "loss 2.179590, accuracy 0.377604\n",
      "loss 2.333058, accuracy 0.296875\n",
      "loss 2.216024, accuracy 0.326823\n",
      "loss 2.218094, accuracy 0.346354\n",
      "loss 2.207774, accuracy 0.337240\n",
      "loss 2.271194, accuracy 0.325521\n",
      "loss 2.219689, accuracy 0.345052\n",
      "loss 2.201262, accuracy 0.328125\n",
      "loss 2.223936, accuracy 0.358073\n",
      "loss 2.202281, accuracy 0.325521\n",
      "loss 2.253930, accuracy 0.334635\n",
      "loss 2.158873, accuracy 0.389323\n",
      "loss 2.212209, accuracy 0.352865\n",
      "loss 2.162310, accuracy 0.347656\n",
      "loss 2.278707, accuracy 0.313802\n",
      "loss 2.290104, accuracy 0.322917\n",
      "loss 2.212209, accuracy 0.337240\n",
      "loss 2.307040, accuracy 0.326823\n",
      "loss 2.209516, accuracy 0.348958\n",
      "loss 2.288372, accuracy 0.299479\n",
      "loss 2.242185, accuracy 0.325521\n",
      "loss 2.177008, accuracy 0.335938\n",
      "loss 2.223237, accuracy 0.352865\n",
      "loss 2.234609, accuracy 0.346354\n",
      "loss 2.205226, accuracy 0.351562\n",
      "loss 2.240129, accuracy 0.333333\n",
      "loss 2.205269, accuracy 0.341146\n",
      "loss 2.213302, accuracy 0.333333\n",
      "loss 2.239055, accuracy 0.339844\n",
      "loss 2.234122, accuracy 0.324219\n",
      "loss 2.263608, accuracy 0.345052\n",
      "loss 2.309627, accuracy 0.298177\n",
      "loss 2.163520, accuracy 0.361979\n",
      "loss 2.294188, accuracy 0.329427\n",
      "loss 2.186895, accuracy 0.347656\n",
      "loss 2.269744, accuracy 0.312500\n",
      "loss 2.219392, accuracy 0.324219\n",
      "loss 2.262868, accuracy 0.324219\n",
      "loss 2.266469, accuracy 0.313802\n",
      "loss 2.299198, accuracy 0.330729\n",
      "loss 2.205569, accuracy 0.339844\n",
      "loss 2.235126, accuracy 0.330729\n",
      "loss 2.312769, accuracy 0.287760\n",
      "loss 2.182243, accuracy 0.351562\n",
      "loss 2.183570, accuracy 0.346354\n",
      "loss 2.211902, accuracy 0.332031\n",
      "loss 2.232610, accuracy 0.332031\n",
      "loss 2.230024, accuracy 0.317708\n",
      "loss 2.235290, accuracy 0.335938\n",
      "loss 2.283585, accuracy 0.319010\n",
      "loss 2.190634, accuracy 0.332031\n",
      "loss 2.214394, accuracy 0.335938\n",
      "loss 2.262305, accuracy 0.339844\n",
      "loss 2.238996, accuracy 0.321615\n",
      "loss 2.239997, accuracy 0.354167\n",
      "loss 2.218096, accuracy 0.320312\n",
      "loss 2.190199, accuracy 0.358073\n",
      "loss 2.233136, accuracy 0.352865\n",
      "loss 2.232684, accuracy 0.325521\n",
      "loss 2.217300, accuracy 0.330729\n",
      "loss 2.197052, accuracy 0.325521\n",
      "loss 2.253830, accuracy 0.325521\n",
      "loss 2.333372, accuracy 0.311198\n",
      "loss 2.255895, accuracy 0.342448\n",
      "loss 2.261765, accuracy 0.326823\n",
      "loss 2.180458, accuracy 0.338542\n",
      "loss 2.233062, accuracy 0.325521\n",
      "loss 2.257791, accuracy 0.330729\n",
      "loss 2.256703, accuracy 0.341146\n",
      "loss 2.185265, accuracy 0.342448\n",
      "loss 2.167603, accuracy 0.360677\n",
      "loss 2.213726, accuracy 0.343750\n",
      "loss 2.178101, accuracy 0.369792\n",
      "loss 2.150296, accuracy 0.361979\n",
      "loss 2.275775, accuracy 0.320312\n",
      "loss 2.296211, accuracy 0.299479\n",
      "loss 2.202104, accuracy 0.326823\n",
      "loss 2.268398, accuracy 0.307292\n",
      "loss 2.281053, accuracy 0.317708\n",
      "loss 2.242859, accuracy 0.324219\n",
      "loss 2.281829, accuracy 0.315104\n",
      "loss 2.269410, accuracy 0.321615\n",
      "loss 2.145974, accuracy 0.352865\n",
      "loss 2.208998, accuracy 0.342448\n",
      "loss 2.210272, accuracy 0.345052\n",
      "loss 2.194683, accuracy 0.348958\n",
      "loss 2.259260, accuracy 0.325521\n",
      "loss 2.255681, accuracy 0.320312\n",
      "loss 2.239323, accuracy 0.341146\n",
      "loss 2.240764, accuracy 0.324219\n",
      "loss 2.361673, accuracy 0.298177\n",
      "loss 2.194031, accuracy 0.316406\n",
      "loss 2.202403, accuracy 0.334635\n",
      "loss 2.143895, accuracy 0.326823\n",
      "loss 2.330828, accuracy 0.309896\n",
      "loss 2.182346, accuracy 0.350260\n",
      "loss 2.266241, accuracy 0.321615\n",
      "loss 2.154383, accuracy 0.337240\n",
      "loss 2.165313, accuracy 0.337240\n",
      "loss 2.242324, accuracy 0.298177\n",
      "loss 2.276202, accuracy 0.300781\n",
      "loss 2.186090, accuracy 0.345052\n",
      "loss 2.242077, accuracy 0.322917\n",
      "loss 2.301523, accuracy 0.299479\n",
      "loss 2.272676, accuracy 0.305990\n",
      "loss 2.142998, accuracy 0.341146\n",
      "loss 2.198977, accuracy 0.350260\n",
      "loss 2.260779, accuracy 0.325521\n",
      "loss 2.210663, accuracy 0.342448\n",
      "loss 2.285533, accuracy 0.307292\n",
      "loss 2.211271, accuracy 0.324219\n",
      "loss 2.204623, accuracy 0.329427\n",
      "loss 2.267639, accuracy 0.302083\n",
      "loss 2.298082, accuracy 0.304688\n",
      "loss 2.178311, accuracy 0.335938\n",
      "loss 2.229631, accuracy 0.312500\n",
      "loss 2.173078, accuracy 0.332031\n",
      "loss 2.264526, accuracy 0.319010\n",
      "loss 2.240279, accuracy 0.324219\n",
      "loss 2.382037, accuracy 0.295573\n",
      "loss 2.247484, accuracy 0.311198\n",
      "loss 2.245519, accuracy 0.315104\n",
      "loss 2.208108, accuracy 0.346354\n",
      "loss 2.275097, accuracy 0.307292\n",
      "loss 2.206698, accuracy 0.358073\n",
      "loss 2.185164, accuracy 0.359375\n",
      "loss 2.182592, accuracy 0.325521\n",
      "loss 2.261985, accuracy 0.296875\n",
      "loss 2.273789, accuracy 0.342448\n",
      "loss 2.265918, accuracy 0.319010\n",
      "loss 2.173160, accuracy 0.354167\n",
      "loss 2.169507, accuracy 0.337240\n",
      "loss 2.170551, accuracy 0.333333\n",
      "loss 2.219329, accuracy 0.342448\n",
      "loss 2.209355, accuracy 0.315104\n",
      "loss 2.217143, accuracy 0.339844\n",
      "loss 2.217289, accuracy 0.337240\n",
      "loss 2.202504, accuracy 0.335938\n",
      "loss 2.223735, accuracy 0.322917\n",
      "loss 2.173679, accuracy 0.307292\n",
      "loss 2.187999, accuracy 0.345052\n",
      "loss 2.187639, accuracy 0.355469\n",
      "loss 2.243024, accuracy 0.322917\n",
      "loss 2.166583, accuracy 0.341146\n",
      "loss 2.208253, accuracy 0.345052\n",
      "loss 2.238258, accuracy 0.343750\n",
      "loss 2.243933, accuracy 0.341146\n",
      "loss 2.262966, accuracy 0.319010\n",
      "loss 2.215805, accuracy 0.332031\n",
      "loss 2.210253, accuracy 0.330729\n",
      "loss 2.174542, accuracy 0.348958\n",
      "loss 2.218644, accuracy 0.326823\n",
      "loss 2.221329, accuracy 0.330729\n",
      "loss 2.130728, accuracy 0.348958\n",
      "loss 2.211878, accuracy 0.330729\n",
      "loss 2.229325, accuracy 0.346354\n",
      "loss 2.243293, accuracy 0.321615\n",
      "loss 2.207014, accuracy 0.335938\n",
      "loss 2.193891, accuracy 0.355469\n",
      "loss 2.240352, accuracy 0.337240\n",
      "loss 2.273684, accuracy 0.326823\n",
      "loss 2.264379, accuracy 0.328125\n",
      "loss 2.089703, accuracy 0.377604\n",
      "loss 2.183799, accuracy 0.343750\n",
      "loss 2.211237, accuracy 0.326823\n",
      "loss 2.187667, accuracy 0.317708\n",
      "loss 2.290722, accuracy 0.313802\n",
      "loss 2.128162, accuracy 0.372396\n",
      "loss 2.209289, accuracy 0.326823\n",
      "loss 2.209639, accuracy 0.335938\n",
      "loss 2.224491, accuracy 0.329427\n",
      "loss 2.286472, accuracy 0.300781\n",
      "loss 2.192604, accuracy 0.381510\n",
      "loss 2.210618, accuracy 0.338542\n",
      "loss 2.216981, accuracy 0.342448\n",
      "loss 2.209388, accuracy 0.352865\n",
      "loss 2.197325, accuracy 0.332031\n",
      "loss 2.163518, accuracy 0.360677\n",
      "loss 2.220939, accuracy 0.335938\n",
      "loss 2.269075, accuracy 0.307292\n",
      "loss 2.219342, accuracy 0.333333\n",
      "loss 2.273161, accuracy 0.334635\n",
      "loss 2.252221, accuracy 0.332031\n",
      "loss 2.197312, accuracy 0.334635\n",
      "loss 2.292414, accuracy 0.311198\n",
      "loss 2.231110, accuracy 0.326823\n",
      "loss 2.190812, accuracy 0.345052\n",
      "loss 2.258697, accuracy 0.315104\n",
      "loss 2.205826, accuracy 0.350260\n",
      "loss 2.235439, accuracy 0.351562\n",
      "loss 2.297866, accuracy 0.309896\n",
      "loss 2.263740, accuracy 0.317708\n",
      "loss 2.267722, accuracy 0.308594\n",
      "loss 2.233427, accuracy 0.309896\n",
      "loss 2.188665, accuracy 0.330729\n",
      "loss 2.302323, accuracy 0.308594\n",
      "loss 2.306239, accuracy 0.308594\n",
      "loss 2.257372, accuracy 0.324219\n",
      "loss 2.211714, accuracy 0.326823\n",
      "loss 2.245149, accuracy 0.315104\n",
      "loss 2.178863, accuracy 0.348958\n",
      "loss 2.233922, accuracy 0.326823\n",
      "loss 2.236790, accuracy 0.316406\n",
      "loss 2.264886, accuracy 0.311198\n",
      "loss 2.224673, accuracy 0.334635\n",
      "loss 2.250700, accuracy 0.339844\n",
      "loss 2.268879, accuracy 0.322917\n",
      "loss 2.185982, accuracy 0.325521\n",
      "loss 2.238207, accuracy 0.324219\n",
      "loss 2.228565, accuracy 0.309896\n",
      "loss 2.198820, accuracy 0.343750\n",
      "loss 2.221766, accuracy 0.339844\n",
      "loss 2.251379, accuracy 0.315104\n",
      "loss 2.211312, accuracy 0.351562\n",
      "loss 2.198551, accuracy 0.342448\n",
      "loss 2.153478, accuracy 0.332031\n",
      "loss 2.265772, accuracy 0.300781\n",
      "loss 2.213219, accuracy 0.339844\n",
      "loss 2.265233, accuracy 0.322917\n",
      "loss 2.203950, accuracy 0.345052\n",
      "loss 2.188749, accuracy 0.348958\n",
      "loss 2.290659, accuracy 0.298177\n",
      "loss 2.231756, accuracy 0.334635\n",
      "loss 2.260280, accuracy 0.291667\n",
      "loss 2.333347, accuracy 0.290365\n",
      "loss 2.232488, accuracy 0.355469\n",
      "loss 2.172173, accuracy 0.346354\n",
      "loss 2.244135, accuracy 0.342448\n",
      "loss 2.285291, accuracy 0.313802\n",
      "loss 2.259760, accuracy 0.347656\n",
      "loss 2.261323, accuracy 0.309896\n",
      "loss 2.194916, accuracy 0.342448\n",
      "loss 2.228613, accuracy 0.334635\n",
      "loss 2.195244, accuracy 0.337240\n",
      "loss 2.218599, accuracy 0.342448\n",
      "loss 2.287972, accuracy 0.321615\n",
      "loss 2.246458, accuracy 0.333333\n",
      "loss 2.190190, accuracy 0.364583\n",
      "loss 2.313086, accuracy 0.305990\n",
      "loss 2.217299, accuracy 0.339844\n",
      "loss 2.205433, accuracy 0.347656\n",
      "loss 2.250698, accuracy 0.335938\n",
      "loss 2.219792, accuracy 0.356771\n",
      "loss 2.182359, accuracy 0.338542\n",
      "loss 2.211146, accuracy 0.325521\n",
      "loss 2.192901, accuracy 0.351562\n",
      "loss 2.213621, accuracy 0.333333\n",
      "loss 2.182861, accuracy 0.338542\n",
      "loss 2.225524, accuracy 0.341146\n",
      "loss 2.183620, accuracy 0.319010\n",
      "loss 2.279555, accuracy 0.300781\n",
      "loss 2.191641, accuracy 0.334635\n",
      "loss 2.209559, accuracy 0.350260\n",
      "loss 2.201371, accuracy 0.329427\n",
      "loss 2.250877, accuracy 0.300781\n",
      "loss 2.202576, accuracy 0.365885\n",
      "loss 2.175526, accuracy 0.351562\n",
      "loss 2.210032, accuracy 0.356771\n",
      "loss 2.194320, accuracy 0.320312\n",
      "loss 2.187535, accuracy 0.329427\n",
      "loss 2.240608, accuracy 0.341146\n",
      "loss 2.235417, accuracy 0.329427\n",
      "loss 2.184406, accuracy 0.346354\n",
      "loss 2.255094, accuracy 0.332031\n",
      "loss 2.197056, accuracy 0.329427\n",
      "loss 2.213186, accuracy 0.330729\n",
      "loss 2.206865, accuracy 0.337240\n",
      "loss 2.265881, accuracy 0.312500\n",
      "loss 2.212496, accuracy 0.329427\n",
      "loss 2.238297, accuracy 0.316406\n",
      "loss 2.194198, accuracy 0.363281\n",
      "loss 2.166657, accuracy 0.341146\n",
      "loss 2.193304, accuracy 0.347656\n",
      "loss 2.151017, accuracy 0.347656\n",
      "loss 2.213861, accuracy 0.322917\n",
      "loss 2.192204, accuracy 0.350260\n",
      "loss 2.223788, accuracy 0.335938\n",
      "loss 2.248962, accuracy 0.307292\n",
      "loss 2.186285, accuracy 0.325521\n",
      "loss 2.245617, accuracy 0.321615\n",
      "loss 2.224581, accuracy 0.352865\n",
      "loss 2.245016, accuracy 0.334635\n",
      "loss 2.209366, accuracy 0.341146\n",
      "loss 2.216815, accuracy 0.319010\n",
      "loss 2.212309, accuracy 0.348958\n",
      "loss 2.228953, accuracy 0.316406\n",
      "loss 2.223922, accuracy 0.343750\n",
      "loss 2.230588, accuracy 0.325521\n",
      "loss 2.275750, accuracy 0.300781\n",
      "loss 2.195228, accuracy 0.339844\n",
      "loss 2.122241, accuracy 0.356771\n",
      "loss 2.198862, accuracy 0.351562\n",
      "loss 2.166152, accuracy 0.343750\n",
      "loss 2.280148, accuracy 0.313802\n",
      "loss 2.197713, accuracy 0.321615\n",
      "loss 2.201396, accuracy 0.352865\n",
      "loss 2.205528, accuracy 0.321615\n",
      "loss 2.265308, accuracy 0.322917\n",
      "loss 2.259268, accuracy 0.320312\n",
      "loss 2.224956, accuracy 0.335938\n",
      "loss 2.210356, accuracy 0.332031\n",
      "loss 2.222789, accuracy 0.325521\n",
      "loss 2.276970, accuracy 0.334635\n",
      "loss 2.228934, accuracy 0.324219\n",
      "loss 2.233864, accuracy 0.325521\n",
      "loss 2.178440, accuracy 0.343750\n",
      "loss 2.144685, accuracy 0.371094\n",
      "loss 2.186126, accuracy 0.339844\n",
      "loss 2.274653, accuracy 0.317708\n",
      "loss 2.224464, accuracy 0.350260\n",
      "loss 2.228734, accuracy 0.338542\n",
      "loss 2.303469, accuracy 0.339844\n",
      "loss 2.287741, accuracy 0.328125\n",
      "loss 2.163265, accuracy 0.339844\n",
      "loss 2.231063, accuracy 0.328125\n",
      "loss 2.175879, accuracy 0.360677\n",
      "loss 2.208529, accuracy 0.324219\n",
      "loss 2.241135, accuracy 0.350260\n",
      "loss 2.207325, accuracy 0.298177\n",
      "loss 2.264312, accuracy 0.330729\n",
      "loss 2.238551, accuracy 0.324219\n",
      "loss 2.235820, accuracy 0.334635\n",
      "loss 2.227654, accuracy 0.347656\n",
      "loss 2.285005, accuracy 0.316406\n",
      "loss 2.205047, accuracy 0.355469\n",
      "loss 2.342769, accuracy 0.279948\n",
      "loss 2.139607, accuracy 0.363281\n",
      "loss 2.242705, accuracy 0.329427\n",
      "loss 2.250992, accuracy 0.302083\n",
      "loss 2.254179, accuracy 0.339844\n",
      "loss 2.146138, accuracy 0.381510\n",
      "loss 2.193106, accuracy 0.328125\n",
      "loss 2.201768, accuracy 0.342448\n",
      "loss 2.165958, accuracy 0.335938\n",
      "loss 2.291109, accuracy 0.328125\n",
      "loss 2.194937, accuracy 0.342448\n",
      "loss 2.223810, accuracy 0.321615\n",
      "loss 2.186300, accuracy 0.341146\n",
      "loss 2.315563, accuracy 0.291667\n",
      "loss 2.189305, accuracy 0.332031\n",
      "loss 2.206928, accuracy 0.334635\n",
      "loss 2.227025, accuracy 0.326823\n",
      "loss 2.227204, accuracy 0.316406\n",
      "loss 2.223025, accuracy 0.325521\n",
      "loss 2.166564, accuracy 0.351562\n",
      "loss 2.211694, accuracy 0.359375\n",
      "loss 2.216140, accuracy 0.337240\n",
      "loss 2.222193, accuracy 0.333333\n",
      "loss 2.193274, accuracy 0.330729\n",
      "loss 2.208811, accuracy 0.330729\n",
      "loss 2.285984, accuracy 0.326823\n",
      "loss 2.167139, accuracy 0.360677\n",
      "loss 2.154353, accuracy 0.345052\n",
      "loss 2.249361, accuracy 0.325521\n",
      "loss 2.224395, accuracy 0.335938\n",
      "loss 2.257598, accuracy 0.334635\n",
      "loss 2.180634, accuracy 0.373698\n",
      "loss 2.146206, accuracy 0.360677\n",
      "loss 2.152173, accuracy 0.358073\n",
      "loss 2.241570, accuracy 0.311198\n",
      "loss 2.206916, accuracy 0.341146\n",
      "loss 2.284286, accuracy 0.316406\n",
      "loss 2.224720, accuracy 0.337240\n",
      "loss 2.155470, accuracy 0.338542\n",
      "loss 2.214553, accuracy 0.311198\n",
      "loss 2.208896, accuracy 0.321615\n",
      "loss 2.144060, accuracy 0.339844\n",
      "loss 2.205911, accuracy 0.335938\n",
      "loss 2.206540, accuracy 0.329427\n",
      "loss 2.204680, accuracy 0.333333\n",
      "loss 2.230647, accuracy 0.308594\n",
      "loss 2.176955, accuracy 0.338542\n",
      "loss 2.263785, accuracy 0.307292\n",
      "loss 2.293326, accuracy 0.303385\n",
      "loss 2.150741, accuracy 0.338542\n",
      "loss 2.207727, accuracy 0.346354\n",
      "loss 2.191537, accuracy 0.328125\n",
      "loss 2.167839, accuracy 0.339844\n",
      "loss 2.170517, accuracy 0.350260\n",
      "loss 2.300803, accuracy 0.326823\n",
      "loss 2.151678, accuracy 0.345052\n",
      "loss 2.151235, accuracy 0.350260\n",
      "loss 2.255892, accuracy 0.320312\n",
      "loss 2.285493, accuracy 0.305990\n",
      "loss 2.256174, accuracy 0.319010\n",
      "loss 2.232077, accuracy 0.322917\n",
      "loss 2.197723, accuracy 0.351562\n",
      "loss 2.190003, accuracy 0.371094\n",
      "loss 2.202006, accuracy 0.337240\n",
      "loss 2.222337, accuracy 0.351562\n",
      "loss 2.195319, accuracy 0.324219\n",
      "loss 2.238429, accuracy 0.345052\n",
      "loss 2.253771, accuracy 0.326823\n",
      "loss 2.214902, accuracy 0.320312\n",
      "loss 2.261513, accuracy 0.334635\n",
      "loss 2.258655, accuracy 0.321615\n",
      "loss 2.174164, accuracy 0.342448\n",
      "loss 2.160887, accuracy 0.354167\n",
      "loss 2.190623, accuracy 0.345052\n",
      "loss 2.262765, accuracy 0.307292\n",
      "loss 2.321774, accuracy 0.309896\n",
      "loss 2.151801, accuracy 0.345052\n",
      "loss 2.202652, accuracy 0.326823\n",
      "loss 2.245823, accuracy 0.324219\n",
      "loss 2.278851, accuracy 0.287760\n",
      "loss 2.138057, accuracy 0.334635\n",
      "loss 2.249765, accuracy 0.335938\n",
      "loss 2.158089, accuracy 0.330729\n",
      "loss 2.228381, accuracy 0.339844\n",
      "loss 2.201360, accuracy 0.345052\n",
      "loss 2.229308, accuracy 0.312500\n",
      "loss 2.213143, accuracy 0.337240\n",
      "loss 2.227766, accuracy 0.337240\n",
      "loss 2.267718, accuracy 0.315104\n",
      "loss 2.228734, accuracy 0.329427\n",
      "loss 2.214248, accuracy 0.333333\n",
      "loss 2.232539, accuracy 0.341146\n",
      "loss 2.253339, accuracy 0.335938\n",
      "loss 2.215877, accuracy 0.316406\n",
      "loss 2.176169, accuracy 0.343750\n",
      "loss 2.215648, accuracy 0.337240\n",
      "loss 2.138359, accuracy 0.339844\n",
      "loss 2.177606, accuracy 0.334635\n",
      "loss 2.171555, accuracy 0.360677\n",
      "loss 2.182890, accuracy 0.334635\n",
      "loss 2.248043, accuracy 0.339844\n",
      "loss 2.141343, accuracy 0.355469\n",
      "loss 2.238141, accuracy 0.312500\n",
      "loss 2.249245, accuracy 0.342448\n",
      "loss 2.233087, accuracy 0.307292\n",
      "loss 2.185202, accuracy 0.335938\n",
      "loss 2.200991, accuracy 0.329427\n",
      "loss 2.201895, accuracy 0.317708\n",
      "loss 2.238558, accuracy 0.351562\n",
      "loss 2.207135, accuracy 0.348958\n",
      "loss 2.228240, accuracy 0.333333\n",
      "loss 2.182473, accuracy 0.330729\n",
      "loss 2.252034, accuracy 0.350260\n",
      "loss 2.214516, accuracy 0.324219\n",
      "loss 2.178264, accuracy 0.342448\n",
      "loss 2.118064, accuracy 0.355469\n",
      "loss 2.244731, accuracy 0.319010\n",
      "loss 2.217385, accuracy 0.311198\n",
      "loss 2.223584, accuracy 0.346354\n",
      "loss 2.292731, accuracy 0.322917\n",
      "loss 2.277973, accuracy 0.309896\n",
      "loss 2.202044, accuracy 0.305990\n",
      "loss 2.192938, accuracy 0.330729\n",
      "loss 2.193801, accuracy 0.338542\n",
      "loss 2.281462, accuracy 0.326823\n",
      "loss 2.260753, accuracy 0.313802\n",
      "loss 2.217407, accuracy 0.312500\n",
      "loss 2.135882, accuracy 0.360677\n",
      "loss 2.222905, accuracy 0.316406\n",
      "loss 2.274348, accuracy 0.308594\n",
      "loss 2.196067, accuracy 0.334635\n",
      "loss 2.178558, accuracy 0.368490\n",
      "loss 2.141682, accuracy 0.358073\n",
      "loss 2.231443, accuracy 0.329427\n",
      "loss 2.210272, accuracy 0.329427\n",
      "loss 2.232892, accuracy 0.338542\n",
      "loss 2.155731, accuracy 0.360677\n",
      "loss 2.213807, accuracy 0.321615\n",
      "loss 2.253117, accuracy 0.325521\n",
      "loss 2.193550, accuracy 0.367188\n",
      "loss 2.170631, accuracy 0.350260\n",
      "loss 2.236763, accuracy 0.335938\n",
      "loss 2.196585, accuracy 0.330729\n",
      "loss 2.139962, accuracy 0.364583\n",
      "loss 2.209803, accuracy 0.321615\n",
      "loss 2.297678, accuracy 0.333333\n",
      "loss 2.243154, accuracy 0.337240\n",
      "loss 2.253391, accuracy 0.299479\n",
      "loss 2.273009, accuracy 0.303385\n",
      "loss 2.211265, accuracy 0.330729\n",
      "loss 2.187006, accuracy 0.337240\n",
      "loss 2.190794, accuracy 0.333333\n",
      "loss 2.182585, accuracy 0.332031\n",
      "loss 2.209208, accuracy 0.342448\n",
      "loss 2.189840, accuracy 0.351562\n",
      "loss 2.101873, accuracy 0.381510\n",
      "loss 2.170597, accuracy 0.334635\n",
      "loss 2.239135, accuracy 0.328125\n",
      "loss 2.269659, accuracy 0.296875\n",
      "loss 2.257373, accuracy 0.343750\n",
      "loss 2.182314, accuracy 0.367188\n",
      "loss 2.200202, accuracy 0.346354\n",
      "loss 2.124497, accuracy 0.359375\n",
      "loss 2.183820, accuracy 0.324219\n",
      "loss 2.155221, accuracy 0.343750\n",
      "loss 2.207728, accuracy 0.350260\n",
      "loss 2.245039, accuracy 0.333333\n",
      "loss 2.194708, accuracy 0.352865\n",
      "loss 2.222498, accuracy 0.355469\n",
      "loss 2.182214, accuracy 0.346354\n",
      "loss 2.181542, accuracy 0.334635\n",
      "loss 2.281737, accuracy 0.311198\n",
      "loss 2.169934, accuracy 0.351562\n",
      "loss 2.139406, accuracy 0.342448\n",
      "loss 2.239868, accuracy 0.335938\n",
      "loss 2.205346, accuracy 0.334635\n",
      "loss 2.187508, accuracy 0.328125\n",
      "loss 2.187114, accuracy 0.348958\n",
      "loss 2.198515, accuracy 0.365885\n",
      "loss 2.203159, accuracy 0.330729\n",
      "loss 2.256301, accuracy 0.315104\n",
      "loss 2.233394, accuracy 0.321615\n",
      "loss 2.179471, accuracy 0.343750\n",
      "loss 2.211263, accuracy 0.325521\n",
      "loss 2.250906, accuracy 0.324219\n",
      "loss 2.190210, accuracy 0.337240\n",
      "loss 2.173383, accuracy 0.324219\n",
      "loss 2.173174, accuracy 0.356771\n",
      "loss 2.198404, accuracy 0.332031\n",
      "loss 2.226908, accuracy 0.325521\n",
      "loss 2.209976, accuracy 0.342448\n",
      "loss 2.129075, accuracy 0.352865\n",
      "loss 2.222952, accuracy 0.321615\n",
      "loss 2.236165, accuracy 0.332031\n",
      "loss 2.167363, accuracy 0.332031\n",
      "loss 2.280611, accuracy 0.319010\n",
      "loss 2.209878, accuracy 0.316406\n",
      "loss 2.238903, accuracy 0.328125\n",
      "loss 2.262645, accuracy 0.319010\n",
      "loss 2.162218, accuracy 0.351562\n",
      "loss 2.246062, accuracy 0.351562\n",
      "loss 2.203823, accuracy 0.345052\n",
      "loss 2.183074, accuracy 0.342448\n",
      "loss 2.160957, accuracy 0.342448\n",
      "loss 2.201613, accuracy 0.330729\n",
      "loss 2.284440, accuracy 0.321615\n",
      "loss 2.216430, accuracy 0.332031\n",
      "loss 2.172625, accuracy 0.345052\n",
      "loss 2.154981, accuracy 0.364583\n",
      "loss 2.179002, accuracy 0.346354\n",
      "loss 2.215721, accuracy 0.341146\n",
      "loss 2.100863, accuracy 0.343750\n",
      "loss 2.290935, accuracy 0.307292\n",
      "loss 2.210298, accuracy 0.333333\n",
      "loss 2.194179, accuracy 0.330729\n",
      "loss 2.238629, accuracy 0.322917\n",
      "loss 2.205169, accuracy 0.296875\n",
      "loss 2.183907, accuracy 0.311198\n",
      "loss 2.181158, accuracy 0.332031\n",
      "loss 2.128960, accuracy 0.373698\n",
      "loss 2.226432, accuracy 0.335938\n",
      "loss 2.215240, accuracy 0.325521\n",
      "loss 2.216141, accuracy 0.350260\n",
      "loss 2.207863, accuracy 0.356771\n",
      "loss 2.107439, accuracy 0.346354\n",
      "loss 2.132825, accuracy 0.373698\n",
      "loss 2.176962, accuracy 0.337240\n",
      "loss 2.215388, accuracy 0.335938\n",
      "loss 2.144161, accuracy 0.341146\n",
      "loss 2.214336, accuracy 0.326823\n",
      "loss 2.137583, accuracy 0.360677\n",
      "loss 2.180447, accuracy 0.332031\n",
      "loss 2.251113, accuracy 0.337240\n",
      "loss 2.166890, accuracy 0.329427\n",
      "loss 2.245920, accuracy 0.350260\n",
      "loss 2.237706, accuracy 0.324219\n",
      "loss 2.193648, accuracy 0.324219\n",
      "loss 2.172313, accuracy 0.342448\n",
      "loss 2.170446, accuracy 0.345052\n",
      "loss 2.224310, accuracy 0.302083\n",
      "loss 2.168327, accuracy 0.361979\n",
      "loss 2.117419, accuracy 0.364583\n",
      "loss 2.263634, accuracy 0.342448\n",
      "loss 2.159943, accuracy 0.345052\n",
      "loss 2.188327, accuracy 0.319010\n",
      "loss 2.194895, accuracy 0.361979\n",
      "loss 2.129365, accuracy 0.346354\n",
      "loss 2.263970, accuracy 0.328125\n",
      "loss 2.208095, accuracy 0.333333\n",
      "loss 2.175422, accuracy 0.372396\n",
      "loss 2.158748, accuracy 0.343750\n",
      "loss 2.175387, accuracy 0.337240\n",
      "loss 2.156188, accuracy 0.341146\n",
      "loss 2.216873, accuracy 0.334635\n",
      "loss 2.209116, accuracy 0.333333\n",
      "loss 2.162213, accuracy 0.350260\n",
      "loss 2.223158, accuracy 0.320312\n",
      "loss 2.175805, accuracy 0.350260\n",
      "loss 2.267615, accuracy 0.321615\n",
      "loss 2.167892, accuracy 0.346354\n",
      "loss 2.205804, accuracy 0.335938\n",
      "loss 2.220594, accuracy 0.334635\n",
      "loss 2.208357, accuracy 0.328125\n",
      "loss 2.245503, accuracy 0.332031\n",
      "loss 2.201235, accuracy 0.333333\n",
      "loss 2.169969, accuracy 0.348958\n",
      "loss 2.173421, accuracy 0.351562\n",
      "loss 2.215682, accuracy 0.321615\n",
      "loss 2.173518, accuracy 0.339844\n",
      "loss 2.197638, accuracy 0.339844\n",
      "loss 2.231435, accuracy 0.341146\n",
      "loss 2.226327, accuracy 0.341146\n",
      "loss 2.104091, accuracy 0.382812\n",
      "loss 2.199566, accuracy 0.346354\n",
      "loss 2.199322, accuracy 0.334635\n",
      "loss 2.272109, accuracy 0.337240\n",
      "loss 2.248788, accuracy 0.294271\n",
      "loss 2.204525, accuracy 0.324219\n",
      "loss 2.135376, accuracy 0.335938\n",
      "loss 2.206230, accuracy 0.339844\n",
      "loss 2.212681, accuracy 0.312500\n",
      "loss 2.219097, accuracy 0.335938\n",
      "loss 2.249129, accuracy 0.319010\n",
      "loss 2.164988, accuracy 0.338542\n",
      "loss 2.254324, accuracy 0.321615\n",
      "loss 2.101065, accuracy 0.363281\n",
      "loss 2.190300, accuracy 0.348958\n",
      "loss 2.157946, accuracy 0.346354\n",
      "loss 2.156175, accuracy 0.363281\n",
      "loss 2.143034, accuracy 0.369792\n",
      "loss 2.230639, accuracy 0.333333\n",
      "loss 2.087131, accuracy 0.372396\n",
      "loss 2.206836, accuracy 0.339844\n",
      "loss 2.171808, accuracy 0.346354\n",
      "loss 2.176757, accuracy 0.351562\n",
      "loss 2.212679, accuracy 0.330729\n",
      "loss 2.228050, accuracy 0.350260\n",
      "loss 2.209211, accuracy 0.341146\n",
      "loss 2.197641, accuracy 0.352865\n",
      "loss 2.257403, accuracy 0.322917\n",
      "loss 2.110647, accuracy 0.367188\n",
      "loss 2.230427, accuracy 0.311198\n",
      "loss 2.229499, accuracy 0.345052\n",
      "loss 2.211824, accuracy 0.317708\n",
      "loss 2.199665, accuracy 0.343750\n",
      "loss 2.207201, accuracy 0.351562\n",
      "loss 2.263960, accuracy 0.334635\n",
      "loss 2.182573, accuracy 0.338542\n",
      "loss 2.229794, accuracy 0.334635\n",
      "loss 2.196655, accuracy 0.347656\n",
      "loss 2.167266, accuracy 0.341146\n",
      "loss 2.149179, accuracy 0.337240\n",
      "loss 2.110581, accuracy 0.363281\n",
      "loss 2.273711, accuracy 0.295573\n",
      "loss 2.195260, accuracy 0.339844\n",
      "loss 2.242852, accuracy 0.352865\n",
      "loss 2.213896, accuracy 0.351562\n",
      "loss 2.172362, accuracy 0.373698\n",
      "loss 2.231288, accuracy 0.321615\n",
      "loss 2.223690, accuracy 0.333333\n",
      "loss 2.220010, accuracy 0.355469\n",
      "loss 2.226846, accuracy 0.337240\n",
      "loss 2.206196, accuracy 0.356771\n",
      "loss 2.205821, accuracy 0.371094\n",
      "loss 2.186065, accuracy 0.342448\n",
      "loss 2.199310, accuracy 0.341146\n",
      "loss 2.235598, accuracy 0.320312\n",
      "loss 2.205171, accuracy 0.343750\n",
      "loss 2.142406, accuracy 0.360677\n",
      "loss 2.219012, accuracy 0.339844\n",
      "loss 2.100750, accuracy 0.348958\n",
      "loss 2.203883, accuracy 0.333333\n",
      "loss 2.198613, accuracy 0.346354\n",
      "loss 2.126631, accuracy 0.347656\n",
      "loss 2.239690, accuracy 0.312500\n",
      "loss 2.151731, accuracy 0.365885\n",
      "loss 2.238126, accuracy 0.334635\n",
      "loss 2.225796, accuracy 0.317708\n",
      "loss 2.206868, accuracy 0.312500\n",
      "loss 2.211531, accuracy 0.329427\n",
      "loss 2.239219, accuracy 0.319010\n",
      "loss 2.164689, accuracy 0.342448\n",
      "loss 2.198342, accuracy 0.342448\n",
      "loss 2.171479, accuracy 0.350260\n",
      "loss 2.228443, accuracy 0.320312\n",
      "loss 2.177994, accuracy 0.321615\n",
      "loss 2.235927, accuracy 0.337240\n",
      "loss 2.192581, accuracy 0.342448\n",
      "loss 2.194733, accuracy 0.361979\n",
      "loss 2.207229, accuracy 0.337240\n",
      "loss 2.169139, accuracy 0.350260\n",
      "loss 2.194350, accuracy 0.325521\n",
      "loss 2.223584, accuracy 0.320312\n",
      "loss 2.150020, accuracy 0.352865\n",
      "loss 2.229898, accuracy 0.330729\n",
      "loss 2.168081, accuracy 0.337240\n",
      "loss 2.180391, accuracy 0.334635\n",
      "loss 2.158629, accuracy 0.332031\n",
      "loss 2.187990, accuracy 0.351562\n",
      "loss 2.244797, accuracy 0.330729\n",
      "loss 2.229393, accuracy 0.333333\n",
      "loss 2.245938, accuracy 0.329427\n",
      "loss 2.161765, accuracy 0.356771\n",
      "loss 2.213782, accuracy 0.338542\n",
      "loss 2.251751, accuracy 0.322917\n",
      "loss 2.250043, accuracy 0.332031\n",
      "loss 2.186706, accuracy 0.356771\n",
      "loss 2.262279, accuracy 0.321615\n",
      "loss 2.247675, accuracy 0.330729\n",
      "loss 2.260142, accuracy 0.330729\n",
      "loss 2.252545, accuracy 0.311198\n",
      "loss 2.192184, accuracy 0.339844\n",
      "loss 2.264050, accuracy 0.316406\n",
      "loss 2.139326, accuracy 0.343750\n",
      "loss 2.161390, accuracy 0.333333\n",
      "loss 2.142802, accuracy 0.369792\n",
      "loss 2.170522, accuracy 0.337240\n",
      "loss 2.186836, accuracy 0.342448\n",
      "loss 2.142488, accuracy 0.339844\n",
      "loss 2.170063, accuracy 0.352865\n",
      "loss 2.199838, accuracy 0.320312\n",
      "loss 2.202936, accuracy 0.341146\n",
      "loss 2.219789, accuracy 0.360677\n",
      "loss 2.198536, accuracy 0.335938\n",
      "loss 2.168445, accuracy 0.330729\n",
      "loss 2.187665, accuracy 0.321615\n",
      "loss 2.206475, accuracy 0.347656\n",
      "loss 2.178617, accuracy 0.354167\n",
      "loss 2.222896, accuracy 0.328125\n",
      "loss 2.224356, accuracy 0.317708\n",
      "loss 2.218354, accuracy 0.348958\n",
      "loss 2.175718, accuracy 0.338542\n",
      "loss 2.235705, accuracy 0.330729\n",
      "loss 2.211681, accuracy 0.322917\n",
      "loss 2.209549, accuracy 0.348958\n",
      "loss 2.162078, accuracy 0.351562\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):  \n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:,n,:] = onehot[id1, :]\n",
    "        batch_y[:,n,:] = onehot[id2, :]\n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, Wc, Wo, V))\n",
    "        prev_c = layers[-1][7]\n",
    "        prev_h = layers[-1][10]\n",
    "        out_logits[:, n, :] = layers[-1][-4]\n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)),axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWf = np.zeros(Wf.shape)\n",
    "    dWi = np.zeros(Wi.shape)\n",
    "    dWc = np.zeros(Wc.shape)\n",
    "    dWo = np.zeros(Wo.shape)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWf_t, dWi_t, dWc_t, dWo_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, \n",
    "                                                                    Wc, Wo, V, d_mul_v, layers[n])\n",
    "        prev_c = layers[n][7]\n",
    "        prev_h = layers[n][10]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWf += dWf_t\n",
    "        dWi += dWi_t\n",
    "        dWc += dWc_t\n",
    "        dWo += dWo_t\n",
    "    U_g += dU ** 2\n",
    "    U += -learning_rate * dU / np.sqrt(U_g + epsilon)\n",
    "    V_g += dV ** 2\n",
    "    V += -learning_rate * dV / np.sqrt(V_g + epsilon)\n",
    "    Wf_g += dWf ** 2\n",
    "    Wf += -learning_rate * dWf / np.sqrt(Wf_g + epsilon)\n",
    "    Wi_g += dWi ** 2\n",
    "    Wi += -learning_rate * dWi / np.sqrt(Wi_g + epsilon)\n",
    "    Wc_g += dWc ** 2\n",
    "    Wc += -learning_rate * dWc / np.sqrt(Wc_g + epsilon)\n",
    "    Wo_g += dWo ** 2\n",
    "    Wo += -learning_rate * dWo / np.sqrt(Wo_g + epsilon)\n",
    "    \n",
    "    print('loss %f, accuracy %f'%(loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
