{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Layer \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np \n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_sentence_list(file_path): \n",
    "\twith open(file_path, 'r') as file: \n",
    "\t\ttext = file.read() \n",
    "\n",
    "\t# Splitting the text into sentences using \n",
    "\t# delimiters like '.', '?', and '!' \n",
    "\tsentences = [sentence.strip() for sentence in re.split( \n",
    "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "\n",
    "\treturn sentences \n",
    "\n",
    "file_path = 'pizza.txt'\n",
    "text_data = file_to_sentence_list(file_path) \n",
    "\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0] \n",
    "\tfor i in range(1, len(token_list)): \n",
    "\t\tn_gram_sequence = token_list[:i+1] \n",
    "\t\tinput_sequences.append(n_gram_sequence) \n",
    "\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "\tinput_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "\n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfGRU(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(SelfGRU, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units * 3),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units * 3),\n",
    "                                                initializer='orthogonal',\n",
    "                                                name='recurrent_kernel')\n",
    "        self.bias = self.add_weight(shape=(self.units * 3,),\n",
    "                                    initializer='zeros',\n",
    "                                    name='bias')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states=None):\n",
    "        prev_state = states[0] if states is not None else tf.zeros(shape=(inputs.shape[0], self.units))\n",
    "        z = tf.matmul(inputs, self.kernel)\n",
    "        z += tf.matmul(prev_state, self.recurrent_kernel)\n",
    "        z += self.bias\n",
    "\n",
    "        z_update, z_reset, z_new = tf.split(z, num_or_size_splits=3, axis=1)\n",
    "\n",
    "        update_gate = tf.nn.sigmoid(z_update)\n",
    "        reset_gate = tf.nn.sigmoid(z_reset)\n",
    "        new_state = tf.nn.tanh(z_new)\n",
    "\n",
    "        output = new_state * (1 - update_gate) + prev_state * update_gate\n",
    "\n",
    "        return output, [output]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SelfGRU, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model \n",
    "model = Sequential() \n",
    "model.add(Embedding(total_words, 10, \n",
    "\t\t\t\t\tinput_length=max_sequence_len-1)) \n",
    "model.add(GRU(128))\n",
    "model.add(Dense(total_words, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "\t\t\toptimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "51/51 [==============================] - 2s 10ms/step - loss: 6.2843 - accuracy: 0.0455\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.8035 - accuracy: 0.0571\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.7193 - accuracy: 0.0541\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.6863 - accuracy: 0.0571\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.6288 - accuracy: 0.0663\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 5.5502 - accuracy: 0.0676\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.4369 - accuracy: 0.0651\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.3053 - accuracy: 0.0780\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 5.1517 - accuracy: 0.0952\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 4.9953 - accuracy: 0.1075\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 4.8342 - accuracy: 0.1100\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 4.6555 - accuracy: 0.1210\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 4.4805 - accuracy: 0.1351\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 4.3047 - accuracy: 0.1462\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 4.1303 - accuracy: 0.1603\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 3.9600 - accuracy: 0.1757\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 3.7885 - accuracy: 0.1830\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 3.6126 - accuracy: 0.2076\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 3.4458 - accuracy: 0.2267\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 0s 10ms/step - loss: 3.2892 - accuracy: 0.2568\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 3.1273 - accuracy: 0.2930\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.9679 - accuracy: 0.3317\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.8126 - accuracy: 0.3839\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.6727 - accuracy: 0.4036\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.5321 - accuracy: 0.4490\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.3970 - accuracy: 0.4969\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.2622 - accuracy: 0.5362\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.1301 - accuracy: 0.5706\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 2.0136 - accuracy: 0.6014\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.8928 - accuracy: 0.6284\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.7847 - accuracy: 0.6486\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.6824 - accuracy: 0.6775\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.5796 - accuracy: 0.7039\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.4860 - accuracy: 0.7242\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.3938 - accuracy: 0.7469\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.3105 - accuracy: 0.7770\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.2301 - accuracy: 0.7862\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.1559 - accuracy: 0.8059\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 1s 10ms/step - loss: 1.0832 - accuracy: 0.8256\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 1.0173 - accuracy: 0.8354\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.9565 - accuracy: 0.8403\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.8956 - accuracy: 0.8618\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.8398 - accuracy: 0.8716\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.7928 - accuracy: 0.8870\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.7468 - accuracy: 0.8931\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.7062 - accuracy: 0.8999\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.6624 - accuracy: 0.9152\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.6258 - accuracy: 0.9189\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 0.5906 - accuracy: 0.9208\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.5593 - accuracy: 0.9294\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.5298 - accuracy: 0.9287\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.5049 - accuracy: 0.9324\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.4811 - accuracy: 0.9361\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.4575 - accuracy: 0.9361\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.4335 - accuracy: 0.9392\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.4159 - accuracy: 0.9386\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.3950 - accuracy: 0.9410\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 1s 11ms/step - loss: 0.3795 - accuracy: 0.9453\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.3624 - accuracy: 0.9429\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.3511 - accuracy: 0.9478\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.3351 - accuracy: 0.9478\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.3237 - accuracy: 0.9472\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 0.3104 - accuracy: 0.9509\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 0.3029 - accuracy: 0.9496\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2875 - accuracy: 0.9545\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2800 - accuracy: 0.9515\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 1s 15ms/step - loss: 0.2722 - accuracy: 0.9539\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 1s 14ms/step - loss: 0.2618 - accuracy: 0.9552\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2532 - accuracy: 0.9533\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2474 - accuracy: 0.9545\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2373 - accuracy: 0.9588\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2321 - accuracy: 0.9564\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2261 - accuracy: 0.9595\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2201 - accuracy: 0.9564\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2138 - accuracy: 0.9576\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2082 - accuracy: 0.9601\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 1s 13ms/step - loss: 0.2035 - accuracy: 0.9582\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1975 - accuracy: 0.9570\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1918 - accuracy: 0.9595\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1891 - accuracy: 0.9601\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1846 - accuracy: 0.9607\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1788 - accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1759 - accuracy: 0.9613\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1716 - accuracy: 0.9601\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1686 - accuracy: 0.9607\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1652 - accuracy: 0.9619\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1616 - accuracy: 0.9650\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2825 - accuracy: 0.9429\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.3872 - accuracy: 0.9312\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.2397 - accuracy: 0.9552\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1856 - accuracy: 0.9619\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1696 - accuracy: 0.9662\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1629 - accuracy: 0.9625\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1566 - accuracy: 0.9613\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1529 - accuracy: 0.9638\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1501 - accuracy: 0.9607\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1447 - accuracy: 0.9650\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1418 - accuracy: 0.9625\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1401 - accuracy: 0.9656\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 1s 12ms/step - loss: 0.1365 - accuracy: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21d6ab2af70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "model.fit(X, y, epochs=100, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Next predicted words: Pizza have different  much mobile owned establishments to\n"
     ]
    }
   ],
   "source": [
    "# Generate next word predictions \n",
    "seed_text = \"Pizza have different \"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range(next_words): \n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "\ttoken_list = pad_sequences( \n",
    "\t\t[token_list], maxlen=max_sequence_len-1, padding='pre') \n",
    "\tpredicted_probs = model.predict(token_list) \n",
    "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)] \n",
    "\tseed_text += \" \" + predicted_word \n",
    "\n",
    "print(\"Next predicted words:\", seed_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model2 = Sequential() \n",
    "model2.add(Embedding(total_words, 10, \n",
    "\t\t\t\t\tinput_length=max_sequence_len-1)) \n",
    "model2.add(SelfGRU(128))  # Using the custom SelfGRU layer\n",
    "model2.add(Dense(total_words, activation='softmax')) \n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "model2.fit(X, y, epochs=100, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate next word predictions \n",
    "seed_text = \"Pizza have different \"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range(next_words): \n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "\ttoken_list = pad_sequences( \n",
    "\t\t[token_list], maxlen=max_sequence_len-1, padding='pre') \n",
    "\tpredicted_probs = model2.predict(token_list) \n",
    "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)] \n",
    "\tseed_text += \" \" + predicted_word \n",
    "\n",
    "print(\"Next predicted words:\", seed_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,14) and (1,14) not aligned: 14 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m--> 105\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI am a bot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mtrain_gru\u001b[1;34m(gru, sentence, vocab, learning_rate)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m one_hot(word, vocab)\n\u001b[1;32m---> 91\u001b[0m     h_prev \u001b[38;5;241m=\u001b[39m \u001b[43mgru\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m gru\u001b[38;5;241m.\u001b[39mpredict(h_prev)\n\u001b[0;32m     93\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m one_hot(word, vocab)\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, x, h_prev)\u001b[0m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add an extra dimension to x\u001b[39;00m\n\u001b[0;32m     38\u001b[0m concat_h_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((h_prev, x))\n\u001b[1;32m---> 39\u001b[0m z \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_h_x\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbz)\n\u001b[0;32m     40\u001b[0m r \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWr, concat_h_x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbr)\n\u001b[0;32m     41\u001b[0m concat_r_h_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((r \u001b[38;5;241m*\u001b[39m h_prev, x))\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,14) and (1,14) not aligned: 14 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Weight initialization\n",
    "        self.Wz = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.Wr = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.Wh = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.Wy = np.random.randn(vocab_size, hidden_dim)\n",
    "\n",
    "        # Bias initialization\n",
    "        self.bz = np.zeros(hidden_dim)\n",
    "        self.br = np.zeros(hidden_dim)\n",
    "        self.bh = np.zeros(hidden_dim)\n",
    "        self.by = np.zeros(vocab_size)\n",
    "\n",
    "    def predict(self, h):\n",
    "        return np.dot(self.Wy, h) + self.by\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        x = x.reshape(1, -1)  # Add an extra dimension to x\n",
    "        concat_h_x = np.hstack((h_prev, x))\n",
    "        z = sigmoid(np.dot(self.Wz, concat_h_x) + self.bz)\n",
    "        r = sigmoid(np.dot(self.Wr, concat_h_x) + self.br)\n",
    "        concat_r_h_x = np.hstack((r * h_prev, x))\n",
    "        h_tilde = tanh(np.dot(self.Wh, concat_r_h_x) + self.bh)\n",
    "        h_next = (1 - z) * h_prev + z * h_tilde\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, x, h_prev, h_next, y_true):\n",
    "        dWy, dWz, dWr, dWh = np.zeros_like(self.Wy), np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(self.Wh)\n",
    "        dby, dbz, dbr, dbh = np.zeros_like(self.by), np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(self.bh)\n",
    "        concat_h_x = np.hstack((h_prev, x))\n",
    "\n",
    "        # Backpropagation through time\n",
    "        dy = np.copy(y_true)\n",
    "        dh_next = np.dot(self.Wy.T, dy)\n",
    "        dz = dh_next * (h_next - h_prev) * tanh_grad(h_next)\n",
    "        dr = dh_next * np.hstack((h_prev, np.zeros(self.input_dim))) * tanh_grad(h_next * r)\n",
    "        dh_tilde = dh_next * z\n",
    "        dh_prev = dh_next * (1 - z) + dr * h_prev * tanh_grad(h_next * r)\n",
    "        concat_r_h_x = np.hstack((r * h_prev, x))\n",
    "\n",
    "        dWy += np.dot(dy, h_next.T)\n",
    "        dWz += np.dot(dz, concat_h_x.T)\n",
    "        dWr += np.dot(dr, concat_h_x.T)\n",
    "        dWh += np.dot(dh_tilde, concat_r_h_x.T)\n",
    "        dby += dy\n",
    "        dbz += dz\n",
    "        dbr += dr\n",
    "        dbh += dh_tilde\n",
    "\n",
    "        return dWy, dby, dWz, dbz, dWr, dbr, dWh, dbh\n",
    "\n",
    "    def update_weights(self, dWy, dby, dWz, dbz, dWr, dbr, dWh, dbh, learning_rate):\n",
    "        self.Wy -= learning_rate * dWy\n",
    "        self.by -= learning_rate * dby\n",
    "        self.Wz -= learning_rate * dWz\n",
    "        self.bz -= learning_rate * dbz\n",
    "        self.Wr -= learning_rate * dWr\n",
    "        self.br -= learning_rate * dbr\n",
    "        self.Wh -= learning_rate * dWh\n",
    "        self.bh -= learning_rate * dbh\n",
    "\n",
    "def one_hot(word, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    vec[vocab.index(word)] = 1\n",
    "    return vec\n",
    "\n",
    "def train_gru(gru, sentence, vocab, learning_rate=0.01):\n",
    "    h_prev = np.zeros((1, gru.hidden_dim))  # Initialize h_prev with the correct shape\n",
    "    loss = 0\n",
    "    for word in sentence.split():\n",
    "        x = one_hot(word, vocab)\n",
    "        h_prev = gru.forward(x, h_prev)\n",
    "        y_pred = gru.predict(h_prev)\n",
    "        y_true = one_hot(word, vocab)\n",
    "        loss += np.sum((y_pred - y_true) ** 2)  # Mean squared error loss\n",
    "        dWy, dby, dWz, dbz, dWr, dbr, dWh, dbh = gru.backward(x, h_prev, y_pred, y_true)\n",
    "        gru.update_weights(dWy, dby, dWz, dbz, dWr, dbr, dWh, dbh, learning_rate)\n",
    "\n",
    "    return loss\n",
    "\n",
    "vocab = ['I', 'am', 'a', 'bot']\n",
    "gru = GRU(input_dim=len(vocab), hidden_dim=10, vocab_size=len(vocab))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    loss = train_gru(gru, 'I am a bot', vocab)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Prediction\n",
    "h_prev = np.zeros(gru.hidden_dim)\n",
    "sentence = 'I am a'\n",
    "for word in sentence.split():\n",
    "    x = np.array(one_hot(word, vocab))\n",
    "    h_prev = gru.forward(x, h_prev)\n",
    "\n",
    "y_pred = gru.predict(h_prev)\n",
    "print(f\"Predicted next word: {vocab[np.argmax(y_pred)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,132) (128,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     h_prev \u001b[38;5;241m=\u001b[39m gru\u001b[38;5;241m.\u001b[39mforward(x, h_prev)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Predict the last word\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m last_word_logits \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgru\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_h\u001b[49m\n\u001b[0;32m     73\u001b[0m last_word_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(last_word_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     74\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m idx_to_word[last_word_idx]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,132) (128,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Initialize weights\n",
    "        self.W_z = np.random.randn(input_dim + hidden_dim, hidden_dim)\n",
    "        self.W_r = np.random.randn(input_dim + hidden_dim, hidden_dim)\n",
    "        self.W_h = np.random.randn(input_dim + hidden_dim, hidden_dim)\n",
    "        self.b_z = np.zeros((1, hidden_dim))\n",
    "        self.b_r = np.zeros((1, hidden_dim))\n",
    "        self.b_h = np.zeros((hidden_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # Add an extra dimension to the input vector\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.column_stack((x, h_prev))\n",
    "        # Calculate gates\n",
    "        z = sigmoid(np.dot(combined, self.W_z) + self.b_z)\n",
    "        r = sigmoid(np.dot(combined, self.W_r) + self.b_r)\n",
    "        h_tilde = np.tanh(np.dot(np.column_stack((x, r * h_prev)), self.W_h) + self.b_h)\n",
    "        # Calculate next hidden state\n",
    "        h_next = (1 - z) * h_prev + z * h_tilde\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, x, h_prev, h_next, dh_next):\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = np.column_stack((x, h_prev))\n",
    "\n",
    "        # Calculate gates\n",
    "        z = sigmoid(np.dot(combined, self.W_z) + self.b_z)\n",
    "        r = sigmoid(np.dot(combined, self.W_r) + self.b_r)\n",
    "        h_tilde = np.tanh(np.dot(np.column_stack((x, r * h_prev)), self.W_h) + self.b_h)\n",
    "\n",
    "        # Calculate gradients\n",
    "        dh_tilde = dh_next * z\n",
    "        dz = dh_next * (h_tilde - h_prev)\n",
    "        dr = dh_next * (1 - z) * np.dot(np.column_stack((x, h_prev * r)), self.W_h.T) * (1 - r ** 2)\n",
    "        dx = np.dot(dh_next * (1 - z), self.W_h[:, :self.input_dim].T) + np.dot(dr * h_prev, self.W_h[:, :self.input_dim].T)\n",
    "        dh_prev = dh_next * (1 - z) * r + np.dot(dr, self.W_r[:, self.input_dim:].T) + np.dot(dh_next * (1 - z) * (1 - h_tilde ** 2), self.W_h[:, self.input_dim:].T)\n",
    "\n",
    "        # Calculate weight gradients\n",
    "        dW_z = np.dot(combined.T, dz)\n",
    "        dW_r = np.dot(combined.T, dr)\n",
    "        dW_h = np.dot(np.column_stack((x, r * h_prev)).T, dh_tilde)\n",
    "        db_z = np.sum(dz, axis=0, keepdims=True)\n",
    "        db_r = np.sum(dr, axis=0, keepdims=True)\n",
    "        db_h = np.sum(dh_tilde, axis=0, keepdims=True)\n",
    "\n",
    "        return dx, dh_prev, dW_z, dW_r, dW_h, db_z, db_r, db_h\n",
    "    \n",
    "input_sequence = ['I', 'am', 'a', 'bot']\n",
    "# Create a dictionary to map words to indices\n",
    "word_to_idx = {word: idx for idx, word in enumerate(set(input_sequence))}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "# One-hot encode the input sequence\n",
    "input_sequence = [np.eye(len(word_to_idx))[word_to_idx[word]] for word in input_sequence]\n",
    "# Initialize the GRU\n",
    "input_dim = len(word_to_idx)\n",
    "hidden_dim = 128\n",
    "gru = GRU(input_dim, hidden_dim)\n",
    "# Forward pass through the GRU\n",
    "h_prev = np.zeros((1, hidden_dim))\n",
    "for x in input_sequence[:-1]:\n",
    "    h_prev = gru.forward(x, h_prev)\n",
    "# Predict the last word\n",
    "last_word_logits = np.dot(h_prev, gru.W_h.T) + gru.b_h\n",
    "last_word_idx = np.argmax(last_word_logits, axis=1)[0]\n",
    "predicted_word = idx_to_word[last_word_idx]\n",
    "print(f\"Predicted last word: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
