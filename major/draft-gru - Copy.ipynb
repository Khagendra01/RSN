{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input * (1 - input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def softmax(input):\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum(axis=0, keepdims=True)\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_size = hidden_size\n",
    "        self.learning_rate = 1e-1\n",
    "\n",
    "        self.init_parameters()\n",
    "        self.init_adagrad()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.Wz = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bz = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.br = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(self.h_size + self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.bh = np.zeros((self.h_size, 1))\n",
    "\n",
    "        self.Wy = np.random.rand(self.vocab_size, self.h_size) * 0.1 - 0.05\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "    def init_adagrad(self):\n",
    "        self.mdWy = np.zeros_like(self.Wy)\n",
    "        self.mdWh = np.zeros_like(self.Wh)\n",
    "        self.mdWr = np.zeros_like(self.Wr)\n",
    "        self.mdWz = np.zeros_like(self.Wz)\n",
    "        self.mdby = np.zeros_like(self.by)\n",
    "        self.mdbh = np.zeros_like(self.bh)\n",
    "        self.mdbr = np.zeros_like(self.br)\n",
    "        self.mdbz = np.zeros_like(self.bz)\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        hprev = np.zeros((self.h_size, 1))\n",
    "        z, r, h_hat, h = {}, {}, {}, {-1: hprev}\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "\n",
    "            concat_hx = np.concatenate((h[t-1], x))\n",
    "            r[t] = sigmoid(np.dot(self.Wr.T, concat_hx) + self.br)\n",
    "            z[t] = sigmoid(np.dot(self.Wz.T, concat_hx) + self.bz)\n",
    "\n",
    "            concat_hrx = np.concatenate((np.multiply(r[t], h[t-1]), x))\n",
    "            h_hat[t] = tanh(np.dot(self.Wh.T, concat_hrx) + self.bh)\n",
    "            h[t] = np.multiply(z[t], h[t-1]) + np.multiply(1 - z[t], h_hat[t])\n",
    "\n",
    "        y = np.dot(self.Wy, h[t]) + self.by\n",
    "        probDis = softmax(y)\n",
    "\n",
    "        return z, r, h_hat, h, y, probDis\n",
    "\n",
    "    def backward_pass(self, z, r, h_hat, h, y, probDis, inputs, targets):\n",
    "        # Initialize the gradients for each parameter\n",
    "        dWy, dby = np.zeros_like(self.Wy), np.zeros_like(self.by)\n",
    "        dWh, dWr, dWz = np.zeros_like(self.Wh), np.zeros_like(self.Wr), np.zeros_like(self.Wz)\n",
    "        dbh, dbr, dbz = np.zeros_like(self.bh), np.zeros_like(self.br), np.zeros_like(self.bz)\n",
    "\n",
    "        # Initialize gradients for hidden states\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dh_hat_next = np.zeros_like(h_hat[0])\n",
    "\n",
    "        # Compute loss and gradients at output\n",
    "        loss = -np.sum(np.log(probDis[targets, 0]))\n",
    "        dy = probDis.copy()\n",
    "        dy[targets] -= 1\n",
    "\n",
    "        # Update gradients for Wy and by\n",
    "        dWy += np.dot(dy, h[len(inputs) - 1].T)\n",
    "        dby += dy\n",
    "\n",
    "        # Backpropagation through time\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[inputs[t]] = 1\n",
    "            # Derivatives of the hidden state\n",
    "            dh = np.dot(self.Wy.T, dy) + dh_next\n",
    "            dh_hat = dh * (1 - z[t])\n",
    "            dz = dh * (h[t - 1] - h_hat[t])\n",
    "\n",
    "            # Derivatives of candidate hidden state\n",
    "            dh_hat_tilde = dh_hat * (1 - h_hat[t] ** 2)\n",
    "            dWh += np.dot(np.concatenate((r[t] * h[t - 1], x), axis=0), dh_hat_tilde.T)\n",
    "            dbh += dh_hat_tilde\n",
    "\n",
    "            # Derivatives of reset gate\n",
    "            dr = np.dot(self.Wh[:self.h_size, :].T, dh_hat_tilde) * h[t - 1]\n",
    "            dr = dr * r[t] * (1 - r[t])\n",
    "            dWr += np.dot(np.concatenate((h[t - 1], x), axis=0), dr.T)\n",
    "            dbr += dr\n",
    "\n",
    "            # Derivatives of update gate\n",
    "            dz = dz * z[t] * (1 - z[t])\n",
    "            dWz += np.dot(np.concatenate((h[t - 1], x), axis=0), dz.T)\n",
    "            dbz += dz\n",
    "\n",
    "            # Update dh_next for next timestep\n",
    "            dh_next = dh * z[t] + np.dot(self.Wr[:, :self.h_size].T, dr) + np.dot(self.Wh[:, :self.h_size].T, dh_hat_tilde)\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for dparam in [dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # Update parameters using AdaGrad or similar optimizer steps\n",
    "        self.update_params(dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_params(self, dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz):\n",
    "        for param, dparam, mem in zip([self.Wy, self.by, self.Wh, self.bh, self.Wr, self.br, self.Wz, self.bz],\n",
    "                                    [dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz],\n",
    "                                    [self.mdWy, self.mdby, self.mdWh, self.mdbh, self.mdWr, self.mdbr, self.mdWz, self.mdbz]):\n",
    "            mem += dparam * dparam\n",
    "            param -= self.learning_rate * dparam / (np.sqrt(mem) + 1e-8)\n",
    "\n",
    "    def train(self, inputs, targets, n_iters=100):\n",
    "        for j in range(n_iters):\n",
    "            for i in range(len(inputs)):\n",
    "                z, r, h_hat, h, y, probDis = self.forward_pass(inputs[i])\n",
    "                tgt = np.zeros((self.vocab_size, 1))\n",
    "                tgt[targets] = 1\n",
    "                loss = self.backward_pass(z, r, h_hat, h, y, probDis, inputs[i], targets[i])\n",
    "            print(f\"iteration: {j} loss: {loss}\")\n",
    "\n",
    "    def generate_text(self, inputs):\n",
    "        _, _, _, _, _, probDis = self.forward_pass(inputs)\n",
    "        return np.argmax(probDis, axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 57254 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "with open('input.txt', 'r') as file: \n",
    "\tdata = file.read() \n",
    "    \n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "seq_length = 5\n",
    "oo = 0\n",
    "for k in range(0, len(data) - seq_length):\n",
    "    input_seq = [char_to_ix[ch] for ch in data[k:k+seq_length]]\n",
    "    target_char = char_to_ix[data[k+seq_length]]\n",
    "    inputs.append(input_seq)\n",
    "    targets.append(target_char)\n",
    "\n",
    "inputs = inputs[:100]\n",
    "targets = targets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,77) and (10,1) not aligned: 77 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m neee \u001b[38;5;241m=\u001b[39m GRUModel(vocab_size, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mneee\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[77], line 142\u001b[0m, in \u001b[0;36mGRUModel.train\u001b[1;34m(self, inputs, targets, n_iters)\u001b[0m\n\u001b[0;32m    140\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    141\u001b[0m     tgt[targets] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 142\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobDis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 117\u001b[0m, in \u001b[0;36mGRUModel.backward_pass\u001b[1;34m(self, z, r, h_hat, h, y, probDis, inputs, targets)\u001b[0m\n\u001b[0;32m    114\u001b[0m     dbz \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dz\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Update dh_next for next timestep\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     dh_next \u001b[38;5;241m=\u001b[39m dh \u001b[38;5;241m*\u001b[39m z[t] \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_size\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWh[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_size]\u001b[38;5;241m.\u001b[39mT, dh_hat_tilde)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Clip gradients to prevent exploding gradients\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dparam \u001b[38;5;129;01min\u001b[39;00m [dWy, dby, dWh, dbh, dWr, dbr, dWz, dbz]:\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,77) and (10,1) not aligned: 77 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "neee = GRUModel(vocab_size, 10)\n",
    "neee.train(inputs, targets,100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "hh = neee.generate_text(inputs[2])\n",
    "print(ix_to_char[targets[5]])\n",
    "print(hh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
