{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self, X, Y, X_val, Y_val, hidden_layer_sizes=[128, 64, 32], loss_fn='bce'):\n",
    "        self.X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "        self.Y = np.squeeze(np.eye(10)[Y.astype(int).reshape(-1)])\n",
    "        self.X_val = np.concatenate((X_val, np.ones((X_val.shape[0], 1))), axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(10)[Y_val.astype(int).reshape(-1)])\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes = np.array([self.X.shape[1]] + self.hidden_layer_sizes + [self.Y.shape[1]])\n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss, self.train_acc, self.val_loss, self.val_acc, self.train_time, self.tot_time]\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        # VCompute the sigmoid\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        if self.loss_fn == 'bce':\n",
    "            return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "        elif self.loss_fn == 'cce':\n",
    "            return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "        else:\n",
    "            return \"madhurchud\"\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __relu_prime(self,h):\n",
    "        # Compute the derivative of relu \n",
    "        return np.where(h > 0, 1, 0)\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__relu(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__relu_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__relu_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            \n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Select only the first 1000 samples\n",
    "x_train, y_train = x_train[:1000], y_train[:1000]\n",
    "x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "# Normalize data (each pixel starts as int 0-255; we want it to be a float between 0 and 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\K-Gen\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "32/32 [==============================] - 1s 7ms/step - loss: 1.9379 - accuracy: 0.3850 - val_loss: 1.6408 - val_accuracy: 0.5920\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2940 - accuracy: 0.7430 - val_loss: 1.2655 - val_accuracy: 0.7090\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9701 - accuracy: 0.8150 - val_loss: 1.0650 - val_accuracy: 0.7390\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7978 - accuracy: 0.8500 - val_loss: 0.9403 - val_accuracy: 0.7590\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.8650 - val_loss: 0.8598 - val_accuracy: 0.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f1aaa6ba30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "# we'll use sparse_categorical_crossentropy as the loss function\n",
    "LOSS_FN = keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "# compile the model with standard backprop training algorithm called 'adam'\n",
    "model.compile(optimizer='adam',loss=LOSS_FN,metrics=['accuracy'])\n",
    "\n",
    "# train on training data, and validate on test data\n",
    "# we'll train for 5 epochs\n",
    "model.fit(x_train_flat, y_train, epochs=5, validation_data=(x_test_flat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = nan | acc = 0.101 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.046\n",
      "Epoch 2: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.064 | tot_time = 0.067\n",
      "Epoch 3: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_7568\\3160381693.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  exponent = np.exp(x)\n",
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_7568\\3160381693.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  return exponent/exponent.sum(axis=1,keepdims=True)\n",
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_7568\\3160381693.py:41: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
      "C:\\Users\\K-Gen\\AppData\\Local\\Temp\\ipykernel_7568\\3160381693.py:41: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 5: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 6: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 7: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.043 | tot_time = 0.043\n",
      "Epoch 8: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 9: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 10: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 11: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.04 | tot_time = 0.045\n",
      "Epoch 12: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.031\n",
      "Epoch 13: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.031\n",
      "Epoch 14: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.023\n",
      "Epoch 15: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 16: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.044 | tot_time = 0.047\n",
      "Epoch 17: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 18: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.047\n",
      "Epoch 19: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 20: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 21: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 22: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 23: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 24: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.043 | tot_time = 0.045\n",
      "Epoch 25: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 26: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.038\n",
      "Epoch 27: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.039 | tot_time = 0.042\n",
      "Epoch 28: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.033\n",
      "Epoch 29: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 30: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.018 | tot_time = 0.033\n",
      "Epoch 31: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 32: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 33: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.034\n",
      "Epoch 34: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.033\n",
      "Epoch 35: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 36: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.036 | tot_time = 0.038\n",
      "Epoch 37: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 38: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 39: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 40: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.017 | tot_time = 0.017\n",
      "Epoch 41: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.045 | tot_time = 0.046\n",
      "Epoch 42: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.03\n",
      "Epoch 43: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 44: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 45: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 46: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.033\n",
      "Epoch 47: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 48: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.033\n",
      "Epoch 49: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 50: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 51: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 52: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.036\n",
      "Epoch 53: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 54: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 55: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.03\n",
      "Epoch 56: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 57: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.038\n",
      "Epoch 58: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.032\n",
      "Epoch 59: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 60: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.037 | tot_time = 0.039\n",
      "Epoch 61: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 62: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 63: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.033\n",
      "Epoch 64: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 65: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 66: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 67: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.034\n",
      "Epoch 68: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.035\n",
      "Epoch 69: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 70: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 71: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.035\n",
      "Epoch 72: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 73: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 74: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 75: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 76: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 77: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 78: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 79: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.031\n",
      "Epoch 80: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.03\n",
      "Epoch 81: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 82: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 83: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 84: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 85: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.034\n",
      "Epoch 86: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 87: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 88: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 89: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 90: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 91: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 92: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 93: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.034\n",
      "Epoch 94: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 95: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 96: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 97: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.035\n",
      "Epoch 98: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 99: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 100: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 101: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.017 | tot_time = 0.017\n",
      "Epoch 102: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 103: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 104: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.017 | tot_time = 0.017\n",
      "Epoch 105: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 106: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.036 | tot_time = 0.036\n",
      "Epoch 107: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 108: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 109: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 110: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.047\n",
      "Epoch 111: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.019 | tot_time = 0.019\n",
      "Epoch 112: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 113: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 114: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 115: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.02 | tot_time = 0.02\n",
      "Epoch 116: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.033\n",
      "Epoch 117: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 118: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 119: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 120: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 121: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 122: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.04\n",
      "Epoch 123: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 124: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.034\n",
      "Epoch 125: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.02 | tot_time = 0.02\n",
      "Epoch 126: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.039 | tot_time = 0.039\n",
      "Epoch 127: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 128: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 129: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 130: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 131: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 132: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 133: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 134: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 135: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 136: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.04 | tot_time = 0.042\n",
      "Epoch 137: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 138: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.029\n",
      "Epoch 139: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 140: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 141: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 142: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 143: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 144: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.034\n",
      "Epoch 145: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 146: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 147: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 148: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 149: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 150: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 151: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.033\n",
      "Epoch 152: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 153: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 154: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 155: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 156: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 157: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.035\n",
      "Epoch 158: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.014 | tot_time = 0.03\n",
      "Epoch 159: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.015 | tot_time = 0.031\n",
      "Epoch 160: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 161: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.039 | tot_time = 0.039\n",
      "Epoch 162: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.034\n",
      "Epoch 163: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 164: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 165: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.032\n",
      "Epoch 166: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 167: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 168: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 169: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 170: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 171: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 172: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.032\n",
      "Epoch 173: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 174: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 175: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 176: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.042\n",
      "Epoch 177: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 178: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 179: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.042 | tot_time = 0.042\n",
      "Epoch 180: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 181: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 182: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 183: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 184: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 185: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 186: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.037 | tot_time = 0.037\n",
      "Epoch 187: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 188: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 189: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.044 | tot_time = 0.047\n",
      "Epoch 190: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 191: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 192: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 193: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.037 | tot_time = 0.037\n",
      "Epoch 194: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 195: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 196: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 197: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 198: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 199: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.048\n",
      "Epoch 200: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 201: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 202: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 203: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.032\n",
      "Epoch 204: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 205: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 206: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 207: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 208: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.038\n",
      "Epoch 209: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 210: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 211: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 212: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 213: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 214: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 215: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 216: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 217: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 218: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 219: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 220: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 221: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 222: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 223: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 224: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 225: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 226: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 227: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 228: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 229: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 230: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.036\n",
      "Epoch 231: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 232: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 233: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.034\n",
      "Epoch 234: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 235: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 236: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 237: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.031\n",
      "Epoch 238: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 239: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 240: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 241: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.037\n",
      "Epoch 242: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 243: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 244: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 245: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 246: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 247: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 248: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 249: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 250: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 251: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 252: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.014 | tot_time = 0.029\n",
      "Epoch 253: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 254: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.045 | tot_time = 0.047\n",
      "Epoch 255: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.034\n",
      "Epoch 256: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 257: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.036\n",
      "Epoch 258: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 259: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 260: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 261: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 262: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 263: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.03\n",
      "Epoch 264: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 265: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 266: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 267: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 268: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 269: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 270: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 271: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.034\n",
      "Epoch 272: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 273: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 274: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.039\n",
      "Epoch 275: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.03\n",
      "Epoch 276: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 277: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 278: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.034\n",
      "Epoch 279: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 280: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 281: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 282: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 283: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.038\n",
      "Epoch 284: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 285: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 286: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 287: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.028\n",
      "Epoch 288: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 289: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 290: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.028\n",
      "Epoch 291: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 292: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.035\n",
      "Epoch 293: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.03\n",
      "Epoch 294: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 295: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.035\n",
      "Epoch 296: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.023 | tot_time = 0.03\n",
      "Epoch 297: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.031\n",
      "Epoch 298: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 299: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.032\n",
      "Epoch 300: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.028\n",
      "Epoch 301: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 302: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 303: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 304: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 305: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 306: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 307: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 308: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 309: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 310: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 311: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 312: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 313: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 314: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 315: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 316: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 317: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 318: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 319: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 320: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 321: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 322: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 323: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 324: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 325: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 326: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 327: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 328: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 329: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.029\n",
      "Epoch 330: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 331: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 332: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 333: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.034 | tot_time = 0.035\n",
      "Epoch 334: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 335: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 336: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 337: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 338: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 339: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 340: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 341: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 342: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 343: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 344: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 345: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.028\n",
      "Epoch 346: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 347: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 348: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 349: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 350: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.029\n",
      "Epoch 351: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 352: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 353: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 354: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 355: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 356: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 357: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 358: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 359: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 360: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 361: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 362: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 363: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 364: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 365: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 366: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 367: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 368: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 369: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 370: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 371: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.036\n",
      "Epoch 372: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 373: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 374: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 375: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 376: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 377: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 378: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 379: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 380: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 381: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 382: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 383: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 384: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 385: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 386: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 387: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 388: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 389: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 390: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 391: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 392: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.031\n",
      "Epoch 393: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 394: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 395: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.035\n",
      "Epoch 396: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 397: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 398: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 399: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 400: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 401: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 402: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 403: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.039 | tot_time = 0.039\n",
      "Epoch 404: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.018 | tot_time = 0.018\n",
      "Epoch 405: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 406: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.039\n",
      "Epoch 407: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.019 | tot_time = 0.019\n",
      "Epoch 408: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 409: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 410: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 411: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.043 | tot_time = 0.045\n",
      "Epoch 412: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 413: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.034\n",
      "Epoch 414: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 415: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 416: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 417: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 418: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 419: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.034\n",
      "Epoch 420: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.018 | tot_time = 0.018\n",
      "Epoch 421: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.033\n",
      "Epoch 422: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 423: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 424: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 425: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 426: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 427: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.033\n",
      "Epoch 428: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.021 | tot_time = 0.021\n",
      "Epoch 429: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 430: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 431: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 432: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.039 | tot_time = 0.039\n",
      "Epoch 433: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 434: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.031\n",
      "Epoch 435: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 436: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 437: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 438: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 439: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.03\n",
      "Epoch 440: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 441: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.03 | tot_time = 0.032\n",
      "Epoch 442: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.02 | tot_time = 0.02\n",
      "Epoch 443: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.036 | tot_time = 0.038\n",
      "Epoch 444: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.032\n",
      "Epoch 445: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.057 | tot_time = 0.057\n",
      "Epoch 446: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.047\n",
      "Epoch 447: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.019 | tot_time = 0.034\n",
      "Epoch 448: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 449: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.041 | tot_time = 0.041\n",
      "Epoch 450: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 451: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 452: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 453: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 454: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 455: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 456: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 457: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.032\n",
      "Epoch 458: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.038 | tot_time = 0.039\n",
      "Epoch 459: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.02 | tot_time = 0.02\n",
      "Epoch 460: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.036 | tot_time = 0.036\n",
      "Epoch 461: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.017 | tot_time = 0.017\n",
      "Epoch 462: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 463: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 464: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 465: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 466: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 467: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 468: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.037\n",
      "Epoch 469: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.028 | tot_time = 0.028\n",
      "Epoch 470: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 471: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.035 | tot_time = 0.035\n",
      "Epoch 472: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 473: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 474: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 475: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.025 | tot_time = 0.025\n",
      "Epoch 476: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.034\n",
      "Epoch 477: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.032\n",
      "Epoch 478: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.022 | tot_time = 0.022\n",
      "Epoch 479: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 480: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 481: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 482: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.033 | tot_time = 0.033\n",
      "Epoch 483: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 484: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.032 | tot_time = 0.033\n",
      "Epoch 485: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.024 | tot_time = 0.024\n",
      "Epoch 486: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 487: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 488: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 489: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.042 | tot_time = 0.042\n",
      "Epoch 490: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 491: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.032\n",
      "Epoch 492: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.026 | tot_time = 0.026\n",
      "Epoch 493: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.031 | tot_time = 0.031\n",
      "Epoch 494: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n",
      "Epoch 495: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 496: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.041 | tot_time = 0.041\n",
      "Epoch 497: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.016\n",
      "Epoch 498: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.029 | tot_time = 0.03\n",
      "Epoch 499: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.027 | tot_time = 0.027\n",
      "Epoch 500: loss = nan | acc = 0.097 | val_loss = nan | val_acc = 0.085 | train_time = 0.016 | tot_time = 0.031\n"
     ]
    }
   ],
   "source": [
    "modelMLP = MLP(x_train_flat,y_train,x_test_flat,y_test,loss_fn='cce')\n",
    "modelMLP.train(batch_size=32,epochs=500,lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loss\u001b[49m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(model\u001b[38;5;241m.\u001b[39mval_loss,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'train_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(model.train_loss,label=\"Train loss\")\n",
    "ax[0].plot(model.val_loss,label=\"Val loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(model.train_acc,label=\"Train acc\")\n",
    "ax[1].plot(model.val_acc,label=\"Val acc\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib.numpy_pickle_utils import xrange\n",
    "from numpy import *\n",
    "\n",
    "class binaryClassifierNeuron(): \n",
    "\tdef __init__(self): \n",
    "\t\t# Generate random numbers \n",
    "\t\trandom.seed(1)\n",
    "\t\tself.lr = 0.1 \n",
    "\n",
    "\t\t# Assign random weights to a 3 x 1 matrix, \n",
    "\t\tself.synaptic_weights = 2 * random.random((784, 1)) - 1\n",
    "\n",
    "\t# The Sigmoid function \n",
    "\tdef __sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + exp(-x)) \n",
    "\n",
    "\t# The derivative of the Sigmoid function. \n",
    "\t# This is the gradient of the Sigmoid curve. \n",
    "\tdef __sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\t# Train the neural network and adjust the weights each time. \n",
    "\tdef train(self, inputs, outputs, training_iterations): \n",
    "\t\tfor iteration in xrange(training_iterations): \n",
    "\t\t\t# Pass the training set through the network. \n",
    "\t\t\toutput = self.predict(inputs) \n",
    "\n",
    "\t\t\t# Calculate the error \n",
    "\t\t\terror = outputs - output \n",
    "\n",
    "\t\t\t# Adjust the weights by a factor \n",
    "\t\t\tfactor = dot(inputs.T, error * self.__sigmoid_derivative(output)) \n",
    "\t\t\tself.synaptic_weights += (factor * self.lr) \n",
    "\n",
    "\t\t# The neural network thinks. \n",
    "\n",
    "\tdef predict(self, inputs): \n",
    "\t\treturn self.__sigmoid(dot(inputs, self.synaptic_weights)) \n",
    "\n",
    "class multiClassifierNeuron():\n",
    "\tdef __init__(self): \n",
    "\t\t# Generate random numbers \n",
    "\t\trandom.seed(1)\n",
    "\t\tself.lr = 0.1 \n",
    "\n",
    "\t\t# Assign random weights to a 3 x 1 matrix, \n",
    "\t\tself.synaptic_weights = 2 * random.random((784, 1)) - 1\n",
    "\n",
    "\tdef __softmax(self, x):\n",
    "\t\te_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "\t\treturn e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\t\n",
    "\tdef __softmax_derivative(self, outputs):\n",
    "\t\treturn outputs * (1 - outputs)\n",
    "\t\n",
    "\tdef __categorical_cross_entropy(self, outputs, targets):\n",
    "\t\treturn -np.sum(targets * np.log(outputs + 1e-8), axis=1)\n",
    "\t\n",
    "\tdef train(self, inputs, targets, training_iterations):\n",
    "\t\tfor iteration in xrange(training_iterations):\n",
    "\t\t\t# Pass the training set through the network.\n",
    "\t\t\toutputs = self.predict(inputs)\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the error/loss\n",
    "\t\t\terror = self.__categorical_cross_entropy(outputs, targets)\n",
    "\t\t\t\n",
    "\t\t\t# Adjust the weights by a factor\n",
    "\t\t\tfactor = dot(inputs.T, (outputs - targets) * self.__softmax_derivative(outputs))\n",
    "\t\t\tself.synaptic_weights -= factor * self.lr\n",
    "\n",
    "\tdef predict(self, inputs): \n",
    "\t\treturn self.__softmax(dot(inputs, self.synaptic_weights)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t# Initialize \n",
    "# neural_network = neuron() \n",
    "\n",
    "# \t# The training set. \n",
    "# inputs = array([[0, 1, 1,0], [1, 0, 0,0], [1, 0, 1,0]]) \n",
    "# outputs = array([[1, 0, 1]]).T \n",
    "\n",
    "# \t# Train the neural network \n",
    "# neural_network.train(inputs, outputs, 1000) \n",
    "\n",
    "# \t# Test the neural network with a test example. \n",
    "# print(neural_network.predict([1,0,1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network...\n",
      "Testing the neural network...\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neuron class\n",
    "#neural_network = binaryClassifierNeuron()\n",
    "neural_network = multiClassifierNeuron()\n",
    "\n",
    "# Train the neural network\n",
    "print(\"Training the neural network...\")\n",
    "neural_network.train(x_train_flat, y_train.reshape(y_train.shape[0], 1), 10000)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Testing the neural network...\")\n",
    "predictions = neural_network.predict(x_test_flat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of 0 is [1.]\n",
      "prediction of 1 is [1.]\n",
      "prediction of 2 is [1.]\n",
      "prediction of 3 is [1.]\n",
      "prediction of 4 is [1.]\n",
      "prediction of 5 is [1.]\n",
      "prediction of 6 is [1.]\n",
      "prediction of 7 is [1.]\n",
      "prediction of 8 is [1.]\n",
      "prediction of 9 is [1.]\n",
      "prediction of 10 is [1.]\n",
      "prediction of 11 is [1.]\n",
      "prediction of 12 is [1.]\n",
      "prediction of 13 is [1.]\n",
      "prediction of 14 is [1.]\n",
      "prediction of 15 is [1.]\n",
      "prediction of 16 is [1.]\n",
      "prediction of 17 is [1.]\n",
      "prediction of 18 is [1.]\n",
      "prediction of 19 is [1.]\n",
      "prediction of 20 is [1.]\n",
      "prediction of 21 is [1.]\n",
      "prediction of 22 is [1.]\n",
      "prediction of 23 is [1.]\n",
      "prediction of 24 is [1.]\n",
      "prediction of 25 is [1.]\n",
      "prediction of 26 is [1.]\n",
      "prediction of 27 is [1.]\n",
      "prediction of 28 is [1.]\n",
      "prediction of 29 is [1.]\n",
      "prediction of 30 is [1.]\n",
      "prediction of 31 is [1.]\n",
      "prediction of 32 is [1.]\n",
      "prediction of 33 is [1.]\n",
      "prediction of 34 is [1.]\n",
      "prediction of 35 is [1.]\n",
      "prediction of 36 is [1.]\n",
      "prediction of 37 is [1.]\n",
      "prediction of 38 is [1.]\n",
      "prediction of 39 is [1.]\n",
      "prediction of 40 is [1.]\n",
      "prediction of 41 is [1.]\n",
      "prediction of 42 is [1.]\n",
      "prediction of 43 is [1.]\n",
      "prediction of 44 is [1.]\n",
      "prediction of 45 is [1.]\n",
      "prediction of 46 is [1.]\n",
      "prediction of 47 is [1.]\n",
      "prediction of 48 is [1.]\n",
      "prediction of 49 is [1.]\n",
      "prediction of 50 is [1.]\n",
      "prediction of 51 is [1.]\n",
      "prediction of 52 is [1.]\n",
      "prediction of 53 is [1.]\n",
      "prediction of 54 is [1.]\n",
      "prediction of 55 is [1.]\n",
      "prediction of 56 is [1.]\n",
      "prediction of 57 is [1.]\n",
      "prediction of 58 is [1.]\n",
      "prediction of 59 is [1.]\n",
      "prediction of 60 is [1.]\n",
      "prediction of 61 is [1.]\n",
      "prediction of 62 is [1.]\n",
      "prediction of 63 is [1.]\n",
      "prediction of 64 is [1.]\n",
      "prediction of 65 is [1.]\n",
      "prediction of 66 is [1.]\n",
      "prediction of 67 is [1.]\n",
      "prediction of 68 is [1.]\n",
      "prediction of 69 is [1.]\n",
      "prediction of 70 is [1.]\n",
      "prediction of 71 is [1.]\n",
      "prediction of 72 is [1.]\n",
      "prediction of 73 is [1.]\n",
      "prediction of 74 is [1.]\n",
      "prediction of 75 is [1.]\n",
      "prediction of 76 is [1.]\n",
      "prediction of 77 is [1.]\n",
      "prediction of 78 is [1.]\n",
      "prediction of 79 is [1.]\n",
      "prediction of 80 is [1.]\n",
      "prediction of 81 is [1.]\n",
      "prediction of 82 is [1.]\n",
      "prediction of 83 is [1.]\n",
      "prediction of 84 is [1.]\n",
      "prediction of 85 is [1.]\n",
      "prediction of 86 is [1.]\n",
      "prediction of 87 is [1.]\n",
      "prediction of 88 is [1.]\n",
      "prediction of 89 is [1.]\n",
      "prediction of 90 is [1.]\n",
      "prediction of 91 is [1.]\n",
      "prediction of 92 is [1.]\n",
      "prediction of 93 is [1.]\n",
      "prediction of 94 is [1.]\n",
      "prediction of 95 is [1.]\n",
      "prediction of 96 is [1.]\n",
      "prediction of 97 is [1.]\n",
      "prediction of 98 is [1.]\n",
      "prediction of 99 is [1.]\n",
      "prediction of 100 is [1.]\n",
      "prediction of 101 is [1.]\n",
      "prediction of 102 is [1.]\n",
      "prediction of 103 is [1.]\n",
      "prediction of 104 is [1.]\n",
      "prediction of 105 is [1.]\n",
      "prediction of 106 is [1.]\n",
      "prediction of 107 is [1.]\n",
      "prediction of 108 is [1.]\n",
      "prediction of 109 is [1.]\n",
      "prediction of 110 is [1.]\n",
      "prediction of 111 is [1.]\n",
      "prediction of 112 is [1.]\n",
      "prediction of 113 is [1.]\n",
      "prediction of 114 is [1.]\n",
      "prediction of 115 is [1.]\n",
      "prediction of 116 is [1.]\n",
      "prediction of 117 is [1.]\n",
      "prediction of 118 is [1.]\n",
      "prediction of 119 is [1.]\n",
      "prediction of 120 is [1.]\n",
      "prediction of 121 is [1.]\n",
      "prediction of 122 is [1.]\n",
      "prediction of 123 is [1.]\n",
      "prediction of 124 is [1.]\n",
      "prediction of 125 is [1.]\n",
      "prediction of 126 is [1.]\n",
      "prediction of 127 is [1.]\n",
      "prediction of 128 is [1.]\n",
      "prediction of 129 is [1.]\n",
      "prediction of 130 is [1.]\n",
      "prediction of 131 is [1.]\n",
      "prediction of 132 is [1.]\n",
      "prediction of 133 is [1.]\n",
      "prediction of 134 is [1.]\n",
      "prediction of 135 is [1.]\n",
      "prediction of 136 is [1.]\n",
      "prediction of 137 is [1.]\n",
      "prediction of 138 is [1.]\n",
      "prediction of 139 is [1.]\n",
      "prediction of 140 is [1.]\n",
      "prediction of 141 is [1.]\n",
      "prediction of 142 is [1.]\n",
      "prediction of 143 is [1.]\n",
      "prediction of 144 is [1.]\n",
      "prediction of 145 is [1.]\n",
      "prediction of 146 is [1.]\n",
      "prediction of 147 is [1.]\n",
      "prediction of 148 is [1.]\n",
      "prediction of 149 is [1.]\n",
      "prediction of 150 is [1.]\n",
      "prediction of 151 is [1.]\n",
      "prediction of 152 is [1.]\n",
      "prediction of 153 is [1.]\n",
      "prediction of 154 is [1.]\n",
      "prediction of 155 is [1.]\n",
      "prediction of 156 is [1.]\n",
      "prediction of 157 is [1.]\n",
      "prediction of 158 is [1.]\n",
      "prediction of 159 is [1.]\n",
      "prediction of 160 is [1.]\n",
      "prediction of 161 is [1.]\n",
      "prediction of 162 is [1.]\n",
      "prediction of 163 is [1.]\n",
      "prediction of 164 is [1.]\n",
      "prediction of 165 is [1.]\n",
      "prediction of 166 is [1.]\n",
      "prediction of 167 is [1.]\n",
      "prediction of 168 is [1.]\n",
      "prediction of 169 is [1.]\n",
      "prediction of 170 is [1.]\n",
      "prediction of 171 is [1.]\n",
      "prediction of 172 is [1.]\n",
      "prediction of 173 is [1.]\n",
      "prediction of 174 is [1.]\n",
      "prediction of 175 is [1.]\n",
      "prediction of 176 is [1.]\n",
      "prediction of 177 is [1.]\n",
      "prediction of 178 is [1.]\n",
      "prediction of 179 is [1.]\n",
      "prediction of 180 is [1.]\n",
      "prediction of 181 is [1.]\n",
      "prediction of 182 is [1.]\n",
      "prediction of 183 is [1.]\n",
      "prediction of 184 is [1.]\n",
      "prediction of 185 is [1.]\n",
      "prediction of 186 is [1.]\n",
      "prediction of 187 is [1.]\n",
      "prediction of 188 is [1.]\n",
      "prediction of 189 is [1.]\n",
      "prediction of 190 is [1.]\n",
      "prediction of 191 is [1.]\n",
      "prediction of 192 is [1.]\n",
      "prediction of 193 is [1.]\n",
      "prediction of 194 is [1.]\n",
      "prediction of 195 is [1.]\n",
      "prediction of 196 is [1.]\n",
      "prediction of 197 is [1.]\n",
      "prediction of 198 is [1.]\n",
      "prediction of 199 is [1.]\n",
      "prediction of 200 is [1.]\n",
      "prediction of 201 is [1.]\n",
      "prediction of 202 is [1.]\n",
      "prediction of 203 is [1.]\n",
      "prediction of 204 is [1.]\n",
      "prediction of 205 is [1.]\n",
      "prediction of 206 is [1.]\n",
      "prediction of 207 is [1.]\n",
      "prediction of 208 is [1.]\n",
      "prediction of 209 is [1.]\n",
      "prediction of 210 is [1.]\n",
      "prediction of 211 is [1.]\n",
      "prediction of 212 is [1.]\n",
      "prediction of 213 is [1.]\n",
      "prediction of 214 is [1.]\n",
      "prediction of 215 is [1.]\n",
      "prediction of 216 is [1.]\n",
      "prediction of 217 is [1.]\n",
      "prediction of 218 is [1.]\n",
      "prediction of 219 is [1.]\n",
      "prediction of 220 is [1.]\n",
      "prediction of 221 is [1.]\n",
      "prediction of 222 is [1.]\n",
      "prediction of 223 is [1.]\n",
      "prediction of 224 is [1.]\n",
      "prediction of 225 is [1.]\n",
      "prediction of 226 is [1.]\n",
      "prediction of 227 is [1.]\n",
      "prediction of 228 is [1.]\n",
      "prediction of 229 is [1.]\n",
      "prediction of 230 is [1.]\n",
      "prediction of 231 is [1.]\n",
      "prediction of 232 is [1.]\n",
      "prediction of 233 is [1.]\n",
      "prediction of 234 is [1.]\n",
      "prediction of 235 is [1.]\n",
      "prediction of 236 is [1.]\n",
      "prediction of 237 is [1.]\n",
      "prediction of 238 is [1.]\n",
      "prediction of 239 is [1.]\n",
      "prediction of 240 is [1.]\n",
      "prediction of 241 is [1.]\n",
      "prediction of 242 is [1.]\n",
      "prediction of 243 is [1.]\n",
      "prediction of 244 is [1.]\n",
      "prediction of 245 is [1.]\n",
      "prediction of 246 is [1.]\n",
      "prediction of 247 is [1.]\n",
      "prediction of 248 is [1.]\n",
      "prediction of 249 is [1.]\n",
      "prediction of 250 is [1.]\n",
      "prediction of 251 is [1.]\n",
      "prediction of 252 is [1.]\n",
      "prediction of 253 is [1.]\n",
      "prediction of 254 is [1.]\n",
      "prediction of 255 is [1.]\n",
      "prediction of 256 is [1.]\n",
      "prediction of 257 is [1.]\n",
      "prediction of 258 is [1.]\n",
      "prediction of 259 is [1.]\n",
      "prediction of 260 is [1.]\n",
      "prediction of 261 is [1.]\n",
      "prediction of 262 is [1.]\n",
      "prediction of 263 is [1.]\n",
      "prediction of 264 is [1.]\n",
      "prediction of 265 is [1.]\n",
      "prediction of 266 is [1.]\n",
      "prediction of 267 is [1.]\n",
      "prediction of 268 is [1.]\n",
      "prediction of 269 is [1.]\n",
      "prediction of 270 is [1.]\n",
      "prediction of 271 is [1.]\n",
      "prediction of 272 is [1.]\n",
      "prediction of 273 is [1.]\n",
      "prediction of 274 is [1.]\n",
      "prediction of 275 is [1.]\n",
      "prediction of 276 is [1.]\n",
      "prediction of 277 is [1.]\n",
      "prediction of 278 is [1.]\n",
      "prediction of 279 is [1.]\n",
      "prediction of 280 is [1.]\n",
      "prediction of 281 is [1.]\n",
      "prediction of 282 is [1.]\n",
      "prediction of 283 is [1.]\n",
      "prediction of 284 is [1.]\n",
      "prediction of 285 is [1.]\n",
      "prediction of 286 is [1.]\n",
      "prediction of 287 is [1.]\n",
      "prediction of 288 is [1.]\n",
      "prediction of 289 is [1.]\n",
      "prediction of 290 is [1.]\n",
      "prediction of 291 is [1.]\n",
      "prediction of 292 is [1.]\n",
      "prediction of 293 is [1.]\n",
      "prediction of 294 is [1.]\n",
      "prediction of 295 is [1.]\n",
      "prediction of 296 is [1.]\n",
      "prediction of 297 is [1.]\n",
      "prediction of 298 is [1.]\n",
      "prediction of 299 is [1.]\n",
      "prediction of 300 is [1.]\n",
      "prediction of 301 is [1.]\n",
      "prediction of 302 is [1.]\n",
      "prediction of 303 is [1.]\n",
      "prediction of 304 is [1.]\n",
      "prediction of 305 is [1.]\n",
      "prediction of 306 is [1.]\n",
      "prediction of 307 is [1.]\n",
      "prediction of 308 is [1.]\n",
      "prediction of 309 is [1.]\n",
      "prediction of 310 is [1.]\n",
      "prediction of 311 is [1.]\n",
      "prediction of 312 is [1.]\n",
      "prediction of 313 is [1.]\n",
      "prediction of 314 is [1.]\n",
      "prediction of 315 is [1.]\n",
      "prediction of 316 is [1.]\n",
      "prediction of 317 is [1.]\n",
      "prediction of 318 is [1.]\n",
      "prediction of 319 is [1.]\n",
      "prediction of 320 is [1.]\n",
      "prediction of 321 is [1.]\n",
      "prediction of 322 is [1.]\n",
      "prediction of 323 is [1.]\n",
      "prediction of 324 is [1.]\n",
      "prediction of 325 is [1.]\n",
      "prediction of 326 is [1.]\n",
      "prediction of 327 is [1.]\n",
      "prediction of 328 is [1.]\n",
      "prediction of 329 is [1.]\n",
      "prediction of 330 is [1.]\n",
      "prediction of 331 is [1.]\n",
      "prediction of 332 is [1.]\n",
      "prediction of 333 is [1.]\n",
      "prediction of 334 is [1.]\n",
      "prediction of 335 is [1.]\n",
      "prediction of 336 is [1.]\n",
      "prediction of 337 is [1.]\n",
      "prediction of 338 is [1.]\n",
      "prediction of 339 is [1.]\n",
      "prediction of 340 is [1.]\n",
      "prediction of 341 is [1.]\n",
      "prediction of 342 is [1.]\n",
      "prediction of 343 is [1.]\n",
      "prediction of 344 is [1.]\n",
      "prediction of 345 is [1.]\n",
      "prediction of 346 is [1.]\n",
      "prediction of 347 is [1.]\n",
      "prediction of 348 is [1.]\n",
      "prediction of 349 is [1.]\n",
      "prediction of 350 is [1.]\n",
      "prediction of 351 is [1.]\n",
      "prediction of 352 is [1.]\n",
      "prediction of 353 is [1.]\n",
      "prediction of 354 is [1.]\n",
      "prediction of 355 is [1.]\n",
      "prediction of 356 is [1.]\n",
      "prediction of 357 is [1.]\n",
      "prediction of 358 is [1.]\n",
      "prediction of 359 is [1.]\n",
      "prediction of 360 is [1.]\n",
      "prediction of 361 is [1.]\n",
      "prediction of 362 is [1.]\n",
      "prediction of 363 is [1.]\n",
      "prediction of 364 is [1.]\n",
      "prediction of 365 is [1.]\n",
      "prediction of 366 is [1.]\n",
      "prediction of 367 is [1.]\n",
      "prediction of 368 is [1.]\n",
      "prediction of 369 is [1.]\n",
      "prediction of 370 is [1.]\n",
      "prediction of 371 is [1.]\n",
      "prediction of 372 is [1.]\n",
      "prediction of 373 is [1.]\n",
      "prediction of 374 is [1.]\n",
      "prediction of 375 is [1.]\n",
      "prediction of 376 is [1.]\n",
      "prediction of 377 is [1.]\n",
      "prediction of 378 is [1.]\n",
      "prediction of 379 is [1.]\n",
      "prediction of 380 is [1.]\n",
      "prediction of 381 is [1.]\n",
      "prediction of 382 is [1.]\n",
      "prediction of 383 is [1.]\n",
      "prediction of 384 is [1.]\n",
      "prediction of 385 is [1.]\n",
      "prediction of 386 is [1.]\n",
      "prediction of 387 is [1.]\n",
      "prediction of 388 is [1.]\n",
      "prediction of 389 is [1.]\n",
      "prediction of 390 is [1.]\n",
      "prediction of 391 is [1.]\n",
      "prediction of 392 is [1.]\n",
      "prediction of 393 is [1.]\n",
      "prediction of 394 is [1.]\n",
      "prediction of 395 is [1.]\n",
      "prediction of 396 is [1.]\n",
      "prediction of 397 is [1.]\n",
      "prediction of 398 is [1.]\n",
      "prediction of 399 is [1.]\n",
      "prediction of 400 is [1.]\n",
      "prediction of 401 is [1.]\n",
      "prediction of 402 is [1.]\n",
      "prediction of 403 is [1.]\n",
      "prediction of 404 is [1.]\n",
      "prediction of 405 is [1.]\n",
      "prediction of 406 is [1.]\n",
      "prediction of 407 is [1.]\n",
      "prediction of 408 is [1.]\n",
      "prediction of 409 is [1.]\n",
      "prediction of 410 is [1.]\n",
      "prediction of 411 is [1.]\n",
      "prediction of 412 is [1.]\n",
      "prediction of 413 is [1.]\n",
      "prediction of 414 is [1.]\n",
      "prediction of 415 is [1.]\n",
      "prediction of 416 is [1.]\n",
      "prediction of 417 is [1.]\n",
      "prediction of 418 is [1.]\n",
      "prediction of 419 is [1.]\n",
      "prediction of 420 is [1.]\n",
      "prediction of 421 is [1.]\n",
      "prediction of 422 is [1.]\n",
      "prediction of 423 is [1.]\n",
      "prediction of 424 is [1.]\n",
      "prediction of 425 is [1.]\n",
      "prediction of 426 is [1.]\n",
      "prediction of 427 is [1.]\n",
      "prediction of 428 is [1.]\n",
      "prediction of 429 is [1.]\n",
      "prediction of 430 is [1.]\n",
      "prediction of 431 is [1.]\n",
      "prediction of 432 is [1.]\n",
      "prediction of 433 is [1.]\n",
      "prediction of 434 is [1.]\n",
      "prediction of 435 is [1.]\n",
      "prediction of 436 is [1.]\n",
      "prediction of 437 is [1.]\n",
      "prediction of 438 is [1.]\n",
      "prediction of 439 is [1.]\n",
      "prediction of 440 is [1.]\n",
      "prediction of 441 is [1.]\n",
      "prediction of 442 is [1.]\n",
      "prediction of 443 is [1.]\n",
      "prediction of 444 is [1.]\n",
      "prediction of 445 is [1.]\n",
      "prediction of 446 is [1.]\n",
      "prediction of 447 is [1.]\n",
      "prediction of 448 is [1.]\n",
      "prediction of 449 is [1.]\n",
      "prediction of 450 is [1.]\n",
      "prediction of 451 is [1.]\n",
      "prediction of 452 is [1.]\n",
      "prediction of 453 is [1.]\n",
      "prediction of 454 is [1.]\n",
      "prediction of 455 is [1.]\n",
      "prediction of 456 is [1.]\n",
      "prediction of 457 is [1.]\n",
      "prediction of 458 is [1.]\n",
      "prediction of 459 is [1.]\n",
      "prediction of 460 is [1.]\n",
      "prediction of 461 is [1.]\n",
      "prediction of 462 is [1.]\n",
      "prediction of 463 is [1.]\n",
      "prediction of 464 is [1.]\n",
      "prediction of 465 is [1.]\n",
      "prediction of 466 is [1.]\n",
      "prediction of 467 is [1.]\n",
      "prediction of 468 is [1.]\n",
      "prediction of 469 is [1.]\n",
      "prediction of 470 is [1.]\n",
      "prediction of 471 is [1.]\n",
      "prediction of 472 is [1.]\n",
      "prediction of 473 is [1.]\n",
      "prediction of 474 is [1.]\n",
      "prediction of 475 is [1.]\n",
      "prediction of 476 is [1.]\n",
      "prediction of 477 is [1.]\n",
      "prediction of 478 is [1.]\n",
      "prediction of 479 is [1.]\n",
      "prediction of 480 is [1.]\n",
      "prediction of 481 is [1.]\n",
      "prediction of 482 is [1.]\n",
      "prediction of 483 is [1.]\n",
      "prediction of 484 is [1.]\n",
      "prediction of 485 is [1.]\n",
      "prediction of 486 is [1.]\n",
      "prediction of 487 is [1.]\n",
      "prediction of 488 is [1.]\n",
      "prediction of 489 is [1.]\n",
      "prediction of 490 is [1.]\n",
      "prediction of 491 is [1.]\n",
      "prediction of 492 is [1.]\n",
      "prediction of 493 is [1.]\n",
      "prediction of 494 is [1.]\n",
      "prediction of 495 is [1.]\n",
      "prediction of 496 is [1.]\n",
      "prediction of 497 is [1.]\n",
      "prediction of 498 is [1.]\n",
      "prediction of 499 is [1.]\n",
      "prediction of 500 is [1.]\n",
      "prediction of 501 is [1.]\n",
      "prediction of 502 is [1.]\n",
      "prediction of 503 is [1.]\n",
      "prediction of 504 is [1.]\n",
      "prediction of 505 is [1.]\n",
      "prediction of 506 is [1.]\n",
      "prediction of 507 is [1.]\n",
      "prediction of 508 is [1.]\n",
      "prediction of 509 is [1.]\n",
      "prediction of 510 is [1.]\n",
      "prediction of 511 is [1.]\n",
      "prediction of 512 is [1.]\n",
      "prediction of 513 is [1.]\n",
      "prediction of 514 is [1.]\n",
      "prediction of 515 is [1.]\n",
      "prediction of 516 is [1.]\n",
      "prediction of 517 is [1.]\n",
      "prediction of 518 is [1.]\n",
      "prediction of 519 is [1.]\n",
      "prediction of 520 is [1.]\n",
      "prediction of 521 is [1.]\n",
      "prediction of 522 is [1.]\n",
      "prediction of 523 is [1.]\n",
      "prediction of 524 is [1.]\n",
      "prediction of 525 is [1.]\n",
      "prediction of 526 is [1.]\n",
      "prediction of 527 is [1.]\n",
      "prediction of 528 is [1.]\n",
      "prediction of 529 is [1.]\n",
      "prediction of 530 is [1.]\n",
      "prediction of 531 is [1.]\n",
      "prediction of 532 is [1.]\n",
      "prediction of 533 is [1.]\n",
      "prediction of 534 is [1.]\n",
      "prediction of 535 is [1.]\n",
      "prediction of 536 is [1.]\n",
      "prediction of 537 is [1.]\n",
      "prediction of 538 is [1.]\n",
      "prediction of 539 is [1.]\n",
      "prediction of 540 is [1.]\n",
      "prediction of 541 is [1.]\n",
      "prediction of 542 is [1.]\n",
      "prediction of 543 is [1.]\n",
      "prediction of 544 is [1.]\n",
      "prediction of 545 is [1.]\n",
      "prediction of 546 is [1.]\n",
      "prediction of 547 is [1.]\n",
      "prediction of 548 is [1.]\n",
      "prediction of 549 is [1.]\n",
      "prediction of 550 is [1.]\n",
      "prediction of 551 is [1.]\n",
      "prediction of 552 is [1.]\n",
      "prediction of 553 is [1.]\n",
      "prediction of 554 is [1.]\n",
      "prediction of 555 is [1.]\n",
      "prediction of 556 is [1.]\n",
      "prediction of 557 is [1.]\n",
      "prediction of 558 is [1.]\n",
      "prediction of 559 is [1.]\n",
      "prediction of 560 is [1.]\n",
      "prediction of 561 is [1.]\n",
      "prediction of 562 is [1.]\n",
      "prediction of 563 is [1.]\n",
      "prediction of 564 is [1.]\n",
      "prediction of 565 is [1.]\n",
      "prediction of 566 is [1.]\n",
      "prediction of 567 is [1.]\n",
      "prediction of 568 is [1.]\n",
      "prediction of 569 is [1.]\n",
      "prediction of 570 is [1.]\n",
      "prediction of 571 is [1.]\n",
      "prediction of 572 is [1.]\n",
      "prediction of 573 is [1.]\n",
      "prediction of 574 is [1.]\n",
      "prediction of 575 is [1.]\n",
      "prediction of 576 is [1.]\n",
      "prediction of 577 is [1.]\n",
      "prediction of 578 is [1.]\n",
      "prediction of 579 is [1.]\n",
      "prediction of 580 is [1.]\n",
      "prediction of 581 is [1.]\n",
      "prediction of 582 is [1.]\n",
      "prediction of 583 is [1.]\n",
      "prediction of 584 is [1.]\n",
      "prediction of 585 is [1.]\n",
      "prediction of 586 is [1.]\n",
      "prediction of 587 is [1.]\n",
      "prediction of 588 is [1.]\n",
      "prediction of 589 is [1.]\n",
      "prediction of 590 is [1.]\n",
      "prediction of 591 is [1.]\n",
      "prediction of 592 is [1.]\n",
      "prediction of 593 is [1.]\n",
      "prediction of 594 is [1.]\n",
      "prediction of 595 is [1.]\n",
      "prediction of 596 is [1.]\n",
      "prediction of 597 is [1.]\n",
      "prediction of 598 is [1.]\n",
      "prediction of 599 is [1.]\n",
      "prediction of 600 is [1.]\n",
      "prediction of 601 is [1.]\n",
      "prediction of 602 is [1.]\n",
      "prediction of 603 is [1.]\n",
      "prediction of 604 is [1.]\n",
      "prediction of 605 is [1.]\n",
      "prediction of 606 is [1.]\n",
      "prediction of 607 is [1.]\n",
      "prediction of 608 is [1.]\n",
      "prediction of 609 is [1.]\n",
      "prediction of 610 is [1.]\n",
      "prediction of 611 is [1.]\n",
      "prediction of 612 is [1.]\n",
      "prediction of 613 is [1.]\n",
      "prediction of 614 is [1.]\n",
      "prediction of 615 is [1.]\n",
      "prediction of 616 is [1.]\n",
      "prediction of 617 is [1.]\n",
      "prediction of 618 is [1.]\n",
      "prediction of 619 is [1.]\n",
      "prediction of 620 is [1.]\n",
      "prediction of 621 is [1.]\n",
      "prediction of 622 is [1.]\n",
      "prediction of 623 is [1.]\n",
      "prediction of 624 is [1.]\n",
      "prediction of 625 is [1.]\n",
      "prediction of 626 is [1.]\n",
      "prediction of 627 is [1.]\n",
      "prediction of 628 is [1.]\n",
      "prediction of 629 is [1.]\n",
      "prediction of 630 is [1.]\n",
      "prediction of 631 is [1.]\n",
      "prediction of 632 is [1.]\n",
      "prediction of 633 is [1.]\n",
      "prediction of 634 is [1.]\n",
      "prediction of 635 is [1.]\n",
      "prediction of 636 is [1.]\n",
      "prediction of 637 is [1.]\n",
      "prediction of 638 is [1.]\n",
      "prediction of 639 is [1.]\n",
      "prediction of 640 is [1.]\n",
      "prediction of 641 is [1.]\n",
      "prediction of 642 is [1.]\n",
      "prediction of 643 is [1.]\n",
      "prediction of 644 is [1.]\n",
      "prediction of 645 is [1.]\n",
      "prediction of 646 is [1.]\n",
      "prediction of 647 is [1.]\n",
      "prediction of 648 is [1.]\n",
      "prediction of 649 is [1.]\n",
      "prediction of 650 is [1.]\n",
      "prediction of 651 is [1.]\n",
      "prediction of 652 is [1.]\n",
      "prediction of 653 is [1.]\n",
      "prediction of 654 is [1.]\n",
      "prediction of 655 is [1.]\n",
      "prediction of 656 is [1.]\n",
      "prediction of 657 is [1.]\n",
      "prediction of 658 is [1.]\n",
      "prediction of 659 is [1.]\n",
      "prediction of 660 is [1.]\n",
      "prediction of 661 is [1.]\n",
      "prediction of 662 is [1.]\n",
      "prediction of 663 is [1.]\n",
      "prediction of 664 is [1.]\n",
      "prediction of 665 is [1.]\n",
      "prediction of 666 is [1.]\n",
      "prediction of 667 is [1.]\n",
      "prediction of 668 is [1.]\n",
      "prediction of 669 is [1.]\n",
      "prediction of 670 is [1.]\n",
      "prediction of 671 is [1.]\n",
      "prediction of 672 is [1.]\n",
      "prediction of 673 is [1.]\n",
      "prediction of 674 is [1.]\n",
      "prediction of 675 is [1.]\n",
      "prediction of 676 is [1.]\n",
      "prediction of 677 is [1.]\n",
      "prediction of 678 is [1.]\n",
      "prediction of 679 is [1.]\n",
      "prediction of 680 is [1.]\n",
      "prediction of 681 is [1.]\n",
      "prediction of 682 is [1.]\n",
      "prediction of 683 is [1.]\n",
      "prediction of 684 is [1.]\n",
      "prediction of 685 is [1.]\n",
      "prediction of 686 is [1.]\n",
      "prediction of 687 is [1.]\n",
      "prediction of 688 is [1.]\n",
      "prediction of 689 is [1.]\n",
      "prediction of 690 is [1.]\n",
      "prediction of 691 is [1.]\n",
      "prediction of 692 is [1.]\n",
      "prediction of 693 is [1.]\n",
      "prediction of 694 is [1.]\n",
      "prediction of 695 is [1.]\n",
      "prediction of 696 is [1.]\n",
      "prediction of 697 is [1.]\n",
      "prediction of 698 is [1.]\n",
      "prediction of 699 is [1.]\n",
      "prediction of 700 is [1.]\n",
      "prediction of 701 is [1.]\n",
      "prediction of 702 is [1.]\n",
      "prediction of 703 is [1.]\n",
      "prediction of 704 is [1.]\n",
      "prediction of 705 is [1.]\n",
      "prediction of 706 is [1.]\n",
      "prediction of 707 is [1.]\n",
      "prediction of 708 is [1.]\n",
      "prediction of 709 is [1.]\n",
      "prediction of 710 is [1.]\n",
      "prediction of 711 is [1.]\n",
      "prediction of 712 is [1.]\n",
      "prediction of 713 is [1.]\n",
      "prediction of 714 is [1.]\n",
      "prediction of 715 is [1.]\n",
      "prediction of 716 is [1.]\n",
      "prediction of 717 is [1.]\n",
      "prediction of 718 is [1.]\n",
      "prediction of 719 is [1.]\n",
      "prediction of 720 is [1.]\n",
      "prediction of 721 is [1.]\n",
      "prediction of 722 is [1.]\n",
      "prediction of 723 is [1.]\n",
      "prediction of 724 is [1.]\n",
      "prediction of 725 is [1.]\n",
      "prediction of 726 is [1.]\n",
      "prediction of 727 is [1.]\n",
      "prediction of 728 is [1.]\n",
      "prediction of 729 is [1.]\n",
      "prediction of 730 is [1.]\n",
      "prediction of 731 is [1.]\n",
      "prediction of 732 is [1.]\n",
      "prediction of 733 is [1.]\n",
      "prediction of 734 is [1.]\n",
      "prediction of 735 is [1.]\n",
      "prediction of 736 is [1.]\n",
      "prediction of 737 is [1.]\n",
      "prediction of 738 is [1.]\n",
      "prediction of 739 is [1.]\n",
      "prediction of 740 is [1.]\n",
      "prediction of 741 is [1.]\n",
      "prediction of 742 is [1.]\n",
      "prediction of 743 is [1.]\n",
      "prediction of 744 is [1.]\n",
      "prediction of 745 is [1.]\n",
      "prediction of 746 is [1.]\n",
      "prediction of 747 is [1.]\n",
      "prediction of 748 is [1.]\n",
      "prediction of 749 is [1.]\n",
      "prediction of 750 is [1.]\n",
      "prediction of 751 is [1.]\n",
      "prediction of 752 is [1.]\n",
      "prediction of 753 is [1.]\n",
      "prediction of 754 is [1.]\n",
      "prediction of 755 is [1.]\n",
      "prediction of 756 is [1.]\n",
      "prediction of 757 is [1.]\n",
      "prediction of 758 is [1.]\n",
      "prediction of 759 is [1.]\n",
      "prediction of 760 is [1.]\n",
      "prediction of 761 is [1.]\n",
      "prediction of 762 is [1.]\n",
      "prediction of 763 is [1.]\n",
      "prediction of 764 is [1.]\n",
      "prediction of 765 is [1.]\n",
      "prediction of 766 is [1.]\n",
      "prediction of 767 is [1.]\n",
      "prediction of 768 is [1.]\n",
      "prediction of 769 is [1.]\n",
      "prediction of 770 is [1.]\n",
      "prediction of 771 is [1.]\n",
      "prediction of 772 is [1.]\n",
      "prediction of 773 is [1.]\n",
      "prediction of 774 is [1.]\n",
      "prediction of 775 is [1.]\n",
      "prediction of 776 is [1.]\n",
      "prediction of 777 is [1.]\n",
      "prediction of 778 is [1.]\n",
      "prediction of 779 is [1.]\n",
      "prediction of 780 is [1.]\n",
      "prediction of 781 is [1.]\n",
      "prediction of 782 is [1.]\n",
      "prediction of 783 is [1.]\n",
      "prediction of 784 is [1.]\n",
      "prediction of 785 is [1.]\n",
      "prediction of 786 is [1.]\n",
      "prediction of 787 is [1.]\n",
      "prediction of 788 is [1.]\n",
      "prediction of 789 is [1.]\n",
      "prediction of 790 is [1.]\n",
      "prediction of 791 is [1.]\n",
      "prediction of 792 is [1.]\n",
      "prediction of 793 is [1.]\n",
      "prediction of 794 is [1.]\n",
      "prediction of 795 is [1.]\n",
      "prediction of 796 is [1.]\n",
      "prediction of 797 is [1.]\n",
      "prediction of 798 is [1.]\n",
      "prediction of 799 is [1.]\n",
      "prediction of 800 is [1.]\n",
      "prediction of 801 is [1.]\n",
      "prediction of 802 is [1.]\n",
      "prediction of 803 is [1.]\n",
      "prediction of 804 is [1.]\n",
      "prediction of 805 is [1.]\n",
      "prediction of 806 is [1.]\n",
      "prediction of 807 is [1.]\n",
      "prediction of 808 is [1.]\n",
      "prediction of 809 is [1.]\n",
      "prediction of 810 is [1.]\n",
      "prediction of 811 is [1.]\n",
      "prediction of 812 is [1.]\n",
      "prediction of 813 is [1.]\n",
      "prediction of 814 is [1.]\n",
      "prediction of 815 is [1.]\n",
      "prediction of 816 is [1.]\n",
      "prediction of 817 is [1.]\n",
      "prediction of 818 is [1.]\n",
      "prediction of 819 is [1.]\n",
      "prediction of 820 is [1.]\n",
      "prediction of 821 is [1.]\n",
      "prediction of 822 is [1.]\n",
      "prediction of 823 is [1.]\n",
      "prediction of 824 is [1.]\n",
      "prediction of 825 is [1.]\n",
      "prediction of 826 is [1.]\n",
      "prediction of 827 is [1.]\n",
      "prediction of 828 is [1.]\n",
      "prediction of 829 is [1.]\n",
      "prediction of 830 is [1.]\n",
      "prediction of 831 is [1.]\n",
      "prediction of 832 is [1.]\n",
      "prediction of 833 is [1.]\n",
      "prediction of 834 is [1.]\n",
      "prediction of 835 is [1.]\n",
      "prediction of 836 is [1.]\n",
      "prediction of 837 is [1.]\n",
      "prediction of 838 is [1.]\n",
      "prediction of 839 is [1.]\n",
      "prediction of 840 is [1.]\n",
      "prediction of 841 is [1.]\n",
      "prediction of 842 is [1.]\n",
      "prediction of 843 is [1.]\n",
      "prediction of 844 is [1.]\n",
      "prediction of 845 is [1.]\n",
      "prediction of 846 is [1.]\n",
      "prediction of 847 is [1.]\n",
      "prediction of 848 is [1.]\n",
      "prediction of 849 is [1.]\n",
      "prediction of 850 is [1.]\n",
      "prediction of 851 is [1.]\n",
      "prediction of 852 is [1.]\n",
      "prediction of 853 is [1.]\n",
      "prediction of 854 is [1.]\n",
      "prediction of 855 is [1.]\n",
      "prediction of 856 is [1.]\n",
      "prediction of 857 is [1.]\n",
      "prediction of 858 is [1.]\n",
      "prediction of 859 is [1.]\n",
      "prediction of 860 is [1.]\n",
      "prediction of 861 is [1.]\n",
      "prediction of 862 is [1.]\n",
      "prediction of 863 is [1.]\n",
      "prediction of 864 is [1.]\n",
      "prediction of 865 is [1.]\n",
      "prediction of 866 is [1.]\n",
      "prediction of 867 is [1.]\n",
      "prediction of 868 is [1.]\n",
      "prediction of 869 is [1.]\n",
      "prediction of 870 is [1.]\n",
      "prediction of 871 is [1.]\n",
      "prediction of 872 is [1.]\n",
      "prediction of 873 is [1.]\n",
      "prediction of 874 is [1.]\n",
      "prediction of 875 is [1.]\n",
      "prediction of 876 is [1.]\n",
      "prediction of 877 is [1.]\n",
      "prediction of 878 is [1.]\n",
      "prediction of 879 is [1.]\n",
      "prediction of 880 is [1.]\n",
      "prediction of 881 is [1.]\n",
      "prediction of 882 is [1.]\n",
      "prediction of 883 is [1.]\n",
      "prediction of 884 is [1.]\n",
      "prediction of 885 is [1.]\n",
      "prediction of 886 is [1.]\n",
      "prediction of 887 is [1.]\n",
      "prediction of 888 is [1.]\n",
      "prediction of 889 is [1.]\n",
      "prediction of 890 is [1.]\n",
      "prediction of 891 is [1.]\n",
      "prediction of 892 is [1.]\n",
      "prediction of 893 is [1.]\n",
      "prediction of 894 is [1.]\n",
      "prediction of 895 is [1.]\n",
      "prediction of 896 is [1.]\n",
      "prediction of 897 is [1.]\n",
      "prediction of 898 is [1.]\n",
      "prediction of 899 is [1.]\n",
      "prediction of 900 is [1.]\n",
      "prediction of 901 is [1.]\n",
      "prediction of 902 is [1.]\n",
      "prediction of 903 is [1.]\n",
      "prediction of 904 is [1.]\n",
      "prediction of 905 is [1.]\n",
      "prediction of 906 is [1.]\n",
      "prediction of 907 is [1.]\n",
      "prediction of 908 is [1.]\n",
      "prediction of 909 is [1.]\n",
      "prediction of 910 is [1.]\n",
      "prediction of 911 is [1.]\n",
      "prediction of 912 is [1.]\n",
      "prediction of 913 is [1.]\n",
      "prediction of 914 is [1.]\n",
      "prediction of 915 is [1.]\n",
      "prediction of 916 is [1.]\n",
      "prediction of 917 is [1.]\n",
      "prediction of 918 is [1.]\n",
      "prediction of 919 is [1.]\n",
      "prediction of 920 is [1.]\n",
      "prediction of 921 is [1.]\n",
      "prediction of 922 is [1.]\n",
      "prediction of 923 is [1.]\n",
      "prediction of 924 is [1.]\n",
      "prediction of 925 is [1.]\n",
      "prediction of 926 is [1.]\n",
      "prediction of 927 is [1.]\n",
      "prediction of 928 is [1.]\n",
      "prediction of 929 is [1.]\n",
      "prediction of 930 is [1.]\n",
      "prediction of 931 is [1.]\n",
      "prediction of 932 is [1.]\n",
      "prediction of 933 is [1.]\n",
      "prediction of 934 is [1.]\n",
      "prediction of 935 is [1.]\n",
      "prediction of 936 is [1.]\n",
      "prediction of 937 is [1.]\n",
      "prediction of 938 is [1.]\n",
      "prediction of 939 is [1.]\n",
      "prediction of 940 is [1.]\n",
      "prediction of 941 is [1.]\n",
      "prediction of 942 is [1.]\n",
      "prediction of 943 is [1.]\n",
      "prediction of 944 is [1.]\n",
      "prediction of 945 is [1.]\n",
      "prediction of 946 is [1.]\n",
      "prediction of 947 is [1.]\n",
      "prediction of 948 is [1.]\n",
      "prediction of 949 is [1.]\n",
      "prediction of 950 is [1.]\n",
      "prediction of 951 is [1.]\n",
      "prediction of 952 is [1.]\n",
      "prediction of 953 is [1.]\n",
      "prediction of 954 is [1.]\n",
      "prediction of 955 is [1.]\n",
      "prediction of 956 is [1.]\n",
      "prediction of 957 is [1.]\n",
      "prediction of 958 is [1.]\n",
      "prediction of 959 is [1.]\n",
      "prediction of 960 is [1.]\n",
      "prediction of 961 is [1.]\n",
      "prediction of 962 is [1.]\n",
      "prediction of 963 is [1.]\n",
      "prediction of 964 is [1.]\n",
      "prediction of 965 is [1.]\n",
      "prediction of 966 is [1.]\n",
      "prediction of 967 is [1.]\n",
      "prediction of 968 is [1.]\n",
      "prediction of 969 is [1.]\n",
      "prediction of 970 is [1.]\n",
      "prediction of 971 is [1.]\n",
      "prediction of 972 is [1.]\n",
      "prediction of 973 is [1.]\n",
      "prediction of 974 is [1.]\n",
      "prediction of 975 is [1.]\n",
      "prediction of 976 is [1.]\n",
      "prediction of 977 is [1.]\n",
      "prediction of 978 is [1.]\n",
      "prediction of 979 is [1.]\n",
      "prediction of 980 is [1.]\n",
      "prediction of 981 is [1.]\n",
      "prediction of 982 is [1.]\n",
      "prediction of 983 is [1.]\n",
      "prediction of 984 is [1.]\n",
      "prediction of 985 is [1.]\n",
      "prediction of 986 is [1.]\n",
      "prediction of 987 is [1.]\n",
      "prediction of 988 is [1.]\n",
      "prediction of 989 is [1.]\n",
      "prediction of 990 is [1.]\n",
      "prediction of 991 is [1.]\n",
      "prediction of 992 is [1.]\n",
      "prediction of 993 is [1.]\n",
      "prediction of 994 is [1.]\n",
      "prediction of 995 is [1.]\n",
      "prediction of 996 is [1.]\n",
      "prediction of 997 is [1.]\n",
      "prediction of 998 is [1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictions)-1):\n",
    "    print(\"prediction of \" + str(i) + \" is \" + str(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43minputs\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "len(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
