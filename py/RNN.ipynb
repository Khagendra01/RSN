{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self,\n",
    "                 layer_list,\n",
    "                 time_steps,\n",
    "                 weight_scaler = 1,\n",
    "                 firing_rate_scaler = 0,\n",
    "                 learning_rate = 0.1,\n",
    "                 loss_function = 'mse',\n",
    "                 optimizer = 'sgd'):\n",
    "        \n",
    "        #Assign basic parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = layer_list\n",
    "        self.time_steps = time_steps\n",
    "        self.loss_func = loss_function\n",
    "        n = len(layer_list)\n",
    "        self.dh_list = []\n",
    "        self.gradient_list=[]\n",
    "        \n",
    "        #Initialize layers\n",
    "        self.layers = []\n",
    "        for i in range(n):\n",
    "            self.layers.append(np.zeros(self.shape[i]))\n",
    "        \n",
    "        #Q and D are initiliazed with zeros\n",
    "        self.Q_intime = []\n",
    "        for t in range(self.time_steps+1):\n",
    "            Q = []\n",
    "            for i in range(n):\n",
    "                Q.append(np.zeros((self.shape[i],1)))\n",
    "            self.Q_intime.append(copy.deepcopy(Q))\n",
    "\n",
    "        # Initialize vertical weights: W_vertical\n",
    "        self.W_ver = []\n",
    "        for i in range(n-1):\n",
    "            self.W_ver.append(np.zeros((self.shape[i],self.shape[i+1])))\n",
    "            self.gradient_list.append(np.zeros((self.shape[i],self.shape[i+1])))\n",
    "        \n",
    "        \n",
    "        #Initialize horizontal weights: W_horizontal\n",
    "        self.W_hor = np.zeros((self.shape[1],self.shape[1]))\n",
    "        self.gradient_list.append(np.zeros((self.shape[1],self.shape[1])))\n",
    "\n",
    "        self.bh = np.zeros((self.shape[1], 1))\n",
    "        self.gradient_list.append(np.zeros((self.shape[1], 1)))\n",
    "        \n",
    "        self.by = np.zeros((self.shape[2], 1))\n",
    "        self.gradient_list.append(np.zeros((self.shape[2], 1)))\n",
    "        \n",
    "        self.init_weights(weight_scaler)\n",
    "        self.bp_counter = 0\n",
    "        self.gradient_list_2=copy.deepcopy(self.gradient_list)\n",
    "        self.gradient_list_3=copy.deepcopy(self.gradient_list)\n",
    "        \n",
    "    def init_weights(self,weight_scaler):\n",
    "        \n",
    "        for i in range(len(self.W_ver)):\n",
    "            self.W_ver[i] = weight_scaler*np.random.rand(self.W_ver[i].shape[0],self.W_ver[i].shape[1])\n",
    "        self.W_hor = weight_scaler*np.random.rand(self.W_hor.shape[0],self.W_hor.shape[1])\n",
    "        \n",
    "        return 1 \n",
    "    \n",
    "    def feedforward(self,input_list):\n",
    "\n",
    "        t=0\n",
    "        \n",
    "        for t,input_t in enumerate(input_list,start = 1):\n",
    "           \n",
    "            self.Q_intime[t][0]=np.array(input_t).reshape(-1,1)\n",
    "            self.Q_intime[t][1]=np.tanh(self.W_ver[0].transpose() @ self.Q_intime[t][0] + \n",
    "                    self.W_hor.transpose() @ self.Q_intime[t-1][1] +\n",
    "                    self.bh)\n",
    "        \n",
    "\n",
    "        self.Q_intime[t][2] = self.W_ver[1].transpose()@self.Q_intime[t][1] + self.by\n",
    "          \n",
    "        return copy.deepcopy(self.Q_intime[t][2])\n",
    "    \n",
    "    def dh_dfi(self,t):\n",
    "        return 1-self.Q_intime[t][1]**2\n",
    "    \n",
    "    def backpropagation(self,real_output,tmp1):\n",
    "        self.bp_counter=self.bp_counter+1\n",
    "        \n",
    "        if self.optimizer=='nag':\n",
    "            weights=np.asarray([self.W_ver[0],self.W_ver[1],self.W_hor,self.bh,self.by])\n",
    "\n",
    "            x_ahead = weights-np.asarray(self.gradient_list)*0.9\n",
    "            \n",
    "            self.W_ver[0] = copy.deepcopy(x_ahead[0])\n",
    "            self.W_ver[1] = copy.deepcopy(x_ahead[1])\n",
    "            self.W_hor = copy.deepcopy(x_ahead[2])\n",
    "            self.bh = copy.deepcopy(x_ahead[3])\n",
    "            self.by = copy.deepcopy(x_ahead[4])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        \n",
    "        d_Wih = []\n",
    "        d_Whh = []\n",
    "        d_Who= []\n",
    "        d_bh= []\n",
    "        d_by = []\n",
    "        \n",
    "        #tmp1 = 1\n",
    "        \n",
    "        d_Who.append(tmp1*self.Q_intime[self.time_steps][1])\n",
    "        d_by.append(tmp1)\n",
    "        \n",
    "        d_hidden_layer = copy.deepcopy(tmp1*self.W_ver[1])\n",
    "        der_chain = d_hidden_layer * self.dh_dfi(self.time_steps)\n",
    "        \n",
    "        for t in reversed(range(1,self.time_steps+1)):#2 1 0\n",
    "            \n",
    "            d_Wih.append((der_chain @ self.Q_intime[t][0].transpose()).transpose())\n",
    "            d_Whh.append((der_chain @ self.Q_intime[t][1].transpose()).transpose())\n",
    "            d_bh.append(der_chain)\n",
    "            self.dh_list.append(der_chain)\n",
    "            der_chain = self.W_hor @ (self.dh_dfi(t-1) * der_chain)\n",
    "            \n",
    "        #dh_list.clear()\n",
    "        #Create final gradients\n",
    "        gradient_Wih = sum(d_Wih)\n",
    "        gradient_Who = sum(d_Who)\n",
    "        gradient_Whh = sum(d_Whh)\n",
    "        gradient_bh = sum(d_bh)\n",
    "        gradient_by = sum(d_by)\n",
    "        \n",
    "        for d in [gradient_Wih, gradient_Whh, gradient_Who, gradient_bh, gradient_by]:\n",
    "          np.clip(d, -1, 1, out=d)\n",
    "        \n",
    "        #grads=np.asarray([sum(d_Wih),sum(d_Who),sum(d_Whh),sum(d_bh),sum(d_by)])\n",
    "        grads=np.asarray([gradient_Wih,gradient_Who,gradient_Whh,gradient_bh,gradient_by])\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.update_weigths(grads*self.learning_rate)\n",
    "\n",
    "        elif self.optimizer == 'momentum':\n",
    "            moment = np.asarray(self.gradient_list)*0.9\n",
    "            grads = moment + grads*self.learning_rate\n",
    "            self.update_weigths(grads)\n",
    "            self.gradient_list=copy.deepcopy(grads)\n",
    "        \n",
    "        elif self.optimizer == 'nag':\n",
    "            self.gradient_list = np.asarray(self.gradient_list)*0.9 + grads*self.learning_rate\n",
    "            self.update_weigths(self.gradient_list)\n",
    "            \n",
    "        elif self.optimizer == 'adagrad':\n",
    "            grads_2=np.square(grads)\n",
    "            self.gradient_list+=copy.deepcopy(grads_2)\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (self.learning_rate/(np.sqrt(self.gradient_list[i]+0.000001)))*grads[i]\n",
    "            self.update_weigths(grads)\n",
    "            #self.gradient_list+=copy.deepcopy(grads_2)\n",
    "            \n",
    "        elif self.optimizer == 'adadelta': #gradient_list_2->gt//gradient_list->teta_t\n",
    "\n",
    "            eps=0.000001;beta=0.90;\n",
    "            grads_2 = np.square(grads)\n",
    "            self.gradient_list_2 = beta*np.asarray(self.gradient_list_2) + (1-beta)*grads_2\n",
    "            \n",
    "            delta_teta = copy.deepcopy(self.gradient_list)\n",
    "            for i in range(len(grads)):\n",
    "                #delta_teta[i] = (self.learning_rate/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "                delta_teta[i] = (np.sqrt(self.gradient_list[i]+0.000001)/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "      \n",
    "            self.gradient_list = beta*np.asarray(self.gradient_list) + (1-beta)*np.square(delta_teta)\n",
    "\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (np.sqrt(self.gradient_list[i]+0.000001)/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "            \n",
    "            self.update_weigths(grads)\n",
    "            #self.gradient_list = beta*np.asarray(self.gradient_list) + (1-beta)*np.square(grads)\n",
    "\n",
    "            \n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            eps=0.00000001\n",
    "            grads_2 = np.square(grads)\n",
    "            self.gradient_list = 0.9*np.asarray(self.gradient_list) + 0.1*grads_2\n",
    "\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (self.learning_rate / np.sqrt(self.gradient_list[i]+eps)) * grads[i]\n",
    "            self.update_weigths(grads)\n",
    "        else:\n",
    "            raise Exception('Unknown optimizer : \\'{}\\''.format(self.optimizer))\n",
    "        \n",
    "        return loss\n",
    "    def update_weigths(self,grads):\n",
    "        self.W_ver[0] = copy.deepcopy(self.W_ver[0] - grads[0])\n",
    "        self.W_ver[1] = copy.deepcopy(self.W_ver[1] - grads[1])\n",
    "        self.W_hor = copy.deepcopy(self.W_hor - grads[2])\n",
    "        self.bh = copy.deepcopy(self.bh - grads[3])\n",
    "        self.by = copy.deepcopy(self.by - grads[4])\n",
    "    def softmax(self,xs):\n",
    "      return np.exp(xs) / sum(np.exp(xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stock_prices.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "df = df[['AAPL']]\n",
    "\n",
    "# Split the data into input sequences and target outputs\n",
    "sequence_length = 30  # Number of previous days to use as input\n",
    "X = []\n",
    "y = []\n",
    "for i in range(sequence_length, len(df)):\n",
    "    X.append(df['AAPL'].values[i-sequence_length:i])\n",
    "    y.append(df['AAPL'].values[i])\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "X_norm = X / np.max(X)\n",
    "y_norm = y / np.max(y)\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_norm, y_norm):\n\u001b[0;32m      7\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mfeedforward(seq\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m, in \u001b[0;36mSimpleRNN.backpropagation\u001b[1;34m(self, real_output, tmp1)\u001b[0m\n\u001b[0;32m    131\u001b[0m   np\u001b[38;5;241m.\u001b[39mclip(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39md)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m#grads=np.asarray([sum(d_Wih),sum(d_Who),sum(d_Whh),sum(d_bh),sum(d_by)])\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m grads\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgradient_Wih\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_Who\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_Whh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_bh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_by\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weigths(grads\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Create an instance of the SimpleRNN class\n",
    "rnn = SimpleRNN(layer_list=[1, 10, 1], time_steps=sequence_length)\n",
    "\n",
    "# Train the RNN\n",
    "for epoch in range(100):\n",
    "    for seq, target in zip(X_norm, y_norm):\n",
    "        predictions = rnn.feedforward(seq.reshape(-1, 1))\n",
    "        loss = rnn.backpropagation(target.reshape(-1, 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 3ms/step - loss: 3.7299 - accuracy: 0.0250\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 3.3501 - accuracy: 0.0500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 3.1219 - accuracy: 0.1000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9395 - accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7897 - accuracy: 0.2750\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6782 - accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6022 - accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5238 - accuracy: 0.2750\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4834 - accuracy: 0.2750\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4259 - accuracy: 0.3000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.3738 - accuracy: 0.3500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.3344 - accuracy: 0.3500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.2819 - accuracy: 0.4250\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.2333 - accuracy: 0.4500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.1886 - accuracy: 0.4250\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.1446 - accuracy: 0.4250\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.1016 - accuracy: 0.4500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.0739 - accuracy: 0.4750\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.0380 - accuracy: 0.5250\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.0026 - accuracy: 0.5250\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.9743 - accuracy: 0.5250\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.9422 - accuracy: 0.4750\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9099 - accuracy: 0.4750\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8737 - accuracy: 0.4750\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.8481 - accuracy: 0.4750\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.8201 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7962 - accuracy: 0.5250\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7695 - accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7358 - accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 475us/step - loss: 1.7081 - accuracy: 0.5500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.6801 - accuracy: 0.5500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.6553 - accuracy: 0.5500\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.6307 - accuracy: 0.6000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6050 - accuracy: 0.6250\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5825 - accuracy: 0.6250\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.5631 - accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5408 - accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.5220 - accuracy: 0.6750\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 706us/step - loss: 1.5074 - accuracy: 0.7000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 780us/step - loss: 1.4860 - accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.4627 - accuracy: 0.7250\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.4370 - accuracy: 0.7250\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.4164 - accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3943 - accuracy: 0.7750\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3786 - accuracy: 0.7750\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 250us/step - loss: 1.3668 - accuracy: 0.7000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.3519 - accuracy: 0.6750\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3369 - accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3186 - accuracy: 0.6750\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2947 - accuracy: 0.6750\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.2735 - accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2515 - accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.2334 - accuracy: 0.7250\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2208 - accuracy: 0.7250\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2120 - accuracy: 0.6750\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.1987 - accuracy: 0.6750\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1932 - accuracy: 0.6250\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1812 - accuracy: 0.6750\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1618 - accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1478 - accuracy: 0.6750\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1257 - accuracy: 0.7250\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1026 - accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0879 - accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.0698 - accuracy: 0.7500\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0535 - accuracy: 0.7250\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.0391 - accuracy: 0.8250\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.0329 - accuracy: 0.8250\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0199 - accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 1.0086 - accuracy: 0.8500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9970 - accuracy: 0.8500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.9851 - accuracy: 0.8500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.9683 - accuracy: 0.8500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9540 - accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9353 - accuracy: 0.8250\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9235 - accuracy: 0.8750\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.9132 - accuracy: 0.8250\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.9012 - accuracy: 0.8250\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.8948 - accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.8841 - accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8721 - accuracy: 0.8250\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8619 - accuracy: 0.8250\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8527 - accuracy: 0.8250\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8433 - accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8414 - accuracy: 0.8250\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8380 - accuracy: 0.8250\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8297 - accuracy: 0.8250\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8135 - accuracy: 0.8500\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7970 - accuracy: 0.9000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7775 - accuracy: 0.9000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.7667 - accuracy: 0.9000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.7600 - accuracy: 0.9000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 196us/step - loss: 0.7532 - accuracy: 0.9000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.7475 - accuracy: 0.9000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7374 - accuracy: 0.9000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7290 - accuracy: 0.9000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7220 - accuracy: 0.9000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7156 - accuracy: 0.9000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7076 - accuracy: 0.9000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6992 - accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6911 - accuracy: 0.9000\n",
      "Generated text: e queqtjflbqtdqtjflbqtdq\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "import numpy as np\n",
    "\n",
    "# Define the text corpus\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a character mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []\n",
    "Y = []\n",
    "seq_length = 4  # Length of input sequences\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    X.append([char_to_int[char] for char in text[i:i+seq_length]])\n",
    "    Y.append(char_to_int[text[i+seq_length]])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# One-hot encode the output\n",
    "num_chars = len(chars)\n",
    "Y = np.eye(num_chars)[Y]\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape=(seq_length, 1)))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X.reshape(X.shape[0], X.shape[1], 1), Y, epochs=100, batch_size=32)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed_text, num_chars):\n",
    "    seed_length = len(seed_text)\n",
    "    x = np.zeros((1, seq_length, 1))\n",
    "    \n",
    "    # Truncate seed_text if it's longer than seq_length\n",
    "    if seed_length > seq_length:\n",
    "        seed_text = seed_text[-seq_length:]\n",
    "\n",
    "    for i, char in enumerate(seed_text):\n",
    "        x[0, i, 0] = char_to_int[char]\n",
    "\n",
    "    text = seed_text\n",
    "    for _ in range(num_chars):\n",
    "        y_pred = model.predict(x, verbose=0)\n",
    "        index = np.argmax(y_pred[0])\n",
    "        char = int_to_char[index]\n",
    "        text += char\n",
    "        x = np.zeros((1, seq_length, 1))\n",
    "        x[0, :-1, 0] = [char_to_int[c] for c in text[-seq_length+1:]]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Generate some text\n",
    "seed_text = \"The qu\"\n",
    "generated_text = generate_text(seed_text, 20)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
