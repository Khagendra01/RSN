{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self,\n",
    "                 layer_list,\n",
    "                 time_steps,\n",
    "                 weight_scaler = 1,\n",
    "                 firing_rate_scaler = 0,\n",
    "                 learning_rate = 0.1,\n",
    "                 loss_function = 'mse',\n",
    "                 optimizer = 'sgd'):\n",
    "        \n",
    "        #Assign basic parameters\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.shape = layer_list\n",
    "        self.time_steps = time_steps\n",
    "        self.loss_func = loss_function\n",
    "        n = len(layer_list)\n",
    "        self.dh_list = []\n",
    "        self.gradient_list=[]\n",
    "        \n",
    "        #Initialize layers\n",
    "        self.layers = []\n",
    "        for i in range(n):\n",
    "            self.layers.append(np.zeros(self.shape[i]))\n",
    "        \n",
    "        #Q and D are initiliazed with zeros\n",
    "        self.Q_intime = []\n",
    "        for t in range(self.time_steps+1):\n",
    "            Q = []\n",
    "            for i in range(n):\n",
    "                Q.append(np.zeros((self.shape[i],1)))\n",
    "            self.Q_intime.append(copy.deepcopy(Q))\n",
    "\n",
    "        # Initialize vertical weights: W_vertical\n",
    "        self.W_ver = []\n",
    "        for i in range(n-1):\n",
    "            self.W_ver.append(np.zeros((self.shape[i],self.shape[i+1])))\n",
    "            self.gradient_list.append(np.zeros((self.shape[i],self.shape[i+1])))\n",
    "        \n",
    "        \n",
    "        #Initialize horizontal weights: W_horizontal\n",
    "        self.W_hor = np.zeros((self.shape[1],self.shape[1]))\n",
    "        self.gradient_list.append(np.zeros((self.shape[1],self.shape[1])))\n",
    "\n",
    "        self.bh = np.zeros((self.shape[1], 1))\n",
    "        self.gradient_list.append(np.zeros((self.shape[1], 1)))\n",
    "        \n",
    "        self.by = np.zeros((self.shape[2], 1))\n",
    "        self.gradient_list.append(np.zeros((self.shape[2], 1)))\n",
    "        \n",
    "        self.init_weights(weight_scaler)\n",
    "        self.bp_counter = 0\n",
    "        self.gradient_list_2=copy.deepcopy(self.gradient_list)\n",
    "        self.gradient_list_3=copy.deepcopy(self.gradient_list)\n",
    "        \n",
    "    def init_weights(self,weight_scaler):\n",
    "        \n",
    "        for i in range(len(self.W_ver)):\n",
    "            self.W_ver[i] = weight_scaler*np.random.rand(self.W_ver[i].shape[0],self.W_ver[i].shape[1])\n",
    "        self.W_hor = weight_scaler*np.random.rand(self.W_hor.shape[0],self.W_hor.shape[1])\n",
    "        \n",
    "        return 1 \n",
    "    \n",
    "    def feedforward(self,input_list):\n",
    "\n",
    "        t=0\n",
    "        \n",
    "        for t,input_t in enumerate(input_list,start = 1):\n",
    "           \n",
    "            self.Q_intime[t][0]=np.array(input_t).reshape(-1,1)\n",
    "            self.Q_intime[t][1]=np.tanh(self.W_ver[0].transpose() @ self.Q_intime[t][0] + \n",
    "                    self.W_hor.transpose() @ self.Q_intime[t-1][1] +\n",
    "                    self.bh)\n",
    "        \n",
    "\n",
    "        self.Q_intime[t][2] = self.W_ver[1].transpose()@self.Q_intime[t][1] + self.by\n",
    "          \n",
    "        return copy.deepcopy(self.Q_intime[t][2])\n",
    "    \n",
    "    def dh_dfi(self,t):\n",
    "        return 1-self.Q_intime[t][1]**2\n",
    "    \n",
    "    def backpropagation(self,real_output,tmp1):\n",
    "        self.bp_counter=self.bp_counter+1\n",
    "        \n",
    "        if self.optimizer=='nag':\n",
    "            weights=np.asarray([self.W_ver[0],self.W_ver[1],self.W_hor,self.bh,self.by])\n",
    "\n",
    "            x_ahead = weights-np.asarray(self.gradient_list)*0.9\n",
    "            \n",
    "            self.W_ver[0] = copy.deepcopy(x_ahead[0])\n",
    "            self.W_ver[1] = copy.deepcopy(x_ahead[1])\n",
    "            self.W_hor = copy.deepcopy(x_ahead[2])\n",
    "            self.bh = copy.deepcopy(x_ahead[3])\n",
    "            self.by = copy.deepcopy(x_ahead[4])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        \n",
    "        d_Wih = []\n",
    "        d_Whh = []\n",
    "        d_Who= []\n",
    "        d_bh= []\n",
    "        d_by = []\n",
    "        \n",
    "        #tmp1 = 1\n",
    "        \n",
    "        d_Who.append(tmp1*self.Q_intime[self.time_steps][1])\n",
    "        d_by.append(tmp1)\n",
    "        \n",
    "        d_hidden_layer = copy.deepcopy(tmp1*self.W_ver[1])\n",
    "        der_chain = d_hidden_layer * self.dh_dfi(self.time_steps)\n",
    "        \n",
    "        for t in reversed(range(1,self.time_steps+1)):#2 1 0\n",
    "            \n",
    "            d_Wih.append((der_chain @ self.Q_intime[t][0].transpose()).transpose())\n",
    "            d_Whh.append((der_chain @ self.Q_intime[t][1].transpose()).transpose())\n",
    "            d_bh.append(der_chain)\n",
    "            self.dh_list.append(der_chain)\n",
    "            der_chain = self.W_hor @ (self.dh_dfi(t-1) * der_chain)\n",
    "            \n",
    "        #dh_list.clear()\n",
    "        #Create final gradients\n",
    "        gradient_Wih = sum(d_Wih)\n",
    "        gradient_Who = sum(d_Who)\n",
    "        gradient_Whh = sum(d_Whh)\n",
    "        gradient_bh = sum(d_bh)\n",
    "        gradient_by = sum(d_by)\n",
    "        \n",
    "        for d in [gradient_Wih, gradient_Whh, gradient_Who, gradient_bh, gradient_by]:\n",
    "          np.clip(d, -1, 1, out=d)\n",
    "        \n",
    "        #grads=np.asarray([sum(d_Wih),sum(d_Who),sum(d_Whh),sum(d_bh),sum(d_by)])\n",
    "        grads=np.asarray([gradient_Wih,gradient_Who,gradient_Whh,gradient_bh,gradient_by])\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.update_weigths(grads*self.learning_rate)\n",
    "\n",
    "        elif self.optimizer == 'momentum':\n",
    "            moment = np.asarray(self.gradient_list)*0.9\n",
    "            grads = moment + grads*self.learning_rate\n",
    "            self.update_weigths(grads)\n",
    "            self.gradient_list=copy.deepcopy(grads)\n",
    "        \n",
    "        elif self.optimizer == 'nag':\n",
    "            self.gradient_list = np.asarray(self.gradient_list)*0.9 + grads*self.learning_rate\n",
    "            self.update_weigths(self.gradient_list)\n",
    "            \n",
    "        elif self.optimizer == 'adagrad':\n",
    "            grads_2=np.square(grads)\n",
    "            self.gradient_list+=copy.deepcopy(grads_2)\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (self.learning_rate/(np.sqrt(self.gradient_list[i]+0.000001)))*grads[i]\n",
    "            self.update_weigths(grads)\n",
    "            #self.gradient_list+=copy.deepcopy(grads_2)\n",
    "            \n",
    "        elif self.optimizer == 'adadelta': #gradient_list_2->gt//gradient_list->teta_t\n",
    "\n",
    "            eps=0.000001;beta=0.90;\n",
    "            grads_2 = np.square(grads)\n",
    "            self.gradient_list_2 = beta*np.asarray(self.gradient_list_2) + (1-beta)*grads_2\n",
    "            \n",
    "            delta_teta = copy.deepcopy(self.gradient_list)\n",
    "            for i in range(len(grads)):\n",
    "                #delta_teta[i] = (self.learning_rate/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "                delta_teta[i] = (np.sqrt(self.gradient_list[i]+0.000001)/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "      \n",
    "            self.gradient_list = beta*np.asarray(self.gradient_list) + (1-beta)*np.square(delta_teta)\n",
    "\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (np.sqrt(self.gradient_list[i]+0.000001)/(np.sqrt(self.gradient_list_2[i]+0.000001)))*grads[i]\n",
    "            \n",
    "            self.update_weigths(grads)\n",
    "            #self.gradient_list = beta*np.asarray(self.gradient_list) + (1-beta)*np.square(grads)\n",
    "\n",
    "            \n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            eps=0.00000001\n",
    "            grads_2 = np.square(grads)\n",
    "            self.gradient_list = 0.9*np.asarray(self.gradient_list) + 0.1*grads_2\n",
    "\n",
    "            for i in range(len(grads)):\n",
    "                grads[i] = (self.learning_rate / np.sqrt(self.gradient_list[i]+eps)) * grads[i]\n",
    "            self.update_weigths(grads)\n",
    "        else:\n",
    "            raise Exception('Unknown optimizer : \\'{}\\''.format(self.optimizer))\n",
    "        \n",
    "        return loss\n",
    "    def update_weigths(self,grads):\n",
    "        self.W_ver[0] = copy.deepcopy(self.W_ver[0] - grads[0])\n",
    "        self.W_ver[1] = copy.deepcopy(self.W_ver[1] - grads[1])\n",
    "        self.W_hor = copy.deepcopy(self.W_hor - grads[2])\n",
    "        self.bh = copy.deepcopy(self.bh - grads[3])\n",
    "        self.by = copy.deepcopy(self.by - grads[4])\n",
    "    def softmax(self,xs):\n",
    "      return np.exp(xs) / sum(np.exp(xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stock_prices.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "df = df[['AAPL']]\n",
    "\n",
    "# Split the data into input sequences and target outputs\n",
    "sequence_length = 30  # Number of previous days to use as input\n",
    "X = []\n",
    "y = []\n",
    "for i in range(sequence_length, len(df)):\n",
    "    X.append(df['AAPL'].values[i-sequence_length:i])\n",
    "    y.append(df['AAPL'].values[i])\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "X_norm = X / np.max(X)\n",
    "y_norm = y / np.max(y)\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_norm, y_norm):\n\u001b[0;32m      7\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mfeedforward(seq\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m, in \u001b[0;36mSimpleRNN.backpropagation\u001b[1;34m(self, real_output, tmp1)\u001b[0m\n\u001b[0;32m    131\u001b[0m   np\u001b[38;5;241m.\u001b[39mclip(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39md)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m#grads=np.asarray([sum(d_Wih),sum(d_Who),sum(d_Whh),sum(d_bh),sum(d_by)])\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m grads\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgradient_Wih\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_Who\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_Whh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_bh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgradient_by\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weigths(grads\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Create an instance of the SimpleRNN class\n",
    "rnn = SimpleRNN(layer_list=[1, 10, 1], time_steps=sequence_length)\n",
    "\n",
    "# Train the RNN\n",
    "for epoch in range(100):\n",
    "    for seq, target in zip(X_norm, y_norm):\n",
    "        predictions = rnn.feedforward(seq.reshape(-1, 1))\n",
    "        loss = rnn.backpropagation(target.reshape(-1, 1), predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
